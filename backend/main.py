from fastapi import FastAPI, File, UploadFile, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
import os
import json
import uuid
# base64 ã¯ä¸è¦ï¼ˆVision API WEB_DETECTIONã‚’ä½¿ç”¨ï¼‰
import re
import logging
# requests ã¯ä¸è¦ï¼ˆhttpxã‚’ä½¿ç”¨ï¼‰
from datetime import datetime
from typing import Dict, List, Optional
from io import BytesIO
from dotenv import load_dotenv
from PIL import Image
# serpapi ã¯ä¸è¦ï¼ˆVision API WEB_DETECTIONã‚’ä½¿ç”¨ï¼‰
import httpx
from bs4 import BeautifulSoup
from google.cloud import vision
import google.generativeai as genai
import hashlib
import csv
from io import StringIO
from urllib.parse import urlparse
from fastapi.responses import Response
import logging
logger = logging.getLogger(__name__)

# PDFå‡¦ç†ç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    import fitz  # PyMuPDF
    PDF_SUPPORT = True
    logger.info("âœ… PDFå‡¦ç†æ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã§ã™ (PyMuPDF)")
except ImportError:
    try:
        from pdf2image import convert_from_bytes
        import PyPDF2
        PDF_SUPPORT = True
        logger.info("âœ… PDFå‡¦ç†æ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã§ã™ (pdf2image + PyPDF2)")
    except ImportError:
        PDF_SUPPORT = False
        logger.warning("âš ï¸ PDFå‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚pip install PyMuPDF ã¾ãŸã¯ pip install pdf2image PyPDF2 ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
try:
    from serpapi import GoogleSearch  # type: ignore
    SerpAPI_available = True
    print("âœ… SerpAPI available")
except ImportError:
    try:
        # ä»£æ›¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ–¹æ³•
        from serpapi.google_search import GoogleSearch  # type: ignore
        SerpAPI_available = True
        print("âœ… SerpAPI available (alternative import)")
    except ImportError:
        GoogleSearch = None
        SerpAPI_available = False
        print("âš ï¸ SerpAPI not available - continuing without it")

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ãƒ­ã‚°ä¿å­˜ç”¨ï¼ˆãƒ¡ãƒ¢ãƒªå†…ï¼‰
system_logs = []
MAX_LOGS = 100  # æœ€å¤§ä¿å­˜ãƒ­ã‚°æ•°

class ListHandler(logging.Handler):
    """ãƒ­ã‚°ã‚’ãƒªã‚¹ãƒˆã«ä¿å­˜ã™ã‚‹ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    def emit(self, record):
        log_entry = {
            "timestamp": datetime.fromtimestamp(record.created).strftime("%Y-%m-%d %H:%M:%S"),
            "level": record.levelname,
            "message": record.getMessage()
        }
        system_logs.append(log_entry)
        # å¤ã„ãƒ­ã‚°ã‚’å‰Šé™¤
        if len(system_logs) > MAX_LOGS:
            system_logs.pop(0)

# ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¿½åŠ 
list_handler = ListHandler()
logger.addHandler(list_handler)

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

app = FastAPI(title="Book Leak Detector", version="1.0.0")

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å¿…è¦ãªAPI_KEYã‚’å–å¾—
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GOOGLE_APPLICATION_CREDENTIALS = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
SERPAPI_KEY = os.getenv("SERPAPI_KEY")
X_API_KEY = os.getenv("X_API_KEY")
X_API_SECRET = os.getenv("X_API_SECRET")
X_ACCESS_TOKEN = os.getenv("X_ACCESS_TOKEN")
X_ACCESS_TOKEN_SECRET = os.getenv("X_ACCESS_TOKEN_SECRET")
X_BEARER_TOKEN = os.getenv("X_BEARER_TOKEN")

# Gemini APIã®è¨­å®š
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    logger.info("âœ… Gemini APIè¨­å®šå®Œäº†")
else:
    logger.warning("âš ï¸ GEMINI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

# API_KEYã®è¨­å®šçŠ¶æ³ã‚’ãƒã‚§ãƒƒã‚¯
missing_keys = []
if not GEMINI_API_KEY:
    missing_keys.append("GEMINI_API_KEY")
if not GOOGLE_APPLICATION_CREDENTIALS:
    missing_keys.append("GOOGLE_APPLICATION_CREDENTIALS")
if not SERPAPI_KEY:
    missing_keys.append("SERPAPI_KEY (ç²¾åº¦å‘ä¸Šç”¨)")
if not X_BEARER_TOKEN:
    missing_keys.append("X_BEARER_TOKEN (Twitterå†…å®¹å–å¾—ç”¨)")

if missing_keys:
    required_missing = [k for k in missing_keys if "ç²¾åº¦å‘ä¸Šç”¨" not in k and "ã‚ªãƒ—ã‚·ãƒ§ãƒ³" not in k]
    if required_missing:
        print(f"è­¦å‘Š: ä»¥ä¸‹ã®å¿…é ˆç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“: {', '.join(required_missing)}")
    optional_missing = [k for k in missing_keys if "ç²¾åº¦å‘ä¸Šç”¨" in k or "ã‚ªãƒ—ã‚·ãƒ§ãƒ³" in k or "Twitterå†…å®¹å–å¾—ç”¨" in k]
    if optional_missing:
        print(f"æƒ…å ±: ä»¥ä¸‹ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“: {', '.join(optional_missing)}")
    print("å®Œå…¨ãªæ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€.envãƒ•ã‚¡ã‚¤ãƒ«ã§ä»¥ä¸‹ã‚’è¨­å®šã—ã¦ãã ã•ã„:")
    print("- GEMINI_API_KEY: Gemini AIç”¨")
    print("- GOOGLE_APPLICATION_CREDENTIALS: Google Vision APIç”¨ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼")
    print("- SERPAPI_KEY: SerpAPIç”¨ï¼ˆç²¾åº¦å‘ä¸Šï¼‰")
    print("- X_BEARER_TOKEN: X APIç”¨ï¼ˆTwitterå†…å®¹å–å¾—ï¼‰")
else:
    print("âœ“ å¿…è¦ãªAPI_KEYãŒæ­£å¸¸ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã™")

# SerpAPIåˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒ­ã‚°å‡ºåŠ›
if SerpAPI_available and SERPAPI_KEY:
    print("âœ“ SerpAPIæ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
elif SERPAPI_KEY:
    print("âš ï¸ SERPAPI_KEYã¯è¨­å®šã•ã‚Œã¦ã„ã¾ã™ãŒã€ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
else:
    print("âš ï¸ SerpAPIæ©Ÿèƒ½ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ï¼ˆAPI KEYæœªè¨­å®šï¼‰")

# CORSè¨­å®š - æœ¬ç•ªç’°å¢ƒå¯¾å¿œ
allowed_origins = [
    "http://localhost:3000",
    "http://localhost:5173",
    "http://localhost:5174",
    "https://fujisan-leak-detector.onrender.com",  # Render ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ URLï¼ˆäºˆå‚™ï¼‰
    "https://fujisan-leak-detector-1.onrender.com",  # å®Ÿéš›ã®ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ URL
]

# ç’°å¢ƒå¤‰æ•°ã§CORSã‚ªãƒªã‚¸ãƒ³ã‚’è¿½åŠ å¯èƒ½
if cors_origins := os.getenv("CORS_ORIGINS"):
    allowed_origins.extend(cors_origins.split(","))

app.add_middleware(
    CORSMiddleware,
    allow_origins=allowed_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
UPLOAD_DIR = "uploads"
os.makedirs(UPLOAD_DIR, exist_ok=True)

# é™çš„ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®šï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”»åƒç”¨ï¼‰
app.mount("/uploads", StaticFiles(directory=UPLOAD_DIR), name="uploads")

# ä¸€æ™‚çš„ãªç”»åƒå…¬é–‹ç”¨ï¼ˆæ¤œç´¢æ™‚ã®ã¿ä½¿ç”¨ï¼‰
app.mount("/temp-images", StaticFiles(directory=UPLOAD_DIR), name="temp-images")

# ãƒ¡ãƒ¢ãƒªå†…ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼ˆæœ¬ç•ªç’°å¢ƒã§ã¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨ï¼‰
upload_records: Dict[str, Dict] = {}
search_results: Dict[str, List[Dict]] = {}

# JSONãƒ•ã‚¡ã‚¤ãƒ«ã§ã®æ°¸ç¶šåŒ–
RECORDS_FILE = "upload_records.json"
HISTORY_FILE = "history.json"

# ãƒ¡ãƒ¢ãƒªå†…å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
analysis_history: List[Dict] = []

# ãƒãƒƒãƒå‡¦ç†çŠ¶æ³ç®¡ç†
batch_jobs: Dict[str, Dict] = {}

def load_records():
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨˜éŒ²ã‚’èª­ã¿è¾¼ã¿"""
    global upload_records
    try:
        if os.path.exists(RECORDS_FILE):
            with open(RECORDS_FILE, 'r', encoding='utf-8') as f:
                upload_records = json.load(f)
    except Exception as e:
        print(f"è¨˜éŒ²ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
        upload_records = {}

def save_records():
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜éŒ²ã‚’ä¿å­˜"""
    try:
        with open(RECORDS_FILE, 'w', encoding='utf-8') as f:
            json.dump(upload_records, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"è¨˜éŒ²ã®ä¿å­˜ã«å¤±æ•—: {e}")

def load_history():
    """å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å±¥æ­´ã‚’èª­ã¿è¾¼ã¿"""
    global analysis_history
    try:
        if os.path.exists(HISTORY_FILE):
            with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                analysis_history = json.load(f)
                logger.info(f"ğŸ“š å±¥æ­´èª­ã¿è¾¼ã¿å®Œäº†: {len(analysis_history)}ä»¶")
    except Exception as e:
        logger.error(f"å±¥æ­´ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
        analysis_history = []

def save_history():
    """å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã«å±¥æ­´ã‚’ä¿å­˜"""
    try:
        with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(analysis_history, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"å±¥æ­´ã®ä¿å­˜ã«å¤±æ•—: {e}")

def calculate_image_hash(image_content: bytes) -> str:
    """
    ç”»åƒã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰SHA-256ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—
    åŒã˜ç”»åƒã‚’è­˜åˆ¥ã™ã‚‹ãŸã‚ã«ä½¿ç”¨
    """
    return hashlib.sha256(image_content).hexdigest()

def save_analysis_to_history(image_id: str, image_hash: str, results: List[Dict]):
    """
    åˆ†æçµæœã‚’å±¥æ­´ã«ä¿å­˜
    """
    global analysis_history

    if image_id not in upload_records:
        return

    upload_record = upload_records[image_id]

    history_entry = {
        "history_id": str(uuid.uuid4()),
        "image_id": image_id,
        "image_hash": image_hash,
        "original_filename": upload_record.get("original_filename", "ä¸æ˜"),
        "analysis_date": datetime.now().isoformat(),
        "analysis_timestamp": int(datetime.now().timestamp()),
        "found_urls_count": upload_record.get("found_urls_count", 0),
        "processed_results_count": len(results),
        "results": results
    }

    analysis_history.append(history_entry)
    save_history()
    logger.info(f"ğŸ“š å±¥æ­´ã«ä¿å­˜: {image_id} ({len(results)}ä»¶ã®çµæœ)")

def get_previous_analysis(image_hash: str, exclude_history_id: Optional[str] = None) -> Dict | None:
    """
    åŒã˜ç”»åƒãƒãƒƒã‚·ãƒ¥ã®éå»ã®åˆ†æçµæœã‚’å–å¾—ï¼ˆæœ€æ–°ã®ã‚‚ã®ï¼‰
    """
    matching_histories = [
        h for h in analysis_history
        if h.get("image_hash") == image_hash and h.get("history_id") != exclude_history_id
    ]

    if not matching_histories:
        return None

    # æœ€æ–°ã®åˆ†æçµæœã‚’è¿”ã™
    return max(matching_histories, key=lambda x: x.get("analysis_timestamp", 0))

def calculate_diff(current_results: List[Dict], previous_results: List[Dict]) -> Dict:
    """
    ç¾åœ¨ã®çµæœã¨éå»ã®çµæœã®å·®åˆ†ã‚’è¨ˆç®—
    """
    # URLã‚’ã‚­ãƒ¼ã¨ã—ã¦ãƒãƒƒãƒ—ã‚’ä½œæˆ
    current_urls = {r["url"]: r for r in current_results}
    previous_urls = {r["url"]: r for r in previous_results}

    # æ–°è¦URLï¼ˆç¾åœ¨ã«ã‚ã‚‹ãŒéå»ã«ãªã„ï¼‰
    new_urls = []
    for url in current_urls:
        if url not in previous_urls:
            new_urls.append(current_urls[url])

    # æ¶ˆå¤±URLï¼ˆéå»ã«ã‚ã‚‹ãŒç¾åœ¨ã«ãªã„ï¼‰
    disappeared_urls = []
    for url in previous_urls:
        if url not in current_urls:
            disappeared_urls.append(previous_urls[url])

    # åˆ¤å®šå¤‰æ›´URLï¼ˆä¸¡æ–¹ã«ã‚ã‚‹ãŒåˆ¤å®šãŒå¤‰ã‚ã£ãŸï¼‰
    changed_urls = []
    for url in current_urls:
        if url in previous_urls:
            current_judgment = current_urls[url].get("judgment", "ï¼Ÿ")
            previous_judgment = previous_urls[url].get("judgment", "ï¼Ÿ")
            if current_judgment != previous_judgment:
                changed_urls.append({
                    "url": url,
                    "current": current_urls[url],
                    "previous": previous_urls[url]
                })

    return {
        "new_urls": new_urls,
        "disappeared_urls": disappeared_urls,
        "changed_urls": changed_urls,
        "has_changes": len(new_urls) > 0 or len(disappeared_urls) > 0 or len(changed_urls) > 0,
        "total_new": len(new_urls),
        "total_disappeared": len(disappeared_urls),
        "total_changed": len(changed_urls)
    }

# ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã«è¨˜éŒ²ã¨å±¥æ­´ã‚’èª­ã¿è¾¼ã¿
load_records()
load_history()

# å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒªã‚¹ãƒˆã¯å‰Šé™¤ï¼ˆGemini AIã§å‹•çš„åˆ¤å®šï¼‰

# Vision APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã§åˆæœŸåŒ–ï¼ˆRenderå¯¾å¿œï¼‰
try:
    import json
    from google.oauth2 import service_account

    # ã¾ãš GOOGLE_APPLICATION_CREDENTIALS_JSON ã‚’ç¢ºèª
    google_credentials_json = os.getenv("GOOGLE_APPLICATION_CREDENTIALS_JSON")
    if google_credentials_json:
        credentials_info = json.loads(google_credentials_json)
        credentials = service_account.Credentials.from_service_account_info(credentials_info)
        vision_client = vision.ImageAnnotatorClient(credentials=credentials)
        logger.info("âœ… Google Vision APIèªè¨¼å®Œäº†ï¼ˆGOOGLE_APPLICATION_CREDENTIALS_JSONï¼‰")
    else:
        # GOOGLE_APPLICATION_CREDENTIALS ã®å€¤ã‚’ç¢ºèª
        google_credentials = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
        if google_credentials:
            # JSONæ–‡å­—åˆ—ã‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚’åˆ¤å®š
            if google_credentials.strip().startswith('{'):
                # JSONæ–‡å­—åˆ—ã¨ã—ã¦å‡¦ç†
                credentials_info = json.loads(google_credentials)
                credentials = service_account.Credentials.from_service_account_info(credentials_info)
                vision_client = vision.ImageAnnotatorClient(credentials=credentials)
                logger.info("âœ… Google Vision APIèªè¨¼å®Œäº†ï¼ˆGOOGLE_APPLICATION_CREDENTIALS JSONå½¢å¼ï¼‰")
            else:
                # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¨ã—ã¦å‡¦ç†
                if os.path.exists(google_credentials):
                    vision_client = vision.ImageAnnotatorClient()
                    logger.info("âœ… Google Vision APIèªè¨¼å®Œäº†ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼‰")
                else:
                    logger.warning(f"âš ï¸ èªè¨¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {google_credentials}")
                    vision_client = None
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆèªè¨¼ã‚’è©¦è¡Œ
            vision_client = vision.ImageAnnotatorClient()
            logger.info("âœ… Google Vision APIèªè¨¼å®Œäº†ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆèªè¨¼ï¼‰")
except Exception as e:
    logger.warning(f"âš ï¸ Google Vision APIåˆæœŸåŒ–å¤±æ•—: {e}")
    vision_client = None

# Geminiãƒ¢ãƒ‡ãƒ«ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã§åˆæœŸåŒ–
if GEMINI_API_KEY:
    try:
        gemini_model = genai.GenerativeModel('gemini-2.5-flash')
        logger.info("âœ… Gemini ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†")
        logger.info("âœ… Gemini APIè¨­å®šç¢ºèªå®Œäº†")
    except Exception as e:
        logger.error(f"âŒ Gemini ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å¤±æ•—: {e}")
        gemini_model = None
else:
    logger.error("âŒ GEMINI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    gemini_model = None

def validate_file(file: UploadFile) -> bool:
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒæœ‰åŠ¹ãªç”»åƒã¾ãŸã¯PDFã‹ã©ã†ã‹ã‚’æ¤œè¨¼"""
    allowed_types = ["image/jpeg", "image/png", "image/jpg", "image/gif", "image/webp"]

    # PDFå¯¾å¿œ
    if PDF_SUPPORT:
        allowed_types.extend(["application/pdf"])

    return file.content_type in allowed_types

# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚
def validate_image_file(file: UploadFile) -> bool:
    """å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ã—ã¦ã„ã‚‹ï¼ˆéæ¨å¥¨ï¼‰"""
    return validate_file(file)

def convert_pdf_to_images(pdf_content: bytes) -> List[bytes]:
    """
    PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”»åƒã®ãƒªã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹
    å„ãƒšãƒ¼ã‚¸ã‚’å€‹åˆ¥ã®ç”»åƒã¨ã—ã¦è¿”ã™
    """
    images = []

    try:
        # æ–¹æ³•1: PyMuPDF (fitz) ã‚’ä½¿ç”¨
        if 'fitz' in globals():
            logger.info("ğŸ”„ PyMuPDF ã§PDFã‚’ç”»åƒã«å¤‰æ›ä¸­...")
            pdf_document = fitz.open(stream=pdf_content, filetype="pdf")

            for page_num in range(pdf_document.page_count):
                page = pdf_document[page_num]
                # é«˜å“è³ªã§PDFãƒšãƒ¼ã‚¸ã‚’ç”»åƒã«å¤‰æ› (PyMuPDF 1.26.3å¯¾å¿œ)
                pix = page.get_pixmap(dpi=200)  # DPIã§å“è³ªæŒ‡å®š
                img_data = pix.tobytes("png")
                images.append(img_data)
                logger.info(f"ğŸ“„ ãƒšãƒ¼ã‚¸ {page_num + 1} ã‚’ç”»åƒã«å¤‰æ›å®Œäº†")

            pdf_document.close()
            return images

    except Exception as e:
        logger.warning(f"âš ï¸ PyMuPDFå¤‰æ›å¤±æ•—: {e}")

    try:
        # æ–¹æ³•2: pdf2image ã‚’ä½¿ç”¨ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if 'convert_from_bytes' in globals():
            logger.info("ğŸ”„ pdf2image ã§PDFã‚’ç”»åƒã«å¤‰æ›ä¸­...")
            pil_images = convert_from_bytes(pdf_content, dpi=200)

            for i, pil_image in enumerate(pil_images):
                img_buffer = BytesIO()
                pil_image.save(img_buffer, format='PNG')
                images.append(img_buffer.getvalue())
                logger.info(f"ğŸ“„ ãƒšãƒ¼ã‚¸ {i + 1} ã‚’ç”»åƒã«å¤‰æ›å®Œäº†")

            return images

    except Exception as e:
        logger.warning(f"âš ï¸ pdf2imageå¤‰æ›å¤±æ•—: {e}")

    logger.error("âŒ PDFã‚’ç”»åƒã«å¤‰æ›ã§ãã¾ã›ã‚“ã§ã—ãŸ")
    return []

def extract_pdf_text(pdf_content: bytes) -> str:
    """
    PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹ï¼ˆè£œåŠ©æƒ…å ±ã¨ã—ã¦ä½¿ç”¨ï¼‰
    """
    try:
        # æ–¹æ³•1: PyMuPDF (fitz) ã‚’ä½¿ç”¨
        if 'fitz' in globals():
            logger.info("ğŸ”„ PyMuPDF ã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºä¸­...")
            pdf_document = fitz.open(stream=pdf_content, filetype="pdf")
            text = ""

            for page_num in range(pdf_document.page_count):
                page = pdf_document[page_num]
                page_text = page.get_text()
                text += f"[ãƒšãƒ¼ã‚¸ {page_num + 1}]\n{page_text}\n\n"

            pdf_document.close()
            return text.strip()

    except Exception as e:
        logger.warning(f"âš ï¸ PyMuPDF ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå¤±æ•—: {e}")

    try:
        # æ–¹æ³•2: PyPDF2 ã‚’ä½¿ç”¨ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if 'PyPDF2' in globals():
            logger.info("ğŸ”„ PyPDF2 ã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºä¸­...")
            pdf_reader = PyPDF2.PdfReader(BytesIO(pdf_content))
            text = ""

            for page_num, page in enumerate(pdf_reader.pages):
                page_text = page.extract_text()
                text += f"[ãƒšãƒ¼ã‚¸ {page_num + 1}]\n{page_text}\n\n"

            return text.strip()

    except Exception as e:
        logger.warning(f"âš ï¸ PyPDF2 ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå¤±æ•—: {e}")

    return ""

def is_pdf_file(content_type: str, filename: str = "") -> bool:
    """ãƒ•ã‚¡ã‚¤ãƒ«ãŒPDFã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
    return (content_type == "application/pdf" or
            bool(filename and filename.lower().endswith('.pdf')))

# Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰é–¢æ•°ã¯å‰Šé™¤ï¼ˆä¸è¦ï¼‰

def validate_url_availability(url: str) -> bool:
    """
    URLã®æœ‰åŠ¹æ€§ã‚’äº‹å‰ã«ãƒã‚§ãƒƒã‚¯ã™ã‚‹ï¼ˆHEADãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼‰
    200ç•ªå°ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã®å ´åˆã®ã¿Trueã‚’è¿”ã™
    """
    try:
        with httpx.Client(timeout=5.0, follow_redirects=True) as client:
            response = client.head(url, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            return 200 <= response.status_code < 300
    except Exception as e:
        logger.warning(f"âš ï¸ URLæœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯å¤±æ•— {url}: {e}")
        return False

def is_reliable_domain(url: str) -> bool:
    """
    ãƒ‰ãƒ¡ã‚¤ãƒ³ãŒä¿¡é ¼ã§ãã‚‹ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹
    ç–‘ã‚ã—ã„ç”»åƒãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ã‚„æ€ªã—ã„ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’é™¤å¤–
    """
    try:
        from urllib.parse import urlparse
        parsed = urlparse(url)
        domain = parsed.netloc.lower()

        # é™¤å¤–ã™ã¹ãç”»åƒãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°/CDNãƒ‰ãƒ¡ã‚¤ãƒ³
        excluded_domains = [
            'pbs.twimg.com',
            'm.media-amazon.com',
            'img-cdn.theqoo.net',
            'i.imgur.com',
            'cdn.discordapp.com',
            'media.discordapp.net',
            'images.unsplash.com',
            'cdn.pixabay.com',
            'images.pexels.com',
            'img.freepik.com',
            'thumbs.dreamstime.com',
            'previews.123rf.com',
            'st.depositphotos.com',
            'c8.alamy.com',
            'media.gettyimages.com',
            'us.123rf.com',
            'image.shutterstock.com',
            't3.ftcdn.net',
            't4.ftcdn.net',
            'static.turbosquid.com',
            'render.fineartamerica.com'
        ]

        # é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯
        for excluded in excluded_domains:
            if excluded in domain:
                logger.info(f"â­ï¸ é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {domain}")
                return False

        # æ¥µç«¯ã«çŸ­ã„ãƒ‰ãƒ¡ã‚¤ãƒ³åã‚’é™¤å¤–ï¼ˆæ€ªã—ã„ãƒ‰ãƒ¡ã‚¤ãƒ³ã®å¯èƒ½æ€§ï¼‰
        if len(domain.replace('.', '')) < 5:
            logger.info(f"â­ï¸ çŸ­ã™ãã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {domain}")
            return False

        # æ•°å­—ã®ã¿ã®ã‚µãƒ–ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’é™¤å¤–
        if any(part.isdigit() for part in domain.split('.')):
            logger.info(f"â­ï¸ æ•°å­—ã‚µãƒ–ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {domain}")
            return False

        return True
    except Exception as e:
        logger.warning(f"âš ï¸ ãƒ‰ãƒ¡ã‚¤ãƒ³ä¿¡é ¼æ€§ãƒã‚§ãƒƒã‚¯å¤±æ•— {url}: {e}")
        return False

def search_web_for_image(image_content: bytes) -> list[str]:
    """
    ç”»åƒã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å—ã‘å–ã‚Šã€Google Vision API + SerpAPIã§
    é¡ä¼¼ãƒ»åŒä¸€ç”»åƒãŒä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹URLã®ãƒªã‚¹ãƒˆã‚’è¿”ã™ã€‚
    ç²¾åº¦å‘ä¸Šã®ãŸã‚ã€ä¸¡æ–¹ã®APIã‚’çµ„ã¿åˆã‚ã›ã¦ä½¿ç”¨ã€‚
    """
    logger.info("ğŸ” ç”»åƒæ¤œç´¢é–‹å§‹ï¼ˆVision API + SerpAPIä½µç”¨ï¼‰")

    all_urls = []

    try:
        # 1. Google Vision API WEB_DETECTION
        logger.info("ğŸ“Š ã€Phase 1ã€‘Google Vision API WEB_DETECTION")
        image = vision.Image(content=image_content)
        response = vision_client.web_detection(image=image)  # type: ignore
        web_detection = response.web_detection

        # ãƒ‡ãƒãƒƒã‚°ç”¨: å„ãƒãƒƒãƒã‚¿ã‚¤ãƒ—ã®ä»¶æ•°ã‚’ãƒ­ã‚°å‡ºåŠ›
        exact_matches_count = len(web_detection.best_guess_labels) if web_detection.best_guess_labels else 0
        full_matching_count = len(web_detection.full_matching_images) if web_detection.full_matching_images else 0
        partial_matching_count = len(web_detection.partial_matching_images) if web_detection.partial_matching_images else 0
        pages_count = len(web_detection.pages_with_matching_images) if web_detection.pages_with_matching_images else 0

        logger.info(f"ğŸ“ˆ Vision APIæ¤œå‡ºçµæœ:")
        logger.info(f"  - å®Œå…¨ä¸€è‡´ãƒšãƒ¼ã‚¸æ•°: {exact_matches_count}ä»¶")
        logger.info(f"  - å®Œå…¨ä¸€è‡´ç”»åƒæ•°: {full_matching_count}ä»¶")
        logger.info(f"  - éƒ¨åˆ†ä¸€è‡´ç”»åƒæ•°: {partial_matching_count}ä»¶ï¼ˆé«˜å“è³ªã®ã¿ä½¿ç”¨ï¼‰")
        logger.info(f"  - ãƒãƒƒãƒç”»åƒå«ã‚€ãƒšãƒ¼ã‚¸æ•°: {pages_count}ä»¶")

        vision_urls = []

        # Vision APIã‹ã‚‰URLåé›†
        if web_detection.pages_with_matching_images:
            logger.info("ğŸ¯ ãƒãƒƒãƒãƒšãƒ¼ã‚¸ã‹ã‚‰URLæŠ½å‡ºä¸­...")
            for page in web_detection.pages_with_matching_images:
                if page.url and page.url.startswith(('http://', 'https://')):
                    score = getattr(page, 'score', 1.0)
                    if score >= 0.1 or score == 0.0:
                        vision_urls.append(page.url)
                        logger.info(f"  âœ… ãƒšãƒ¼ã‚¸è¿½åŠ  (score: {score:.2f}): {page.url}")

        if web_detection.full_matching_images:
            logger.info("ğŸ¯ å®Œå…¨ä¸€è‡´ç”»åƒã‹ã‚‰URLæŠ½å‡ºä¸­...")
            for img in web_detection.full_matching_images:
                if img.url and img.url.startswith(('http://', 'https://')):
                    vision_urls.append(img.url)
                    logger.info(f"  âœ… å®Œå…¨ä¸€è‡´ç”»åƒè¿½åŠ : {img.url}")

        if web_detection.partial_matching_images and len(vision_urls) < 5:
            logger.info("ğŸ¯ é«˜å“è³ªéƒ¨åˆ†ä¸€è‡´ã‹ã‚‰URLè£œå®Œä¸­...")
            for i, img in enumerate(web_detection.partial_matching_images[:5]):
                if img.url and img.url.startswith(('http://', 'https://')):
                    vision_urls.append(img.url)
                    logger.info(f"  âœ… éƒ¨åˆ†ä¸€è‡´è£œå®Œè¿½åŠ : {img.url}")

        all_urls.extend(vision_urls)
        logger.info(f"âœ… Vision API: {len(vision_urls)}ä»¶ã®URLå–å¾—")

        # 2. SerpAPI ç”»åƒé€†æ¤œç´¢ï¼ˆè¿½åŠ æ¤œç´¢ï¼‰
        logger.info("ğŸ“Š ã€Phase 2ã€‘SerpAPI ç”»åƒé€†æ¤œç´¢")

        # Vision APIã§å–å¾—ã—ãŸç”»åƒURLã‚’ä½¿ã£ã¦SerpAPIæ¤œç´¢
        serpapi_urls = []
        if vision_urls and SERPAPI_KEY:
            # æœ€åˆã®æ•°å€‹ã®ç”»åƒURLã§SerpAPIæ¤œç´¢ã‚’å®Ÿè¡Œ
            for i, img_url in enumerate(vision_urls[:3]):  # æœ€åˆã®3ã¤ã§æ¤œç´¢
                logger.info(f"ğŸ” SerpAPIæ¤œç´¢ ({i+1}/3): {img_url}")
                serp_results = search_with_serpapi(img_url)
                serpapi_urls.extend(serp_results)

                if len(serpapi_urls) >= 10:  # ååˆ†ãªæ•°ãŒé›†ã¾ã£ãŸã‚‰åœæ­¢
                    break

        all_urls.extend(serpapi_urls)
        logger.info(f"âœ… SerpAPI: {len(serpapi_urls)}ä»¶ã®URLå–å¾—")

        # é‡è¤‡é™¤å»ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        logger.info("ğŸ”§ URLå“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é–‹å§‹...")
        filtered_urls = []
        seen = set()

        for url in all_urls:
            if url in seen:
                continue
            seen.add(url)

            # ãƒ‰ãƒ¡ã‚¤ãƒ³ä¿¡é ¼æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€ä½é™ã®é™¤å¤–ã®ã¿ï¼‰
            if not is_reliable_domain_relaxed(url):
                continue

            # URLæœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆå³æ ¼ç‰ˆï¼‰
            logger.info(f"ğŸ” URLæœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯ä¸­: {url}")
            if not validate_url_availability_fast(url):
                logger.info(f"  âŒ ç„¡åŠ¹URLã‚¹ã‚­ãƒƒãƒ—: {url}")
                continue

            filtered_urls.append(url)
            logger.info(f"  âœ… æœ‰åŠ¹URLè¿½åŠ : {url}")

            # æœ€å¤§25ä»¶ã«åˆ¶é™ï¼ˆä¸¡APIä½µç”¨ã«ã‚ˆã‚Šå¢—åŠ ï¼‰
            if len(filtered_urls) >= 25:
                break

        logger.info(f"ğŸŒ æœ€çµ‚çš„ã«é¸åˆ¥ã•ã‚ŒãŸURL: {len(filtered_urls)}ä»¶")
        logger.info(f"ğŸ“Š å†…è¨³: Vision API={len(vision_urls)}ä»¶, SerpAPI={len(serpapi_urls)}ä»¶")

        for i, url in enumerate(filtered_urls[:10]):
            logger.info(f"  {i+1}: {url}")

        if len(filtered_urls) > 10:
            logger.info(f"  ... ä»– {len(filtered_urls) - 10}ä»¶")

        return filtered_urls

    except Exception as e:
        logger.error(f"âŒ ç”»åƒæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []

def is_reliable_domain_relaxed(url: str) -> bool:
    """
    ãƒ‰ãƒ¡ã‚¤ãƒ³ä¿¡é ¼æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€ä½é™ã®é™¤å¤–ã®ã¿ï¼‰
    æœ¬æ¥ã®è¶£æ—¨ï¼šæ€ªã—ã„ãƒ‰ãƒ¡ã‚¤ãƒ³ã“ãAIåˆ¤å®šã§æ‚ªç”¨ãƒã‚§ãƒƒã‚¯ã™ã‚‹ãŸã‚ã€é™¤å¤–ã¯æœ€å°é™ã«
    """
    try:
        from urllib.parse import urlparse
        parsed = urlparse(url)
        domain = parsed.netloc.lower()

        # æœ€ä½é™ã®é™¤å¤–ï¼šæ˜ã‚‰ã‹ã«ç”»åƒã‚µãƒ¼ãƒ“ã‚¹ã®ã¿
        image_only_domains = [
            'i.imgur.com',
            'cdn.discordapp.com',
            'media.discordapp.net',
            'images.unsplash.com',
            'cdn.pixabay.com',
            'images.pexels.com',
        ]

        # ç”»åƒã‚µãƒ¼ãƒ“ã‚¹ã®ã¿é™¤å¤–ï¼ˆä»–ã¯ã™ã¹ã¦AIåˆ¤å®šå¯¾è±¡ï¼‰
        for image_domain in image_only_domains:
            if image_domain in domain:
                logger.info(f"â­ï¸ ç”»åƒã‚µãƒ¼ãƒ“ã‚¹ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {domain}")
                return False

        # ãã®ä»–ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã¯ã™ã¹ã¦é€šã™ï¼ˆæ‚ªç”¨ãƒã‚§ãƒƒã‚¯ã®ãŸã‚ï¼‰
        return True
    except Exception as e:
        logger.warning(f"âš ï¸ ãƒ‰ãƒ¡ã‚¤ãƒ³ä¿¡é ¼æ€§ãƒã‚§ãƒƒã‚¯å¤±æ•— {url}: {e}")
        return True  # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯é€šã™

def search_with_serpapi(image_url: str) -> list[str]:
    """
    SerpAPIã‚’ä½¿ç”¨ã—ã¦ç”»åƒã®é€†æ¤œç´¢ã‚’å®Ÿè¡Œ
    Google Vision APIã¨çµ„ã¿åˆã‚ã›ã¦ç²¾åº¦å‘ä¸Š
    """
    if not SERPAPI_KEY:
        logger.warning("âš ï¸ SERPAPI_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€SerpAPIæ¤œç´¢ã‚’ã‚¹ã‚­ãƒƒãƒ—")
        return []

    if not SerpAPI_available or not GoogleSearch:
        logger.warning("âš ï¸ SerpAPIãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€SerpAPIæ¤œç´¢ã‚’ã‚¹ã‚­ãƒƒãƒ—")
        return []

    logger.info("ğŸ” SerpAPIç”»åƒé€†æ¤œç´¢é–‹å§‹")

    try:
        # SerpAPIã§ç”»åƒé€†æ¤œç´¢ã‚’å®Ÿè¡Œ
        search = GoogleSearch({
            "engine": "google_reverse_image",
            "image_url": image_url,
            "api_key": SERPAPI_KEY,
            "num": 20,      # æœ€å¤§20ä»¶å–å¾—
            "safe": "off"   # ã‚»ãƒ¼ãƒ•ã‚µãƒ¼ãƒç„¡åŠ¹
        })

        results = search.get_dict()

        # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ ã‚’ãƒ­ã‚°å‡ºåŠ›
        logger.info(f"ğŸ” SerpAPI ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚­ãƒ¼: {list(results.keys())}")

        # è¤‡æ•°ã®å¯èƒ½ãªã‚­ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯
        image_results = None
        if "image_results" in results:
            image_results = results["image_results"]
        elif "images_results" in results:
            image_results = results["images_results"]
        elif "inline_images" in results:
            image_results = results["inline_images"]
        elif "related_searches" in results:
            image_results = results["related_searches"]

        if not image_results:
            logger.warning("âš ï¸ SerpAPI: ç”»åƒæ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            logger.warning(f"ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªã‚­ãƒ¼: {list(results.keys())}")
            return []

        urls = []
        for result in image_results[:15]:  # ä¸Šä½15ä»¶
            # è¤‡æ•°ã®å¯èƒ½ãªURLãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
            url = None
            if isinstance(result, dict):
                url = (result.get("link") or
                      result.get("original") or
                      result.get("source") or
                      result.get("url"))

            if url and isinstance(url, str):
                urls.append(url)

        logger.info(f"âœ… SerpAPIæ¤œç´¢å®Œäº†: {len(urls)}ä»¶ã®URLã‚’ç™ºè¦‹")
        return urls

    except Exception as e:
        logger.error(f"âŒ SerpAPIæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []

def get_x_tweet_content(tweet_url: str) -> str | None:
    """
    Xï¼ˆTwitterï¼‰ã®ãƒ„ã‚¤ãƒ¼ãƒˆURLã‹ã‚‰æŠ•ç¨¿å†…å®¹ã‚’å–å¾—
    X API v2ã®Bearer Tokenèªè¨¼ã‚’ä½¿ç”¨
    """
    if not X_BEARER_TOKEN:
        logger.warning("âš ï¸ X_BEARER_TOKEN ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ãƒ„ã‚¤ãƒ¼ãƒˆå†…å®¹å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—")
        return None

    try:
        import re
        from urllib.parse import urlparse

        # ãƒ„ã‚¤ãƒ¼ãƒˆIDã‚’æŠ½å‡º
        tweet_id_match = re.search(r'/status/(\d+)', tweet_url)
        if not tweet_id_match:
            logger.warning(f"âš ï¸ ãƒ„ã‚¤ãƒ¼ãƒˆIDã‚’æŠ½å‡ºã§ãã¾ã›ã‚“: {tweet_url}")
            return None

        tweet_id = tweet_id_match.group(1)
        logger.info(f"ğŸ¦ ãƒ„ã‚¤ãƒ¼ãƒˆå†…å®¹å–å¾—é–‹å§‹: ID={tweet_id}")

        # X API v2ã§ãƒ„ã‚¤ãƒ¼ãƒˆå†…å®¹ã‚’å–å¾—ï¼ˆBearer Tokenèªè¨¼ï¼‰
        headers = {
            'Authorization': f'Bearer {X_BEARER_TOKEN}',
            'Content-Type': 'application/json'
        }

        with httpx.Client(timeout=10.0) as client:
            response = client.get(
                f"https://api.twitter.com/2/tweets/{tweet_id}",
                headers=headers,
                params={
                    'tweet.fields': 'text,author_id,created_at,public_metrics',
                    'user.fields': 'username,name',
                    'expansions': 'author_id'
                }
            )

            if response.status_code == 200:
                data = response.json()
                if 'data' in data:
                    tweet_text = data['data'].get('text', '')
                    author_info = ""

                    # ä½œè€…æƒ…å ±ã‚‚å–å¾—
                    if 'includes' in data and 'users' in data['includes']:
                        user = data['includes']['users'][0]
                        username = user.get('username', '')
                        name = user.get('name', '')
                        author_info = f"@{username} ({name})"

                    logger.info(f"âœ… ãƒ„ã‚¤ãƒ¼ãƒˆå†…å®¹å–å¾—å®Œäº†: {len(tweet_text)}æ–‡å­—")
                    return f"XæŠ•ç¨¿å†…å®¹ {author_info}: {tweet_text}"
                else:
                    logger.warning("âš ï¸ ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    return None
            else:
                logger.error(f"âŒ X API ã‚¨ãƒ©ãƒ¼: {response.status_code} - {response.text}")
                return None

    except Exception as e:
        logger.error(f"âŒ X APIå–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

def validate_url_availability_fast(url: str) -> bool:
    """
    URLã®æœ‰åŠ¹æ€§ã‚’é«˜é€Ÿãƒã‚§ãƒƒã‚¯ï¼ˆå³æ ¼ç‰ˆï¼‰
    ç™½ç´™ãƒšãƒ¼ã‚¸ã‚„ç„¡åŠ¹ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’äº‹å‰ã«é™¤å¤–
    Twitterç”»åƒURLã¯ç‰¹åˆ¥å‡¦ç†ã®ãŸã‚é€šã™
    """
    try:
        # Twitterç”»åƒURLã¯ç‰¹åˆ¥å‡¦ç†ã®ãŸã‚ã€æ¤œè¨¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦é€šã™
        if 'pbs.twimg.com' in url:
            logger.info(f"ğŸ¦ Twitterç”»åƒURLæ¤œå‡º - ç‰¹åˆ¥å‡¦ç†ã®ãŸã‚é€šé: {url}")
            return True

        with httpx.Client(timeout=5.0, follow_redirects=True) as client:
            # 1. HEADãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª
            try:
                head_response = client.head(url, headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                })

                # 4xx/5xxã‚¨ãƒ©ãƒ¼ã¯å³åº§ã«é™¤å¤–
                if head_response.status_code >= 400:
                    logger.info(f"âŒ HTTPã‚¨ãƒ©ãƒ¼ {head_response.status_code}: {url}")
                    return False

                # Content-Typeãƒã‚§ãƒƒã‚¯
                content_type = head_response.headers.get('content-type', '').lower()
                if content_type and 'text/html' not in content_type:
                    logger.info(f"âŒ éHTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ ({content_type}): {url}")
                    return False

            except httpx.RequestError:
                # HEADãŒå¤±æ•—ã—ãŸå ´åˆã¯GETã§å†è©¦è¡Œ
                pass

            # 2. GETãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æœ‰åŠ¹æ€§ã‚’ç¢ºèª
            response = client.get(url, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })

            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
            if not (200 <= response.status_code < 300):
                logger.info(f"âŒ ç„¡åŠ¹ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ {response.status_code}: {url}")
                return False

            # Content-Typeã®æœ€çµ‚ç¢ºèª
            content_type = response.headers.get('content-type', '').lower()
            if 'text/html' not in content_type:
                logger.info(f"âŒ éHTMLãƒ¬ã‚¹ãƒãƒ³ã‚¹ ({content_type}): {url}")
                return False

            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å®Ÿè³ªæ€§ãƒã‚§ãƒƒã‚¯
            content_length = len(response.text.strip())
            if content_length < 100:  # 100æ–‡å­—æœªæº€ã¯ç©ºç™½ãƒšãƒ¼ã‚¸ã¨ã¿ãªã™
                logger.info(f"âŒ ç©ºç™½ãƒšãƒ¼ã‚¸ (é•·ã•: {content_length}): {url}")
                return False

            # ç©ºç™½ãƒšãƒ¼ã‚¸ã‚„ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ã®å…¸å‹çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
            content_lower = response.text.lower()
            error_indicators = [
                'page not found',
                'not found',
                '404',
                'error',
                'page does not exist',
                'pÃ¡gina no encontrada',  # ã‚¹ãƒšã‚¤ãƒ³èªã®ã€Œãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€
                'no se encontrÃ³',
                'sin contenido',
                'empty page',
                'blank page'
            ]

            for indicator in error_indicators:
                if indicator in content_lower and content_length < 1000:
                    logger.info(f"âŒ ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸æ¤œå‡º ('{indicator}'): {url}")
                    return False

            logger.info(f"âœ… æœ‰åŠ¹ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç¢ºèª: {url}")
            return True

    except httpx.TimeoutException:
        logger.info(f"âŒ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {url}")
        return False
    except httpx.RequestError as e:
        logger.info(f"âŒ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {url} - {e}")
        return False
    except Exception as e:
        logger.warning(f"âš ï¸ URLæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {url} - {e}")
        return False

def is_trusted_news_domain(url: str) -> bool:
    """
    ä¿¡é ¼ã§ãã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»å‡ºç‰ˆç³»ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ãƒã‚§ãƒƒã‚¯
    ã“ã‚Œã‚‰ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã¯Geminiåˆ¤å®šã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ç›´æ¥â—‹åˆ¤å®š
    """
    try:
        from urllib.parse import urlparse
        parsed = urlparse(url)
        domain = parsed.netloc.lower()

                # ä¿¡é ¼ã§ãã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»å‡ºç‰ˆãƒ»å…¬å¼ã‚µã‚¤ãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³
        trusted_domains = [
            'news.yahoo.co.jp',
            'www3.nhk.or.jp',
            'mainichi.jp',
            'www.asahi.com',
            'www.yomiuri.co.jp',
            'www.sankei.com',
            'www.nikkei.com',
            'toyokeizai.net',
            'diamond.jp',
            'gendai.media',
            'bunshun.jp',
            'shinchosha.co.jp',
            'kadokawa.co.jp',
            'www.shogakukan.co.jp',
            'www.amazon.co.jp',
            'books.rakuten.co.jp',
            'honto.jp',
            'www.kinokuniya.co.jp',
            'www.tsutaya.co.jp',
            'natalie.mu',
            'www.oricon.co.jp',
            'more.hpplus.jp',
            'www.vogue.co.jp',
            'www.elle.com',
            'www.cosmopolitan.com',
            'mi-mollet.com',
            'www.25ans.jp',
            'cancam.jp',
            'ray-web.jp',
            'www.biteki.com'
        ]

        # å®Œå…¨ä¸€è‡´ãƒã‚§ãƒƒã‚¯
        if domain in trusted_domains:
            return True

        # ã‚µãƒ–ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’å«ã‚€éƒ¨åˆ†ä¸€è‡´ãƒã‚§ãƒƒã‚¯
        for trusted in trusted_domains:
            if domain.endswith('.' + trusted) or domain == trusted:
                return True

        # æ¥½å¤©ãƒ»Amazonã®éƒ¨åˆ†ä¸€è‡´ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ˆã‚Šåºƒç¯„å›²ã«å¯¾å¿œï¼‰
        trusted_patterns = [
            'rakuten.co.jp',  # search.rakuten.co.jp, books.rakuten.co.jp ãªã©
            'amazon.co.jp',   # www.amazon.co.jp ãªã©
            'amazon.com',     # www.amazon.com ãªã©
        ]

        for pattern in trusted_patterns:
            if pattern in domain:
                logger.info(f"âœ… ä¿¡é ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ä¸€è‡´: {pattern} in {domain}")
                return True

        return False
    except Exception as e:
        logger.warning(f"âš ï¸ ãƒ‰ãƒ¡ã‚¤ãƒ³ä¿¡é ¼æ€§ãƒã‚§ãƒƒã‚¯å¤±æ•— {url}: {e}")
        return False

def convert_twitter_image_to_tweet_url(url: str) -> dict | None:
    """
    Twitterç”»åƒURLï¼ˆpbs.twimg.comï¼‰ã‹ã‚‰å…ƒãƒ„ã‚¤ãƒ¼ãƒˆã®URLã¨å†…å®¹ã‚’å–å¾—ã‚’è©¦ã¿ã‚‹
    pbs.twimg.comç”»åƒURLã‹ã‚‰ãƒ„ã‚¤ãƒ¼ãƒˆIDã‚’æ¨å®šã—ã€å…ƒã®ãƒ„ã‚¤ãƒ¼ãƒˆURLã‚’è¿”ã™
    """
    try:
        from urllib.parse import urlparse
        parsed = urlparse(url)

        # Twitterç”»åƒURLã®å ´åˆ
        if 'pbs.twimg.com' in parsed.netloc:
            logger.info(f"ğŸ¦ Twitterç”»åƒURLæ¤œå‡º: {url}")

            # X APIã¾ãŸã¯SerpAPIãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã€ãƒ„ã‚¤ãƒ¼ãƒˆæ¤œç´¢ã‚’è©¦è¡Œ
            if X_BEARER_TOKEN or (SERPAPI_KEY and SerpAPI_available):
                tweet_result = get_x_tweet_url_and_content_by_image(url)
                if tweet_result:
                    return tweet_result

                # æ¤œç´¢ã§è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆã§ã‚‚ã€Geminiã«ç”»åƒã®æ€§è³ªã‚’ä¼ãˆã‚‹
                logger.info("ğŸ¦ ãƒ„ã‚¤ãƒ¼ãƒˆå†…å®¹ã¯ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸãŒã€Twitterç”»åƒã¨ã—ã¦å‡¦ç†")
                return {
                    "tweet_url": None,
                    "content": f"TWITTER_IMAGE_UNKNOWN:{url}"
                }

            # APIåˆ©ç”¨ä¸å¯ã®å ´åˆã¯å¾“æ¥ã®å‡¦ç†
            return {
                "tweet_url": None,
                "content": f"TWITTER_IMAGE:{url}"
            }

        return None
    except Exception as e:
        logger.warning(f"âš ï¸ Twitter URLå¤‰æ›å¤±æ•— {url}: {e}")
        return None

def get_x_tweet_url_and_content_by_image(image_url: str) -> dict | None:
    """
    ç”»åƒURLã‹ã‚‰ãƒ„ã‚¤ãƒ¼ãƒˆURLã¨å†…å®¹ã‚’æ¢ç´¢ã™ã‚‹ï¼ˆé«˜åº¦ç‰ˆï¼‰
    Google Vision API + X API v2 + SerpAPIã‚’çµ„ã¿åˆã‚ã›ã¦ãƒ„ã‚¤ãƒ¼ãƒˆã‚’ç‰¹å®š
    æˆ»ã‚Šå€¤: {"tweet_url": "https://x.com/...", "content": "ãƒ„ã‚¤ãƒ¼ãƒˆå†…å®¹"}
    """
    try:
        logger.info(f"ğŸ¦ ç”»åƒURLçµŒç”±ã§ãƒ„ã‚¤ãƒ¼ãƒˆURLæ¤œç´¢: {image_url}")

        # æ–¹æ³•1: Google Vision APIã®WEB_DETECTIONã‚’ä½¿ç”¨
        if vision_client:
            try:
                logger.info("ğŸ” Google Vision APIã§WEB_DETECTIONå®Ÿè¡Œä¸­...")

                # ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                import httpx
                with httpx.Client(timeout=10.0) as client:
                    response = client.get(image_url)
                    if response.status_code == 200:
                        image_content = response.content

                        # Vision APIå®Ÿè¡Œ
                        from google.cloud import vision
                        image = vision.Image(content=image_content)
                        response = vision_client.web_detection(image=image)  # type: ignore

                        # é–¢é€£ãƒšãƒ¼ã‚¸ã‹ã‚‰ X/Twitter URLã‚’æ¢ç´¢
                        if response.web_detection.pages_with_matching_images:
                            for page in response.web_detection.pages_with_matching_images[:15]:
                                if page.url and any(domain in page.url for domain in ['x.com', 'twitter.com']):
                                    logger.info(f"ğŸ¦ Vision APIã§ãƒ„ã‚¤ãƒ¼ãƒˆURLç™ºè¦‹: {page.url}")
                                    tweet_content = get_x_tweet_content(page.url)
                                    if tweet_content:
                                        return {
                                            "tweet_url": page.url,
                                            "content": tweet_content
                                        }

                        # ã‚ˆã‚Šè©³ç´°ãªé–¢é€£ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚‚ãƒã‚§ãƒƒã‚¯
                        if response.web_detection.web_entities:
                            for entity in response.web_detection.web_entities[:10]:
                                if entity.description:
                                    # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®èª¬æ˜ã‹ã‚‰Twitteré–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œç´¢
                                    description = entity.description.lower()
                                    if any(keyword in description for keyword in ['twitter', 'tweet', 'x.com']):
                                        logger.info(f"ğŸ” é–¢é€£ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ç™ºè¦‹: {entity.description}")

                                        # ã“ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ä½¿ã£ã¦ã•ã‚‰ã«æ¤œç´¢
                                        if SERPAPI_KEY and SerpAPI_available:
                                            search = GoogleSearch({
                                                "engine": "google",
                                                "q": f'site:x.com OR site:twitter.com "{entity.description}"',
                                                "api_key": SERPAPI_KEY,
                                                "num": 10
                                            })
                                            entity_results = search.get_dict()
                                            if "organic_results" in entity_results:
                                                for result in entity_results["organic_results"][:3]:
                                                    if "link" in result and any(domain in result["link"] for domain in ['x.com', 'twitter.com']):
                                                        logger.info(f"ğŸ¦ ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æ¤œç´¢ã§ãƒ„ã‚¤ãƒ¼ãƒˆURLç™ºè¦‹: {result['link']}")
                                                        tweet_content = get_x_tweet_content(result["link"])
                                                        if tweet_content:
                                                            return {
                                                                "tweet_url": result["link"],
                                                                "content": tweet_content
                                                            }

            except Exception as vision_error:
                logger.warning(f"âš ï¸ Vision APIæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {vision_error}")

        # æ–¹æ³•2: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰Snowflake IDã‚’æŠ½å‡ºã—ã¦ãƒ„ã‚¤ãƒ¼ãƒˆIDã‚’æ¨å®š
        import re
        filename_match = re.search(r'/media/([^?]+)', image_url)
        if filename_match:
            filename = filename_match.group(1).split('.')[0]  # æ‹¡å¼µå­ã‚’é™¤å»
            logger.info(f"ğŸ” ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«å: {filename}")

            # Base64URLãƒ‡ã‚³ãƒ¼ãƒ‰ã‚’è©¦è¡Œã—ã¦Snowflake IDã‚’å–å¾—
            try:
                import base64
                from datetime import datetime

                # Twitterã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åã¯é€šå¸¸Base64URLã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸSnowflake ID
                decoded_bytes = base64.urlsafe_b64decode(filename + '==')  # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°è¿½åŠ 
                snowflake_id = int.from_bytes(decoded_bytes, byteorder='big')

                # Snowflake IDã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’è¨ˆç®—ï¼ˆTwitter Epoch: 2010-11-04 01:42:54 UTCï¼‰
                twitter_epoch = 1288834974657  # Twitter epoch in milliseconds
                timestamp_ms = (snowflake_id >> 22) + twitter_epoch
                tweet_datetime = datetime.fromtimestamp(timestamp_ms / 1000)

                logger.info(f"ğŸ“… æ¨å®šæŠ•ç¨¿æ—¥æ™‚: {tweet_datetime}")

                # ã“ã®æƒ…å ±ã‚’ä½¿ã£ã¦ã‚ˆã‚Šç²¾å¯†ãªæ¤œç´¢ã‚’å®Ÿè¡Œ
                if SERPAPI_KEY and SerpAPI_available:
                    date_str = tweet_datetime.strftime("%Y-%m-%d")
                    search = GoogleSearch({
                        "engine": "google",
                        "q": f'site:x.com OR site:twitter.com "{filename}" after:{date_str}',
                        "api_key": SERPAPI_KEY,
                        "num": 15
                    })

                    date_results = search.get_dict()
                    if "organic_results" in date_results:
                        for result in date_results["organic_results"][:5]:
                            if "link" in result and any(domain in result["link"] for domain in ['x.com', 'twitter.com']):
                                logger.info(f"ğŸ¦ æ—¥ä»˜æ¤œç´¢ã§ãƒ„ã‚¤ãƒ¼ãƒˆURLç™ºè¦‹: {result['link']}")
                                tweet_content = get_x_tweet_content(result["link"])
                                if tweet_content:
                                    return {
                                        "tweet_url": result["link"],
                                        "content": tweet_content
                                    }

            except Exception as decode_error:
                logger.warning(f"âš ï¸ Snowflake ID ãƒ‡ã‚³ãƒ¼ãƒ‰å¤±æ•—: {decode_error}")

        # æ–¹æ³•3: SerpAPIã§ãƒªãƒãƒ¼ã‚¹ç”»åƒæ¤œç´¢ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        if SERPAPI_KEY and SerpAPI_available:
            logger.info("ğŸ” SerpAPIã§ãƒªãƒãƒ¼ã‚¹ç”»åƒæ¤œç´¢å®Ÿè¡Œä¸­...")
            search = GoogleSearch({
                "engine": "google_reverse_image",
                "image_url": image_url,
                "api_key": SERPAPI_KEY,
                "tbs": "simg",
                "num": 30  # ã‚ˆã‚Šå¤šãã®çµæœã‚’å–å¾—
            })

            results = search.get_dict()
            logger.debug(f"ğŸ” SerpAPIçµæœ: {results}")

            # ã‚ˆã‚Šå¹…åºƒã„æ¤œç´¢çµæœã‚’ãƒã‚§ãƒƒã‚¯
            for key in ['images_results', 'inline_images', 'related_searches']:
                if key in results:
                    for result in results[key][:15]:
                        if isinstance(result, dict) and "link" in result:
                            link = result["link"]
                            if any(domain in link for domain in ['x.com', 'twitter.com']):
                                logger.info(f"ğŸ¦ ãƒªãƒãƒ¼ã‚¹æ¤œç´¢ã§ãƒ„ã‚¤ãƒ¼ãƒˆURLç™ºè¦‹: {link}")
                                tweet_content = get_x_tweet_content(link)
                                if tweet_content:
                                    return {
                                        "tweet_url": link,
                                        "content": tweet_content
                                    }

        # æ–¹æ³•4: é€šå¸¸ã®Googleæ¤œç´¢ã§Twitterå†…ã‚’æ¤œç´¢
        if SERPAPI_KEY and SerpAPI_available:
            logger.info("ğŸ” SerpAPIã§Twitterå†…æ¤œç´¢å®Ÿè¡Œä¸­...")
            search = GoogleSearch({
                "engine": "google",
                "q": f"site:x.com OR site:twitter.com {image_url}",
                "api_key": SERPAPI_KEY,
                "num": 15
            })

            results = search.get_dict()

            if "organic_results" in results:
                for result in results["organic_results"][:8]:
                    if "link" in result and any(domain in result["link"] for domain in ['x.com', 'twitter.com']):
                        logger.info(f"ğŸ¦ ã‚µã‚¤ãƒˆå†…æ¤œç´¢ã§ãƒ„ã‚¤ãƒ¼ãƒˆURLç™ºè¦‹: {result['link']}")
                        tweet_content = get_x_tweet_content(result["link"])
                        if tweet_content:
                            return {
                                "tweet_url": result["link"],
                                "content": tweet_content
                            }

        logger.warning("âš ï¸ ç”»åƒã‹ã‚‰ãƒ„ã‚¤ãƒ¼ãƒˆURLã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return None

    except Exception as e:
        logger.error(f"âŒ ç”»åƒçµŒç”±ãƒ„ã‚¤ãƒ¼ãƒˆURLæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

def get_x_tweet_content_by_image(image_url: str) -> str | None:
    """
    ç”»åƒURLã‹ã‚‰ãƒ„ã‚¤ãƒ¼ãƒˆå†…å®¹ã‚’æ¢ç´¢ã™ã‚‹ï¼ˆé«˜åº¦ç‰ˆï¼‰
    Google Vision API + X API v2 + SerpAPIã‚’çµ„ã¿åˆã‚ã›ã¦ãƒ„ã‚¤ãƒ¼ãƒˆã‚’ç‰¹å®š
    """
    try:
        logger.info(f"ğŸ¦ ç”»åƒURLçµŒç”±ã§ãƒ„ã‚¤ãƒ¼ãƒˆæ¤œç´¢: {image_url}")

        # æ–¹æ³•1: Google Vision APIã®WEB_DETECTIONã‚’ä½¿ç”¨
        if vision_client:
            try:
                logger.info("ğŸ” Google Vision APIã§WEB_DETECTIONå®Ÿè¡Œä¸­...")

                # ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                import httpx
                with httpx.Client(timeout=10.0) as client:
                    response = client.get(image_url)
                    if response.status_code == 200:
                        image_content = response.content

                        # Vision APIå®Ÿè¡Œ
                        from google.cloud import vision
                        image = vision.Image(content=image_content)
                        response = vision_client.web_detection(image=image)  # type: ignore

                        # é–¢é€£ãƒšãƒ¼ã‚¸ã‹ã‚‰ X/Twitter URLã‚’æ¢ç´¢
                        if response.web_detection.pages_with_matching_images:
                            for page in response.web_detection.pages_with_matching_images[:15]:
                                if page.url and any(domain in page.url for domain in ['x.com', 'twitter.com']):
                                    logger.info(f"ğŸ¦ Vision APIã§ãƒ„ã‚¤ãƒ¼ãƒˆURLç™ºè¦‹: {page.url}")
                                    tweet_content = get_x_tweet_content(page.url)
                                    if tweet_content:
                                        return tweet_content

                        # ã‚ˆã‚Šè©³ç´°ãªé–¢é€£ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚‚ãƒã‚§ãƒƒã‚¯
                        if response.web_detection.web_entities:
                            for entity in response.web_detection.web_entities[:10]:
                                if entity.description:
                                    # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®èª¬æ˜ã‹ã‚‰Twitteré–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œç´¢
                                    description = entity.description.lower()
                                    if any(keyword in description for keyword in ['twitter', 'tweet', 'x.com']):
                                        logger.info(f"ğŸ” é–¢é€£ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ç™ºè¦‹: {entity.description}")

                                        # ã“ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ä½¿ã£ã¦ã•ã‚‰ã«æ¤œç´¢
                                        if SERPAPI_KEY and SerpAPI_available:
                                            search = GoogleSearch({
                                                "engine": "google",
                                                "q": f'site:x.com OR site:twitter.com "{entity.description}"',
                                                "api_key": SERPAPI_KEY,
                                                "num": 10
                                            })
                                            entity_results = search.get_dict()
                                            if "organic_results" in entity_results:
                                                for result in entity_results["organic_results"][:3]:
                                                    if "link" in result and any(domain in result["link"] for domain in ['x.com', 'twitter.com']):
                                                        logger.info(f"ğŸ¦ ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æ¤œç´¢ã§ãƒ„ã‚¤ãƒ¼ãƒˆURLç™ºè¦‹: {result['link']}")
                                                        tweet_content = get_x_tweet_content(result["link"])
                                                        if tweet_content:
                                                            return tweet_content

            except Exception as vision_error:
                logger.warning(f"âš ï¸ Vision APIæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {vision_error}")

        # æ–¹æ³•2: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰Snowflake IDã‚’æŠ½å‡ºã—ã¦ãƒ„ã‚¤ãƒ¼ãƒˆIDã‚’æ¨å®š
        import re
        filename_match = re.search(r'/media/([^?]+)', image_url)
        if filename_match:
            filename = filename_match.group(1).split('.')[0]  # æ‹¡å¼µå­ã‚’é™¤å»
            logger.info(f"ğŸ” ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«å: {filename}")

            # Base64URLãƒ‡ã‚³ãƒ¼ãƒ‰ã‚’è©¦è¡Œã—ã¦Snowflake IDã‚’å–å¾—
            try:
                import base64
                from datetime import datetime

                # Twitterã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åã¯é€šå¸¸Base64URLã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸSnowflake ID
                decoded_bytes = base64.urlsafe_b64decode(filename + '==')  # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°è¿½åŠ 
                snowflake_id = int.from_bytes(decoded_bytes, byteorder='big')

                # Snowflake IDã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’è¨ˆç®—ï¼ˆTwitter Epoch: 2010-11-04 01:42:54 UTCï¼‰
                twitter_epoch = 1288834974657  # Twitter epoch in milliseconds
                timestamp_ms = (snowflake_id >> 22) + twitter_epoch
                tweet_datetime = datetime.fromtimestamp(timestamp_ms / 1000)

                logger.info(f"ğŸ“… æ¨å®šæŠ•ç¨¿æ—¥æ™‚: {tweet_datetime}")

                # ã“ã®æƒ…å ±ã‚’ä½¿ã£ã¦ã‚ˆã‚Šç²¾å¯†ãªæ¤œç´¢ã‚’å®Ÿè¡Œ
                if SERPAPI_KEY and SerpAPI_available:
                    date_str = tweet_datetime.strftime("%Y-%m-%d")
                    search = GoogleSearch({
                        "engine": "google",
                        "q": f'site:x.com OR site:twitter.com "{filename}" after:{date_str}',
                        "api_key": SERPAPI_KEY,
                        "num": 15
                    })

                    date_results = search.get_dict()
                    if "organic_results" in date_results:
                        for result in date_results["organic_results"][:5]:
                            if "link" in result and any(domain in result["link"] for domain in ['x.com', 'twitter.com']):
                                logger.info(f"ğŸ¦ æ—¥ä»˜æ¤œç´¢ã§ãƒ„ã‚¤ãƒ¼ãƒˆURLç™ºè¦‹: {result['link']}")
                                tweet_content = get_x_tweet_content(result["link"])
                                if tweet_content:
                                    return tweet_content

            except Exception as decode_error:
                logger.warning(f"âš ï¸ Snowflake ID ãƒ‡ã‚³ãƒ¼ãƒ‰å¤±æ•—: {decode_error}")

        # æ–¹æ³•3: SerpAPIã§ãƒªãƒãƒ¼ã‚¹ç”»åƒæ¤œç´¢ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        if SERPAPI_KEY and SerpAPI_available:
            logger.info("ğŸ” SerpAPIã§ãƒªãƒãƒ¼ã‚¹ç”»åƒæ¤œç´¢å®Ÿè¡Œä¸­...")
            search = GoogleSearch({
                "engine": "google_reverse_image",
                "image_url": image_url,
                "api_key": SERPAPI_KEY,
                "tbs": "simg",
                "num": 30  # ã‚ˆã‚Šå¤šãã®çµæœã‚’å–å¾—
            })

            results = search.get_dict()
            logger.debug(f"ğŸ” SerpAPIçµæœ: {results}")

            # ã‚ˆã‚Šå¹…åºƒã„æ¤œç´¢çµæœã‚’ãƒã‚§ãƒƒã‚¯
            for key in ['images_results', 'inline_images', 'related_searches']:
                if key in results:
                    for result in results[key][:15]:
                        if isinstance(result, dict) and "link" in result:
                            link = result["link"]
                            if any(domain in link for domain in ['x.com', 'twitter.com']):
                                logger.info(f"ğŸ¦ ãƒªãƒãƒ¼ã‚¹æ¤œç´¢ã§ãƒ„ã‚¤ãƒ¼ãƒˆURLç™ºè¦‹: {link}")
                                tweet_content = get_x_tweet_content(link)
                                if tweet_content:
                                    return tweet_content

        # æ–¹æ³•4: é€šå¸¸ã®Googleæ¤œç´¢ã§Twitterå†…ã‚’æ¤œç´¢
        if SERPAPI_KEY and SerpAPI_available:
            logger.info("ğŸ” SerpAPIã§Twitterå†…æ¤œç´¢å®Ÿè¡Œä¸­...")
            search = GoogleSearch({
                "engine": "google",
                "q": f"site:x.com OR site:twitter.com {image_url}",
                "api_key": SERPAPI_KEY,
                "num": 15
            })

            results = search.get_dict()

            if "organic_results" in results:
                for result in results["organic_results"][:8]:
                    if "link" in result and any(domain in result["link"] for domain in ['x.com', 'twitter.com']):
                        logger.info(f"ğŸ¦ ã‚µã‚¤ãƒˆå†…æ¤œç´¢ã§ãƒ„ã‚¤ãƒ¼ãƒˆURLç™ºè¦‹: {result['link']}")
                        tweet_content = get_x_tweet_content(result["link"])
                        if tweet_content:
                            return tweet_content

        logger.warning("âš ï¸ ç”»åƒã‹ã‚‰ãƒ„ã‚¤ãƒ¼ãƒˆå†…å®¹ã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return None

    except Exception as e:
        logger.error(f"âŒ ç”»åƒçµŒç”±ãƒ„ã‚¤ãƒ¼ãƒˆæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

def judge_content_with_gemini(content: str) -> dict:
    """
    Gemini AIã‚’ä½¿ã£ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’åˆ¤å®šã™ã‚‹
    """
    if not gemini_model:
        logger.error("âŒ Gemini ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        return {"judgment": "ï¼", "reason": "AIåˆ¤å®šã‚µãƒ¼ãƒ“ã‚¹ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"}

    logger.info("ğŸ¤– Gemini AIåˆ¤å®šé–‹å§‹")

    try:
        # Twitterç”»åƒã®å ´åˆã®ç‰¹åˆ¥å‡¦ç†
        if content.startswith("TWITTER_IMAGE:"):
            logger.info("ğŸ¦ Twitterç”»åƒURLï¼ˆå†…å®¹å–å¾—ä¸å¯ï¼‰ã®ç‰¹åˆ¥å‡¦ç†")
            return {
                "judgment": "ï¼Ÿ",
                "reason": "Twitterç”»åƒã®ãŸã‚æŠ•ç¨¿å†…å®¹ã‚’ç›´æ¥ç¢ºèªã§ãã¾ã›ã‚“"
            }
        elif content.startswith("TWITTER_IMAGE_UNKNOWN:"):
            logger.info("ğŸ¦ Twitterç”»åƒURLï¼ˆå†…å®¹ä¸æ˜ï¼‰ã®ç‰¹åˆ¥å‡¦ç†")
            return {
                "judgment": "ï¼Ÿ",
                "reason": "Twitterç”»åƒã§ã™ãŒæŠ•ç¨¿å†…å®¹ã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸ"
            }
        elif content.startswith("XæŠ•ç¨¿å†…å®¹"):
            logger.info("ğŸ¦ X APIçµŒç”±ã§å–å¾—ã—ãŸãƒ„ã‚¤ãƒ¼ãƒˆå†…å®¹ã‚’åˆ†æ")
            # å®Ÿéš›ã®ãƒ„ã‚¤ãƒ¼ãƒˆå†…å®¹ãŒã‚ã‚‹ã®ã§ã€é€šå¸¸ã®åˆ¤å®šã‚’ç¶™ç¶š

        # ---------- é«˜ç²¾åº¦åˆ¤å®šç”¨ Gemini ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ----------
        # å®Œå…¨ã«å®‰å…¨ãªå…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒã‚§ãƒƒã‚¯ä¸è¦ï¼‰
        official_domains = [
            # å‡ºç‰ˆç¤¾
            'www.kadokawa.co.jp', 'www.shogakukan.co.jp', 'www.shueisha.co.jp',
            'www.kodansha.co.jp',
            # ãƒ¡ãƒ‡ã‚£ã‚¢
            'www.nhk.or.jp', 'www.asahi.com', 'www.yomiuri.co.jp',
            'www.sankei.com', 'www.nikkei.com', 'mainichi.jp', 'news.yahoo.co.jp',
            # æ›¸åº—ãƒ»EC
            'shop.delivered.co.kr', 'www.deliveredh.shop', 'books.rakuten.co.jp',
            'honto.jp', 'www.kinokuniya.co.jp', '7net.omni7.jp', 'www.7net.omni7.jp',
            'www.hmv.co.jp', 'hmv.co.jp', 'www.tsutaya.co.jp', 'tsutaya.co.jp',
            'www.yodobashi.com', 'yodobashi.com', 'www.biccamera.com', 'biccamera.com',
            'www.tower.jp', 'tower.jp', 'books.shufunotomo.co.jp', 'books.bunka.co.jp'
        ]

        # å…¬å¼ã ãŒå†…å®¹ç¢ºèªãŒå¿…è¦ãªãƒ‰ãƒ¡ã‚¤ãƒ³
        check_required_domains = [
            'amazon.co.jp', 'books.rakuten.co.jp', 'twitter.com', 'x.com',
            'facebook.com', 'instagram.com'
        ]

        # ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯
        current_domain = urlparse(url).netloc if 'url' in locals() else 'N/A'

        # å®Œå…¨å®‰å…¨ãƒ‰ãƒ¡ã‚¤ãƒ³ã®å ´åˆã¯å³åº§ã«å®‰å…¨åˆ¤å®š
        if current_domain in official_domains:
            return {"judgment": "â—‹", "reason": "å…¬å¼ã‚µã‚¤ãƒˆ"}

        prohibited_keywords = [
            'ç„¡æ–™ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰','å…¨å·»ç„¡æ–™','PDF','raw','æ¼«ç”»ãƒãƒ³ã‚¯','æµ·è³Šç‰ˆ','ç„¡æ–­è»¢è¼‰',
            'read online free','download full','crack','leak'
        ]

        # few-shot examples (æ—¥æœ¬èª)
        fewshot = """
### ä¾‹1
URL: https://www.kadokawa.co.jp/book/123456/
æœ¬æ–‡æŠœç²‹: æœ¬å•†å“ã¯KADOKAWAå…¬å¼ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§è³¼å…¥ã§ãã¾ã™ã€‚
â†’ åˆ¤å®š: â—‹ / ç†ç”±: å‡ºç‰ˆç¤¾å…¬å¼

### ä¾‹2
URL: https://pirated-site.example.com/onepiece-all-volumes.pdf
æœ¬æ–‡æŠœç²‹: ãƒ¯ãƒ³ãƒ”ãƒ¼ã‚¹å…¨å·»ã‚’PDFã§ç„¡æ–™ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼
â†’ åˆ¤å®š: Ã— / ç†ç”±: ç„¡æ–™å…¨å·»DL

### ä¾‹3
URL: https://blog.example.com/my-review
æœ¬æ–‡æŠœç²‹: ä½œå“ã®æ„Ÿæƒ³ã¨è³¼å…¥ãƒªãƒ³ã‚¯ã®ã¿æ²è¼‰ã€‚
â†’ åˆ¤å®š: â—‹ / ç†ç”±: ãƒ¬ãƒ“ãƒ¥ãƒ¼è¨˜äº‹

### ä¾‹4
URL: https://unknownsite.xyz/abc
æœ¬æ–‡æŠœç²‹: (æœ¬æ–‡ãŒã»ã¨ã‚“ã©ç„¡ã„ / ç”»åƒã®ã¿)
â†’ åˆ¤å®š: ï¼Ÿ / ç†ç”±: æƒ…å ±ä¸è¶³
        """

        # è¦æ³¨æ„ãƒ‰ãƒ¡ã‚¤ãƒ³ã®å ´åˆã¯ç‰¹åˆ¥ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨
        if current_domain in check_required_domains:
            prompt = f"""
ã‚ãªãŸã¯ãƒ—ãƒ­ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç›£è¦–ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™ã€‚
ã“ã®URLã¯ä¿¡é ¼ã§ãã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆ{current_domain}ï¼‰ã§ã™ãŒã€ãƒ¦ãƒ¼ã‚¶ãƒ¼æŠ•ç¨¿ã‚„å‡ºå“ç‰©ã«
æµ·è³Šç‰ˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒå«ã¾ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€å†…å®¹ã®è©³ç´°ãƒã‚§ãƒƒã‚¯ãŒå¿…è¦ã§ã™ã€‚

ä»¥ä¸‹ã®æƒ…å ±ã‚’å‚è€ƒã«åˆ¤å®šã—ã¦ãã ã•ã„ï¼š
URL: {url if 'url' in locals() else 'N/A'}
ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠœç²‹:
{content[:3000]}

ç‰¹ã«ä»¥ä¸‹ã®ç‚¹ã«æ³¨æ„ã—ã¦ãƒã‚§ãƒƒã‚¯:
1. SNSã®å ´åˆ: æµ·è³Šç‰ˆã¸ã®ãƒªãƒ³ã‚¯å…±æœ‰ã€é•æ³•DLã®å‘ŠçŸ¥
2. ECã‚µã‚¤ãƒˆã®å ´åˆ: éæ­£è¦å“ã€ãƒ‡ã‚¸ã‚¿ãƒ«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ç„¡æ–­è»¢è¼‰
3. æŠ•ç¨¿å†…å®¹ã«ç¦æ­¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹ã‹: {', '.join(prohibited_keywords)}

å‡ºåŠ›ã¯å¿…ãš1è¡Œã§ `åˆ¤å®š:â— ç†ç”±:â–³â–³` ã®å½¢å¼ã®ã¿ã€‚ç†ç”±ã¯20å­—ä»¥å†…ã€‚
"""
        else:
            # é€šå¸¸ã®åˆ¤å®šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            prompt = f"""
ã‚ãªãŸã¯ãƒ—ãƒ­ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç›£è¦–ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™ã€‚ä»¥ä¸‹ã®æƒ…å ±ã‚’å‚è€ƒã«ã€
Webãƒšãƒ¼ã‚¸ãŒã€æµ·è³Šç‰ˆï¼ˆÃ—ï¼‰ã€ã€å®‰å…¨ï¼ˆâ—‹ï¼‰ã€ã€åˆ¤æ–­ä¸èƒ½ï¼ˆï¼Ÿï¼‰ã€ã€ã‚¨ãƒ©ãƒ¼ï¼ˆï¼ï¼‰ã€ã®ã©ã‚Œã‹ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

å…¥åŠ›æƒ…å ±:
URL: {url if 'url' in locals() else 'N/A'}
ãƒ‰ãƒ¡ã‚¤ãƒ³: {current_domain}
ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠœç²‹:
{content[:3000]}

åˆ¤å®šåŸºæº–:
1. ä»¥ä¸‹ã¯å³åº§ã«æµ·è³Šç‰ˆåˆ¤å®š:
   - å…¨æ–‡æ²è¼‰ãƒ»PDFç›´ãƒªãƒ³ã‚¯
   - raw/MOBI/EPUBå…±æœ‰
   - ç¦æ­¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(prohibited_keywords)}

2. ä»¥ä¸‹ã¯å®‰å…¨ã¨åˆ¤å®š:
   - å…¬å¼ECã‚µã‚¤ãƒˆï¼ˆå•†å“ãƒšãƒ¼ã‚¸ã®ã¿ï¼‰
   - å‡ºç‰ˆç¤¾å…¬å¼
   - æ›¸è©•ãƒ»ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆå¼•ç”¨ãŒé©åˆ‡ãªç¯„å›²ï¼‰
   - ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹

3. ä»¥ä¸‹ã¯åˆ¤æ–­ä¸èƒ½ï¼ˆï¼Ÿï¼‰:
   - æƒ…å ±ãŒæ¥µç«¯ã«å°‘ãªã„
   - ç”»åƒã®ã¿
   - ã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™ã§æœ¬æ–‡å–å¾—ä¸å¯

4. ä»¥ä¸‹ã¯ã‚¨ãƒ©ãƒ¼ï¼ˆï¼ï¼‰:
   - å‡¦ç†ã‚¨ãƒ©ãƒ¼
   - ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
   - ç„¡åŠ¹ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹

å‡ºåŠ›ã¯å¿…ãš1è¡Œã§ `åˆ¤å®š:â— ç†ç”±:â–³â–³` ã®å½¢å¼ã®ã¿ã€‚ç†ç”±ã¯20å­—ä»¥å†…ã€‚

{fewshot}
---
å‡ºåŠ›ä¾‹: `åˆ¤å®š:â—‹ ç†ç”±:å‡ºç‰ˆç¤¾å…¬å¼`
---
        """

        response = gemini_model.generate_content(prompt)
        result_text = response.text.strip()

        logger.info(f"ğŸ“‹ Geminiå¿œç­”: {result_text}")

        # å¿œç­”ã‚’è§£æ
        lines = result_text.strip().split('\n')
        judgment = "ï¼Ÿ"
        reason = "å¿œç­”è§£æå¤±æ•—"

        # æ–°ã—ã„è§£æãƒ­ã‚¸ãƒƒã‚¯ï¼šä¸€è¡Œå½¢å¼ "åˆ¤å®š:â—‹ ç†ç”±:â–³â–³" ã«å¯¾å¿œ
        import re

        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ä¸€è¡Œå½¢å¼ "åˆ¤å®š:â—‹ ç†ç”±:â–³â–³"
        match = re.search(r'åˆ¤å®š[:ï¼š]([â—‹Ã—ï¼Ÿï¼])\s*ç†ç”±[:ï¼š](.+)', result_text)
        if match:
            judgment = match.group(1).strip()
            reason = match.group(2).strip()
        else:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³2: è¤‡æ•°è¡Œå½¢å¼ï¼ˆå¾“æ¥ï¼‰
            for line in lines:
                line = line.strip()
                if 'åˆ¤å®šï¼š' in line or 'åˆ¤å®š:' in line:
                    judgment_part = line.split('ï¼š')[1] if 'ï¼š' in line else line.split(':')[1]
                    judgment = judgment_part.replace('[','').replace(']','').strip()
                    if judgment not in ['â—‹', 'Ã—', 'ï¼Ÿ', 'ï¼']:
                        judgment = "ï¼Ÿ"
                elif 'ç†ç”±ï¼š' in line or 'ç†ç”±:' in line:
                    reason_part = line.split('ï¼š')[1] if 'ï¼š' in line else line.split(':')[1]
                    reason = reason_part.replace('[','').replace(']','').strip()

        logger.info(f"âœ… Geminiåˆ¤å®šå®Œäº†: {judgment} - {reason}")
        return {"judgment": judgment, "reason": reason}

    except Exception as e:
        error_msg = str(e)
        logger.error(f"âŒ Gemini API ã‚¨ãƒ©ãƒ¼: {error_msg}")

        # ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸåˆ†é¡
        if "not found" in error_msg.lower():
            return {"judgment": "ï¼", "reason": "AIãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}
        elif "quota" in error_msg.lower() or "limit" in error_msg.lower():
            return {"judgment": "ï¼", "reason": "APIåˆ©ç”¨åˆ¶é™ã«é”ã—ã¾ã—ãŸ"}
        elif "auth" in error_msg.lower() or "permission" in error_msg.lower():
            return {"judgment": "ï¼", "reason": "APIèªè¨¼ã‚¨ãƒ©ãƒ¼ã§ã™"}
        elif "network" in error_msg.lower() or "timeout" in error_msg.lower():
            return {"judgment": "ï¼", "reason": "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã§ã™"}
        else:
            return {"judgment": "ï¼Ÿ", "reason": "AIåˆ¤å®šå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ"}

def analyze_url_efficiently(url: str) -> Optional[Dict]:
    """
    URLã‚’åŠ¹ç‡çš„ã«åˆ†æã™ã‚‹
    1. ä¿¡é ¼ã§ãã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã¯äº‹å‰ã«â—‹åˆ¤å®š
    2. Twitterç”»åƒURLã¯ç‰¹åˆ¥å‡¦ç†
    3. ãã®ä»–ã¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°â†’Geminiåˆ¤å®š
    """
    logger.info(f"ğŸ”„ URLåˆ†æé–‹å§‹: {url}")

    # 1. ä¿¡é ¼ã§ãã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»å‡ºç‰ˆç³»ãƒ‰ãƒ¡ã‚¤ãƒ³ã®äº‹å‰ãƒã‚§ãƒƒã‚¯
    if is_trusted_news_domain(url):
        logger.info(f"âœ… ä¿¡é ¼ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãŸã‚äº‹å‰â—‹åˆ¤å®š: {url}")
        return {
            "url": url,
            "judgment": "â—‹",
            "reason": "ä¿¡é ¼ã§ãã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»å‡ºç‰ˆã‚µã‚¤ãƒˆ"
        }

    # 2. Twitterç”»åƒURLã®ç‰¹åˆ¥å‡¦ç†
    twitter_result = convert_twitter_image_to_tweet_url(url)
    if twitter_result:
        if twitter_result["tweet_url"]:
            # å…ƒã®ãƒ„ã‚¤ãƒ¼ãƒˆURLãŒç‰¹å®šã§ããŸå ´åˆã€ãã®URLã§çµæœã‚’è¿”ã™
            judgment_result = judge_content_with_gemini(twitter_result["content"])
            return {
                "url": twitter_result["tweet_url"],  # å…ƒã®ãƒ„ã‚¤ãƒ¼ãƒˆURLã‚’ä½¿ç”¨
                "judgment": judgment_result["judgment"],
                "reason": judgment_result["reason"]
            }
        else:
            # ãƒ„ã‚¤ãƒ¼ãƒˆURLãŒç‰¹å®šã§ããªã‹ã£ãŸå ´åˆã¯å¾“æ¥é€šã‚Š
            judgment_result = judge_content_with_gemini(twitter_result["content"])
            return {
                "url": url,
                "judgment": judgment_result["judgment"],
                "reason": judgment_result["reason"]
            }

    # 3. é€šå¸¸ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°â†’Geminiåˆ¤å®š
    scraped_content = scrape_page_content(url)
    if not scraped_content:
        logger.info(f"  âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—: {url}")
        return None

    judgment_result = judge_content_with_gemini(scraped_content)
    logger.info(f"  âœ… åˆ†æå®Œäº†: {judgment_result['judgment']} - {judgment_result['reason']}")

    return {
        "url": url,
        "judgment": judgment_result["judgment"],
        "reason": judgment_result["reason"]
    }

def scrape_page_content(url: str) -> str | None:
    # 1. æ‹¡å¼µå­ã¨ãƒ‰ãƒ¡ã‚¤ãƒ³ã§ç°¡æ˜“ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp']
    if any(url.lower().endswith(ext) for ext in image_extensions):
        logger.info(f"â­ï¸  ç”»åƒæ‹¡å¼µå­ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {url}")
        return None

    image_host_domains = ['m.media-amazon.com', 'img-cdn.theqoo.net']
    if any(domain in url for domain in image_host_domains):
        logger.info(f"â­ï¸  ç”»åƒãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {url}")
        return None

    # Twitterç”»åƒURLã¯ç‰¹åˆ¥å‡¦ç†ï¼ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
    if 'pbs.twimg.com' in url:
        logger.info(f"ğŸ¦ Twitterç”»åƒURLæ¤œå‡ºã®ãŸã‚ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¹ã‚­ãƒƒãƒ—: {url}")
        return None

    logger.info(f"ğŸŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {url}")
    try:
        with httpx.Client(timeout=10.0, follow_redirects=True) as client:
            # 2. Content-Typeã‚’äº‹å‰ç¢ºèª
            try:
                head_response = client.head(url, headers={'User-Agent': 'Mozilla/5.0'})
                content_type = head_response.headers.get('content-type', '').lower()
                if 'text/html' not in content_type:
                    logger.info(f"â­ï¸  HTMLã§ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ— (Content-Type: {content_type}): {url}")
                    return None
            except httpx.RequestError as e:
                logger.warning(f"âš ï¸ HEADãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•— (GETã§ç¶šè¡Œ): {e}")

            # 3. GETãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—
            response = client.get(url, headers={'User-Agent': 'Mozilla/5.0'})
            response.raise_for_status()

        # 4. BeautifulSoupã§è§£æ
        soup = BeautifulSoup(response.text, 'html.parser')
        title = soup.title.string if soup.title else ""
        body_text = " ".join([p.get_text() for p in soup.find_all('p', limit=5)])

        content = f"Title: {title.strip()}\\n\\nBody: {body_text.strip()}"
        logger.info(f"ğŸ“ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: {len(content)} chars")
        return content

    except httpx.HTTPStatusError as e:
        logger.error(f"âŒ HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚¨ãƒ©ãƒ¼ {url}: {e.response.status_code} {e.response.reason_phrase}")
        return None
    except Exception as e:
        logger.error(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸€èˆ¬ã‚¨ãƒ©ãƒ¼ {url}: {e}")
        return None



# analyze_domainé–¢æ•°ã¯å‰Šé™¤ï¼ˆVision API + Geminiåˆ¤å®šã‚’ä½¿ç”¨ï¼‰



# ä¸è¦ãªé–¢æ•°ã¯å‰Šé™¤ã•ã‚Œã¾ã—ãŸ

# Google Custom Search APIé–¢æ•°ã¯å‰Šé™¤ï¼ˆVision API WEB_DETECTIONã‚’ä½¿ç”¨ï¼‰

# ç”»åƒç‰¹å¾´ãƒ™ãƒ¼ã‚¹æ¤œç´¢é–¢æ•°ã¯å‰Šé™¤ï¼ˆVision API WEB_DETECTIONã‚’ä½¿ç”¨ï¼‰

# SerpAPIé–¢é€£ã®é–¢æ•°ã¯å‰Šé™¤ï¼ˆVision API WEB_DETECTIONã‚’ä½¿ç”¨ï¼‰

@app.get("/")
async def root():
    return {
        "message": "Book Leak Detector API",
        "api_keys": {
            "gemini_api_key_configured": GEMINI_API_KEY is not None,
            "google_vision_api_configured": GOOGLE_APPLICATION_CREDENTIALS is not None
        },
        "upload_count": len(upload_records),
        "search_results_count": len(search_results)
    }

@app.post("/upload")
async def upload_image(file: UploadFile = File(...)):
    """ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ä¿å­˜ã™ã‚‹"""

    logger.info(f"ğŸ“¤ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {file.filename}, content_type: {file.content_type}")

    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼
        if not validate_file(file):
            allowed_types = ["image/jpeg", "image/png", "image/jpg", "image/gif", "image/webp"]
            if PDF_SUPPORT:
                allowed_types.append("application/pdf")

            logger.error(f"âŒ ç„¡åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {file.content_type}")
            raise HTTPException(
                status_code=400,
                detail={
                    "error": "invalid_file_format",
                    "message": "ç„¡åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™ã€‚JPEGã€PNGã€GIFã€WebPã€PDFå½¢å¼ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚" if PDF_SUPPORT else "ç„¡åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™ã€‚JPEGã€PNGã€GIFã€WebPå½¢å¼ã®ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
                    "allowed_types": allowed_types,
                    "received_type": file.content_type
                }
            )

        logger.info("âœ… ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼æ¤œè¨¼OK")

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™ï¼ˆ10MBï¼‰
        content = await file.read()
        file_size_mb = len(content) / (1024 * 1024)
        logger.info(f"ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size_mb:.2f}MB")

        if len(content) > 10 * 1024 * 1024:
            logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã¾ã™: {file_size_mb:.2f}MB")
            raise HTTPException(
                status_code=413,
                detail={
                    "error": "file_too_large",
                    "message": "ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã¾ã™ã€‚10MBä»¥ä¸‹ã®ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
                    "file_size_mb": file_size_mb,
                    "max_size_mb": 10
                }
            )

        # ãƒ•ã‚¡ã‚¤ãƒ«ç¨®åˆ¥ã«ã‚ˆã‚‹æ¤œè¨¼
        is_pdf = is_pdf_file(file.content_type, file.filename or "")

        if is_pdf:
            # PDFæ¤œè¨¼
            if not PDF_SUPPORT:
                raise HTTPException(
                    status_code=400,
                    detail={
                        "error": "pdf_not_supported",
                        "message": "PDFå‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚",
                        "install_instruction": "pip install PyMuPDF ã¾ãŸã¯ pip install pdf2image PyPDF2"
                    }
                )

            try:
                # PDFã®æœ‰åŠ¹æ€§ã‚’ç¢ºèª
                test_images = convert_pdf_to_images(content)
                if not test_images:
                    raise Exception("PDFã‹ã‚‰ç”»åƒã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
                logger.info(f"âœ… PDFæœ‰åŠ¹æ€§æ¤œè¨¼OK ({len(test_images)}ãƒšãƒ¼ã‚¸)")
            except Exception as e:
                logger.error(f"âŒ PDFæ¤œè¨¼å¤±æ•—: {str(e)}")
                raise HTTPException(
                    status_code=400,
                    detail={
                        "error": "corrupted_pdf",
                        "message": "ç ´æã—ãŸPDFãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ã€‚æœ‰åŠ¹ãªPDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
                        "validation_error": str(e)
                    }
                )
        else:
            # ç”»åƒæ¤œè¨¼
            try:
                image = Image.open(BytesIO(content))
                image.verify()
                logger.info("âœ… ç”»åƒæœ‰åŠ¹æ€§æ¤œè¨¼OK")
            except Exception as e:
                logger.error(f"âŒ ç”»åƒæ¤œè¨¼å¤±æ•—: {str(e)}")
                raise HTTPException(
                    status_code=400,
                    detail={
                        "error": "corrupted_image",
                        "message": "ç ´æã—ãŸç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ã€‚æœ‰åŠ¹ãªç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
                        "validation_error": str(e)
                    }
                )

        # ä¸€æ„ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        file_id = str(uuid.uuid4())
        file_extension = os.path.splitext(file.filename or "image")[1].lower() or ".jpg"
        safe_filename = f"{file_id}{file_extension}"
        file_path = os.path.join(UPLOAD_DIR, safe_filename)

        logger.info(f"ğŸ’¾ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜é–‹å§‹: {file_path}")

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        try:
            with open(file_path, "wb") as f:
                f.write(content)
            logger.info("âœ… ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å¤±æ•—: {str(e)}")
            raise HTTPException(
                status_code=500,
                detail={
                    "error": "file_save_failed",
                    "message": f"ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}",
                    "file_path": file_path
                }
            )

        # è¨˜éŒ²ã‚’ä¿å­˜
        upload_record = {
            "id": file_id,
            "original_filename": file.filename,
            "saved_filename": safe_filename,
            "file_path": file_path,
            "content_type": file.content_type,
            "file_size": len(content),
            "upload_time": datetime.now().isoformat(),
            "status": "uploaded",
            "file_type": "pdf" if is_pdf else "image"
        }

        upload_records[file_id] = upload_record
        save_records()

        logger.info(f"âœ… ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: file_id={file_id}")

        return {
            "success": True,
            "file_id": file_id,
            "original_filename": file.filename,
            "saved_filename": safe_filename,
            "file_size": len(content),
            "upload_time": upload_record["upload_time"],
            "file_url": f"/uploads/{safe_filename}"
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": "unexpected_error",
                "message": f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            }
        )

@app.get("/uploads/history")
async def get_upload_history():
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å±¥æ­´ã‚’å–å¾—ã™ã‚‹"""
    # æ—¥ä»˜é †ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„ã‚‚ã®ãŒæœ€åˆï¼‰
    sorted_records = sorted(
        upload_records.values(),
        key=lambda x: x["upload_time"],
        reverse=True
    )

    return {
        "success": True,
        "count": len(sorted_records),
        "uploads": sorted_records
    }

@app.get("/uploads/{file_id}")
async def get_upload_details(file_id: str):
    """ç‰¹å®šã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°ã‚’å–å¾—ã™ã‚‹"""
    if file_id not in upload_records:
        raise HTTPException(
            status_code=404,
            detail="æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        )

    record = upload_records[file_id]

    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    if not os.path.exists(record["file_path"]):
        record["status"] = "file_missing"
        save_records()

    return {
        "success": True,
        "file": record
    }

@app.delete("/uploads/{file_id}")
async def delete_upload(file_id: str):
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã™ã‚‹"""
    if file_id not in upload_records:
        raise HTTPException(
            status_code=404,
            detail="æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        )

    record = upload_records[file_id]

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
    try:
        if os.path.exists(record["file_path"]):
            os.remove(record["file_path"])
    except Exception as e:
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")

    # è¨˜éŒ²ã‹ã‚‰å‰Šé™¤
    del upload_records[file_id]
    save_records()

    return {
        "success": True,
        "message": f"ãƒ•ã‚¡ã‚¤ãƒ« {record['original_filename']} ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚"
    }

@app.get("/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {
        "status": "healthy",
        "api_keys": {
            "gemini_api_key_configured": GEMINI_API_KEY is not None,
            "google_vision_api_configured": GOOGLE_APPLICATION_CREDENTIALS is not None
        },
        "system": {
            "upload_directory_exists": os.path.exists(UPLOAD_DIR),
            "records_file_exists": os.path.exists(RECORDS_FILE),
            "total_uploads": len(upload_records),
            "total_search_results": len(search_results)
        }
    }

@app.post("/search/{image_id}")
async def analyze_image(image_id: str):
    """æŒ‡å®šã•ã‚ŒãŸç”»åƒIDã«å¯¾ã—ã¦Webæ¤œç´¢ã‚’å®Ÿè¡Œã—ã€é¡ä¼¼ç”»åƒã®URLãƒªã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹"""

    logger.info(f"ğŸ” Webç”»åƒæ¤œç´¢é–‹å§‹: image_id={image_id}")

    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è¨˜éŒ²ã‚’ç¢ºèª
    if image_id not in upload_records:
        logger.error(f"âŒ image_id not found: {image_id}")
        raise HTTPException(
            status_code=404,
            detail={
                "error": "image_not_found",
                "message": "æŒ‡å®šã•ã‚ŒãŸimage_idãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚",
                "image_id": image_id
            }
        )

    record = upload_records[image_id]
    file_path = record["file_path"]
    file_type = record.get("file_type", "image")

    logger.info(f"ğŸ“ æ¤œç´¢å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {file_path} (type: {file_type})")

    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã„ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’èª­ã¿è¾¼ã‚€
        with open(file_path, 'rb') as file:
            file_content = file.read()

        logger.info(f"ğŸ“¸ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {len(file_content)} bytes")

        # ãƒ•ã‚¡ã‚¤ãƒ«ç¨®åˆ¥ã«å¿œã˜ã¦å‡¦ç†ã‚’åˆ†å²
        if file_type == "pdf":
            # PDFã®å ´åˆï¼šå„ãƒšãƒ¼ã‚¸ã‚’ç”»åƒã«å¤‰æ›ã—ã¦å‡¦ç†
            logger.info("ğŸ“„ PDFå‡¦ç†é–‹å§‹...")

            pdf_images = convert_pdf_to_images(file_content)
            if not pdf_images:
                raise Exception("PDFã‹ã‚‰ç”»åƒã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")

            logger.info(f"ğŸ“„ PDFå‡¦ç†å®Œäº†: {len(pdf_images)}ãƒšãƒ¼ã‚¸ã‚’æŠ½å‡º")

            # å„ãƒšãƒ¼ã‚¸ã®ç”»åƒãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—ï¼ˆæœ€åˆã®ãƒšãƒ¼ã‚¸ã‚’ãƒ¡ã‚¤ãƒ³ãƒãƒƒã‚·ãƒ¥ã¨ã™ã‚‹ï¼‰
            image_hash = calculate_image_hash(pdf_images[0])
            logger.info(f"ğŸ”‘ ç”»åƒãƒãƒƒã‚·ãƒ¥è¨ˆç®—å®Œäº†ï¼ˆãƒšãƒ¼ã‚¸1ï¼‰: {image_hash[:16]}...")

            # å„ãƒšãƒ¼ã‚¸ã‚’å€‹åˆ¥ã«åˆ†æ
            all_url_lists = []
            for i, page_image_content in enumerate(pdf_images):
                logger.info(f"ğŸŒ ãƒšãƒ¼ã‚¸ {i+1} ã® Google Vision API WEB_DETECTIONå®Ÿè¡Œä¸­...")
                page_urls = search_web_for_image(page_image_content)
                all_url_lists.extend(page_urls)
                logger.info(f"âœ… ãƒšãƒ¼ã‚¸ {i+1} Webæ¤œç´¢å®Œäº†: {len(page_urls)}ä»¶ã®URLã‚’ç™ºè¦‹")

            # é‡è¤‡URLã‚’é™¤å»
            url_list = list(dict.fromkeys(all_url_lists))  # é †åºã‚’ä¿æŒã—ã¤ã¤é‡è¤‡é™¤å»
            logger.info(f"ğŸ“‹ å…¨ãƒšãƒ¼ã‚¸çµ±åˆçµæœ: {len(url_list)}ä»¶ã®ä¸€æ„ãªURLã‚’ç™ºè¦‹")

        else:
            # ç”»åƒã®å ´åˆï¼šå¾“æ¥ã®å‡¦ç†
            image_content = file_content

            # ç”»åƒãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—
            image_hash = calculate_image_hash(image_content)
            logger.info(f"ğŸ”‘ ç”»åƒãƒãƒƒã‚·ãƒ¥è¨ˆç®—å®Œäº†: {image_hash[:16]}...")

            # Google Vision API WEB_DETECTIONã§URLæ¤œç´¢
            logger.info("ğŸŒ Google Vision API WEB_DETECTIONå®Ÿè¡Œä¸­...")
            url_list = search_web_for_image(image_content)
            logger.info(f"âœ… Webæ¤œç´¢å®Œäº†: {len(url_list)}ä»¶ã®URLã‚’ç™ºè¦‹")

        # å„URLã‚’åŠ¹ç‡çš„ã«åˆ†æï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã¯äº‹å‰â—‹åˆ¤å®šã€Twitterã¯ç‰¹åˆ¥å‡¦ç†ï¼‰
        processed_results = []

        for i, url in enumerate(url_list[:15]):  # PDFã®å ´åˆã¯æœ€å¤§15ä»¶ã«æ‹¡å¼µ
            logger.info(f"ğŸ”„ URLå‡¦ç†ä¸­ ({i+1}/{min(len(url_list), 15)}): {url}")

            # åŠ¹ç‡çš„ãªåˆ†æå®Ÿè¡Œ
            result = analyze_url_efficiently(url)

            if result:
                processed_results.append(result)
                logger.info(f"  âœ… å‡¦ç†å®Œäº†: {result['judgment']} - {result['reason']}")
            else:
                # åˆ†æå¤±æ•—æ™‚
                processed_results.append({
                    "url": url,
                    "judgment": "ï¼Ÿ",
                    "reason": "åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ"
                })
                logger.info(f"  âŒ åˆ†æå¤±æ•—: {url}")

        # æœ€çµ‚çµæœã‚’ä¿å­˜
        search_results[image_id] = processed_results

        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è¨˜éŒ²ã‚’æ›´æ–°
        record["analysis_status"] = "completed"
        record["analysis_time"] = datetime.now().isoformat()
        record["found_urls_count"] = len(url_list)
        record["processed_results_count"] = len(processed_results)
        record["image_hash"] = image_hash
        save_records()

        # å±¥æ­´ã«ä¿å­˜
        save_analysis_to_history(image_id, image_hash, processed_results)

        logger.info(f"âœ… åˆ†æå®Œäº†: image_id={image_id}, URLç™ºè¦‹={len(url_list)}ä»¶, å‡¦ç†å®Œäº†={len(processed_results)}ä»¶")

        return {
            "success": True,
            "image_id": image_id,
            "found_urls_count": len(url_list),
            "processed_results_count": len(processed_results),
            "results": processed_results,
            "analysis_time": record["analysis_time"],
            "message": f"Webæ¤œç´¢ãƒ»åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚{len(url_list)}ä»¶ã®URLãŒè¦‹ã¤ã‹ã‚Šã€{len(processed_results)}ä»¶ã‚’åˆ†æã—ã¾ã—ãŸã€‚"
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Webæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")

        # ã‚¨ãƒ©ãƒ¼çŠ¶æ…‹ã‚’è¨˜éŒ²
        record["analysis_status"] = "failed"
        record["analysis_error"] = str(e)
        record["analysis_time"] = datetime.now().isoformat()
        save_records()

        raise HTTPException(
            status_code=500,
            detail={
                "error": "search_failed",
                "message": f"Webæ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
                "image_id": image_id
            }
        )

@app.get("/results")
async def get_all_results():
    """ã™ã¹ã¦ã®æ¤œç´¢çµæœã‚’å–å¾—ã™ã‚‹"""
    return {
        "success": True,
        "total_searches": len(search_results),
        "results": search_results
    }

@app.get("/results/{image_id}")
async def get_search_results(image_id: str):
    """ç‰¹å®šã®image_idã®æ¤œç´¢çµæœã‚’å–å¾—ã™ã‚‹"""
    if image_id not in upload_records:
        raise HTTPException(
            status_code=404,
            detail={"error": "image_not_found", "message": "æŒ‡å®šã•ã‚ŒãŸimage_idã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è¨˜éŒ²ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"}
        )

    record = upload_records[image_id]

    # åˆ†æãŒã¾ã ã€ã¾ãŸã¯å¤±æ•—ã—ã¦ã„ã‚‹å ´åˆ
    if record.get("analysis_status") != "completed":
        return {
            "success": True,
            "image_id": image_id,
            "analysis_status": record.get("analysis_status", "not_started"),
            "message": "åˆ†æãŒå®Œäº†ã—ã¦ã„ã¾ã›ã‚“ã€‚å…ˆã«åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚",
            "details": record.get("analysis_error")
        }

    # åˆ†æã¯å®Œäº†ã—ãŸãŒã€æœ‰åŠ¹ãªçµæœãŒ0ä»¶ã ã£ãŸå ´åˆ
    if record.get("processed_results_count", 0) == 0:
        return {
            "success": True,
            "image_id": image_id,
            "analysis_status": "completed_no_results",
            "message": "åˆ†æã¯å®Œäº†ã—ã¾ã—ãŸãŒã€æœ‰åŠ¹ãªWebãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚",
            "found_urls_count": record.get("found_urls_count", 0),
            "processed_results_count": 0,
            "results": []
        }

    # æ­£å¸¸ãªçµæœã‚’è¿”ã™
    return {
        "success": True,
        "image_id": image_id,
        "analysis_status": "completed",
        "original_filename": record.get("original_filename", "ä¸æ˜"),
        "analysis_time": record.get("analysis_time", "ä¸æ˜"),
        "found_urls_count": record.get("found_urls_count", 0),
        "processed_results_count": record.get("processed_results_count", 0),
        "results": search_results.get(image_id, [])
    }

# ãƒ†ã‚¹ãƒˆç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.get("/test-search")
async def test_search():
    """ãƒ€ãƒŸãƒ¼ç”»åƒã§æ¤œç´¢ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã™ã‚‹"""
    logger.info("ğŸ§ª ãƒ†ã‚¹ãƒˆæ¤œç´¢é–‹å§‹")

    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
    test_image_id = "test-" + str(uuid.uuid4())

    # ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ€ãƒŸãƒ¼çµæœ
    dummy_results = [
        {
            "url": "https://amazon.co.jp/test-book",
            "domain": "amazon.co.jp",
            "title": "ãƒ†ã‚¹ãƒˆæ›¸ç± - Amazon",
            "source": "Amazon Japan",
            "is_official": True,
            "threat_level": "safe",
            "detailed_analysis": {
                "status": "safe",
                "reason": "å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ã§ã™",
                "content_analysis": None
            },
            "thumbnail": "https://example.com/thumb.jpg",
            "analysis_timestamp": datetime.now().isoformat()
        },
        {
            "url": "https://suspicious-site.com/free-download",
            "domain": "suspicious-site.com",
            "title": "ç„¡æ–™ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ - ç–‘ã‚ã—ã„ã‚µã‚¤ãƒˆ",
            "source": "Suspicious Site",
            "is_official": False,
            "threat_level": "suspicious",
            "detailed_analysis": {
                "status": "suspicious",
                "reason": "ç–‘ã‚ã—ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: ç„¡æ–™ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                "content_analysis": "åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆï¼ˆä¸€éƒ¨ï¼‰: ç„¡æ–™ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¯ã“ã¡ã‚‰..."
            },
            "thumbnail": "https://example.com/thumb2.jpg",
            "analysis_timestamp": datetime.now().isoformat()
        }
    ]

    # ãƒ†ã‚¹ãƒˆçµæœã‚’ãƒ¡ãƒ¢ãƒªã«ä¿å­˜
    search_results[test_image_id] = dummy_results

    logger.info(f"âœ… ãƒ†ã‚¹ãƒˆæ¤œç´¢å®Œäº†: {len(dummy_results)}ä»¶ã®çµæœ")

    return {
        "success": True,
        "test_image_id": test_image_id,
        "results_count": len(dummy_results),
        "message": f"ãƒ†ã‚¹ãƒˆæ¤œç´¢ãŒå®Œäº†ã—ã¾ã—ãŸã€‚{len(dummy_results)}ä»¶ã®çµæœãŒã‚ã‚Šã¾ã™ã€‚",
        "test_results": dummy_results
    }

@app.get("/test-domain/{domain}")
async def test_domain_analysis(domain: str):
    """æŒ‡å®šã•ã‚ŒãŸãƒ‰ãƒ¡ã‚¤ãƒ³ã®åˆ¤å®šãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã™ã‚‹ï¼ˆæ–°ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å¯¾å¿œï¼‰"""
    logger.info(f"ğŸ§ª ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆé–‹å§‹: {domain}")

    # ãƒ†ã‚¹ãƒˆç”¨URL
    test_url = f"https://{domain}"

    try:
        # ãƒšãƒ¼ã‚¸å†…å®¹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        content = scrape_page_content(test_url)

        if content:
            # Geminiã§åˆ¤å®š
            result = judge_content_with_gemini(content)

            logger.info(f"âœ… ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆå®Œäº†: {domain} -> {result['judgment']}")

            return {
                "success": True,
                "domain": domain,
                "test_url": test_url,
                "judgment": result['judgment'],
                "reason": result['reason'],
                "scraped_content_length": len(content),
                "test_time": datetime.now().isoformat()
            }
        else:
            logger.warning(f"âš ï¸ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—: {domain}")
            return {
                "success": False,
                "domain": domain,
                "error": "ãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ",
                "test_time": datetime.now().isoformat()
            }

    except Exception as e:
        logger.error(f"âŒ ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {
            "success": False,
            "domain": domain,
            "error": str(e),
            "test_time": datetime.now().isoformat()
        }

@app.get("/debug/logs")
async def get_debug_info():
    """ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å–å¾—ã™ã‚‹"""
    return {
        "system_status": "running",
        "total_uploads": len(upload_records),
        "total_search_results": len(search_results),
        "recent_uploads": list(upload_records.keys())[-5:] if upload_records else [],
        "api_keys_status": {
            "gemini_api_key": GEMINI_API_KEY is not None,
            "google_vision_api": GOOGLE_APPLICATION_CREDENTIALS is not None
        },
        "vision_api_status": "active",
        "timestamp": datetime.now().isoformat()
    }

@app.get("/logs")
async def get_system_logs():
    """ã‚·ã‚¹ãƒ†ãƒ ãƒ­ã‚°ã‚’å–å¾—ã™ã‚‹"""
    logger.info(f"ğŸ“‹ ãƒ­ã‚°å–å¾—è¦æ±‚: {len(system_logs)}ä»¶ã®ãƒ­ã‚°")
    return {
        "success": True,
        "total_logs": len(system_logs),
        "logs": system_logs[-50:],  # æœ€æ–°50ä»¶ã‚’è¿”ã™
        "timestamp": datetime.now().isoformat()
    }

def generate_evidence_hash(data: dict) -> str:
    """
    è¨¼æ‹ ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’ç”Ÿæˆï¼ˆæ”¹ã–ã‚“é˜²æ­¢ç”¨ï¼‰
    """
    # ãƒ‡ãƒ¼ã‚¿ã‚’æ–‡å­—åˆ—ã¨ã—ã¦æ­£è¦åŒ–ã—ã¦ãƒãƒƒã‚·ãƒ¥åŒ–
    json_str = json.dumps(data, sort_keys=True, ensure_ascii=False)
    return hashlib.sha256(json_str.encode('utf-8')).hexdigest()

def create_evidence_data(image_id: str) -> dict:
    """
    è¨¼æ‹ ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã™ã‚‹
    """
    if image_id not in upload_records:
        raise HTTPException(
            status_code=404,
            detail="æŒ‡å®šã•ã‚ŒãŸimage_idãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        )

    if image_id not in search_results:
        raise HTTPException(
            status_code=404,
            detail="ã“ã®ç”»åƒã®åˆ†æçµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã«åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
        )

    upload_record = upload_records[image_id]
    analysis_results = search_results[image_id]

    # ç¾åœ¨æ™‚åˆ»
    current_time = datetime.now()

    # è¨¼æ‹ ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
    evidence_data = {
        "evidence_info": {
            "creation_date": current_time.isoformat(),
            "creation_timestamp": int(current_time.timestamp()),
            "evidence_id": f"evidence_{image_id}_{int(current_time.timestamp())}",
            "system_info": "Book Leak Detector v1.0.0"
        },
        "image_info": {
            "image_id": image_id,
            "original_filename": upload_record.get("original_filename", "ä¸æ˜"),
            "file_size": upload_record.get("file_size", 0),
            "upload_time": upload_record.get("upload_time", "ä¸æ˜"),
            "content_type": upload_record.get("content_type", "ä¸æ˜")
        },
        "analysis_info": {
            "analysis_time": upload_record.get("analysis_time", "ä¸æ˜"),
            "analysis_status": upload_record.get("analysis_status", "ä¸æ˜"),
            "found_urls_count": upload_record.get("found_urls_count", 0),
            "processed_results_count": upload_record.get("processed_results_count", 0)
        },
        "detection_results": {
            "total_urls_detected": len(analysis_results),
            "url_analysis": []
        }
    }

    # å„URLã®åˆ¤å®šçµæœã‚’è¿½åŠ 
    for result in analysis_results:
        url_info = {
            "url": result.get("url", ""),
            "judgment": result.get("judgment", "ï¼Ÿ"),
            "reason": result.get("reason", "ç†ç”±ä¸æ˜"),
            "analysis_timestamp": current_time.isoformat()
        }
        evidence_data["detection_results"]["url_analysis"].append(url_info)

    # ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—ï¼ˆæ”¹ã–ã‚“é˜²æ­¢ç”¨ï¼‰
    evidence_data["integrity"] = {
        "hash_algorithm": "SHA-256",
        "data_hash": generate_evidence_hash(evidence_data),
        "note": "ã“ã®ãƒãƒƒã‚·ãƒ¥å€¤ã¯è¨¼æ‹ ãƒ‡ãƒ¼ã‚¿ã®æ”¹ã–ã‚“ã‚’æ¤œçŸ¥ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™"
    }

    return evidence_data

@app.get("/api/evidence/download/{image_id}")
async def download_evidence(image_id: str):
    """
    æ¤œå‡ºçµæœã‚’è¨¼æ‹ ã¨ã—ã¦ä¿å­˜ç”¨JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
    """
    logger.info(f"ğŸ“¥ è¨¼æ‹ ä¿å…¨è¦æ±‚: image_id={image_id}")

    try:
        # è¨¼æ‹ ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        evidence_data = create_evidence_data(image_id)

        # JSONãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        timestamp = int(datetime.now().timestamp())
        filename = f"evidence_{image_id}_{timestamp}.json"

        # JSONãƒ‡ãƒ¼ã‚¿ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
        json_content = json.dumps(evidence_data, ensure_ascii=False, indent=2)

        logger.info(f"âœ… è¨¼æ‹ ä¿å…¨ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {filename}")

        # ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™
        return Response(
            content=json_content,
            media_type="application/json",
            headers={
                "Content-Disposition": f"attachment; filename={filename}",
                "Content-Type": "application/json; charset=utf-8"
            }
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ è¨¼æ‹ ä¿å…¨ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": "evidence_creation_failed",
                "message": f"è¨¼æ‹ ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}",
                "image_id": image_id
            }
        )

@app.get("/api/history")
async def get_analysis_history():
    """
    éå»ã®æ¤œæŸ»å±¥æ­´ä¸€è¦§ã‚’å–å¾—ã™ã‚‹
    """
    logger.info(f"ğŸ“š å±¥æ­´å–å¾—è¦æ±‚: {len(analysis_history)}ä»¶")

    try:
        # å±¥æ­´ã‚’æ–°ã—ã„é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_history = sorted(
            analysis_history,
            key=lambda x: x.get("analysis_timestamp", 0),
            reverse=True
        )

        # è¡¨ç¤ºç”¨ã«å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
        formatted_history = []
        for entry in sorted_history:
            formatted_entry = {
                "history_id": entry.get("history_id"),
                "image_id": entry.get("image_id"),
                "image_hash": entry.get("image_hash"),
                "original_filename": entry.get("original_filename"),
                "analysis_date": entry.get("analysis_date"),
                "analysis_timestamp": entry.get("analysis_timestamp"),
                "found_urls_count": entry.get("found_urls_count", 0),
                "processed_results_count": entry.get("processed_results_count", 0),
                "summary": {
                    "safe_count": len([r for r in entry.get("results", []) if r.get("judgment") == "â—‹"]),
                    "suspicious_count": len([r for r in entry.get("results", []) if r.get("judgment") == "Ã—"]),
                    "unknown_count": len([r for r in entry.get("results", []) if r.get("judgment") in ["ï¼Ÿ", "ï¼"]])
                }
            }
            formatted_history.append(formatted_entry)

        return {
            "success": True,
            "total_history_count": len(analysis_history),
            "history": formatted_history
        }

    except Exception as e:
        logger.error(f"âŒ å±¥æ­´å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": "history_retrieval_failed",
                "message": f"å±¥æ­´ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}"
            }
        )

@app.get("/api/history/diff/{image_id}")
async def get_analysis_diff(image_id: str):
    """
    æŒ‡å®šã•ã‚ŒãŸç”»åƒIDã®å‰å›æ¤œæŸ»ã¨ã®å·®åˆ†ã‚’å–å¾—ã™ã‚‹
    """
    logger.info(f"ğŸ”„ å·®åˆ†å–å¾—è¦æ±‚: image_id={image_id}")

    try:
        # ç¾åœ¨ã®çµæœã‚’å–å¾—
        if image_id not in upload_records:
            raise HTTPException(
                status_code=404,
                detail="æŒ‡å®šã•ã‚ŒãŸimage_idãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
            )

        record = upload_records[image_id]
        current_results = search_results.get(image_id, [])
        image_hash = record.get("image_hash")

        if not image_hash:
            return {
                "success": True,
                "has_previous": False,
                "message": "ã“ã®ç”»åƒã«å¯¾ã™ã‚‹éå»ã®åˆ†æçµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
            }

        # åŒã˜ãƒãƒƒã‚·ãƒ¥ã®éå»ã®åˆ†æçµæœã‚’å–å¾—
        previous_analysis = get_previous_analysis(image_hash)

        if not previous_analysis:
            return {
                "success": True,
                "has_previous": False,
                "message": "ã“ã®ç”»åƒã«å¯¾ã™ã‚‹éå»ã®åˆ†æçµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
            }

        # å·®åˆ†ã‚’è¨ˆç®—
        diff_result = calculate_diff(current_results, previous_analysis.get("results", []))

        # å‰å›åˆ†ææ—¥æ™‚ã‚’å«ã‚ã¦è¿”ã™
        response_data = {
            "success": True,
            "has_previous": True,
            "image_id": image_id,
            "image_hash": image_hash,
            "current_analysis": {
                "analysis_date": record.get("analysis_time"),
                "results_count": len(current_results)
            },
            "previous_analysis": {
                "analysis_date": previous_analysis.get("analysis_date"),
                "results_count": len(previous_analysis.get("results", []))
            },
            "diff": diff_result
        }

        logger.info(f"âœ… å·®åˆ†è¨ˆç®—å®Œäº†: æ–°è¦={diff_result['total_new']}, æ¶ˆå¤±={diff_result['total_disappeared']}, å¤‰æ›´={diff_result['total_changed']}")

        return response_data

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ å·®åˆ†å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": "diff_calculation_failed",
                "message": f"å·®åˆ†ã®è¨ˆç®—ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}",
                "image_id": image_id
            }
        )

def generate_csv_report(image_id: str) -> str:
    """
    CSVå½¢å¼ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹
    """
    if image_id not in upload_records:
        raise HTTPException(
            status_code=404,
            detail="æŒ‡å®šã•ã‚ŒãŸimage_idãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        )

    record = upload_records[image_id]
    results = search_results.get(image_id, [])

    # StringIOã‚’ä½¿ã£ã¦CSVãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    output = StringIO()

    # BOMä»˜ãUTF-8ã®ãŸã‚ã®BOMã‚’è¿½åŠ 
    output.write('\ufeff')

    writer = csv.writer(output)

    # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œï¼ˆæ—¥æœ¬èªï¼‰
    headers = [
        "æ¤œæŸ»æ—¥æ™‚",
        "ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«å",
        "URL",
        "ãƒ‰ãƒ¡ã‚¤ãƒ³",
        "åˆ¤å®šçµæœ",
        "åˆ¤å®šç†ç”±"
    ]
    writer.writerow(headers)

    # ãƒ‡ãƒ¼ã‚¿è¡Œ
    analysis_time = record.get("analysis_time", "ä¸æ˜")
    filename = record.get("original_filename", "ä¸æ˜")

    for result in results:
        url = result.get("url", "")
        judgment = result.get("judgment", "ï¼Ÿ")
        reason = result.get("reason", "ç†ç”±ä¸æ˜")

        # ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’æŠ½å‡º
        try:
            domain = urlparse(url).netloc
        except:
            domain = "ä¸æ˜"

        writer.writerow([
            analysis_time,
            filename,
            url,
            domain,
            judgment,
            reason
        ])

    return output.getvalue()

def generate_summary_report(image_id: str) -> dict:
    """
    çµŒå–¶å±¤å‘ã‘ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹
    """
    if image_id not in upload_records:
        raise HTTPException(
            status_code=404,
            detail="æŒ‡å®šã•ã‚ŒãŸimage_idãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        )

    record = upload_records[image_id]
    results = search_results.get(image_id, [])

    # çµ±è¨ˆã‚’è¨ˆç®—
    total_count = len(results)
    safe_count = len([r for r in results if r.get("judgment") == "â—‹"])
    dangerous_count = len([r for r in results if r.get("judgment") == "Ã—"])
    warning_count = len([r for r in results if r.get("judgment") in ["ï¼Ÿ", "ï¼"]])

    # å±é™ºãªãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’é›†è¨ˆ
    dangerous_domains = {}
    for result in results:
        if result.get("judgment") == "Ã—":
            try:
                domain = urlparse(result.get("url", "")).netloc
                if domain:
                    dangerous_domains[domain] = dangerous_domains.get(domain, 0) + 1
            except:
                pass

    # TOP5å±é™ºãƒ‰ãƒ¡ã‚¤ãƒ³
    top_dangerous = sorted(dangerous_domains.items(), key=lambda x: x[1], reverse=True)[:5]

    # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
    if dangerous_count > 0:
        if dangerous_count >= 3:
            recommended_action = "è‡³æ€¥å¯¾å¿œãŒå¿…è¦"
            action_details = f"{dangerous_count}ä»¶ã®å±é™ºã‚µã‚¤ãƒˆãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚æ³•çš„å¯¾å¿œã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
        else:
            recommended_action = "è¦æ³¨æ„ãƒ»ç›£è¦–ç¶™ç¶š"
            action_details = f"{dangerous_count}ä»¶ã®å±é™ºã‚µã‚¤ãƒˆãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚ç¶™ç¶šçš„ãªç›£è¦–ãŒå¿…è¦ã§ã™ã€‚"
    elif warning_count > 0:
        recommended_action = "çµŒéè¦³å¯Ÿ"
        action_details = f"{warning_count}ä»¶ã®ä¸æ˜ã‚µã‚¤ãƒˆãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚å®šæœŸçš„ãªå†æ¤œæŸ»ã‚’æ¨å¥¨ã—ã¾ã™ã€‚"
    else:
        recommended_action = "å®‰å…¨"
        action_details = "å±é™ºãªã‚µã‚¤ãƒˆã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚"

    return {
        "summary": {
            "analysis_date": record.get("analysis_time", "ä¸æ˜"),
            "image_filename": record.get("original_filename", "ä¸æ˜"),
            "total_detected": total_count,
            "safe_sites": safe_count,
            "dangerous_sites": dangerous_count,
            "warning_sites": warning_count
        },
        "risk_assessment": {
            "level": "é«˜" if dangerous_count >= 3 else "ä¸­" if dangerous_count > 0 else "ä½",
            "recommended_action": recommended_action,
            "action_details": action_details
        },
        "top_dangerous_domains": [
            {"domain": domain, "count": count} for domain, count in top_dangerous
        ],
        "recommendations": [
            "å®šæœŸçš„ãªå†æ¤œæŸ»ã®å®Ÿæ–½",
            "æ¤œå‡ºã•ã‚ŒãŸå±é™ºã‚µã‚¤ãƒˆã¸ã®æ³•çš„å¯¾å¿œ",
            "ç¤¾å†…ã¸ã®æ³¨æ„å–šèµ·ã¨æ•™è‚²",
            "æ¤œå‡ºçµæœã®ç¤¾å†…å…±æœ‰"
        ]
    }

@app.get("/api/report/csv/{image_id}")
async def download_csv_report(image_id: str):
    """
    CSVå½¢å¼ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
    """
    logger.info(f"ğŸ“Š CSVãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆè¦æ±‚: image_id={image_id}")

    try:
        # CSVãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        csv_content = generate_csv_report(image_id)

        # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        timestamp = int(datetime.now().timestamp())
        filename = f"leak_detection_report_{image_id}_{timestamp}.csv"

        logger.info(f"âœ… CSVãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {filename}")

        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™
        return Response(
            content=csv_content.encode('utf-8'),
            media_type="text/csv",
            headers={
                "Content-Disposition": f"attachment; filename={filename}",
                "Content-Type": "text/csv; charset=utf-8"
            }
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ CSVãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": "csv_report_generation_failed",
                "message": f"CSVãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}",
                "image_id": image_id
            }
        )

@app.get("/api/report/summary/{image_id}")
async def get_summary_report(image_id: str):
    """
    çµŒå–¶å±¤å‘ã‘ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’å–å¾—ã™ã‚‹
    """
    logger.info(f"ğŸ“Š ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆè¦æ±‚: image_id={image_id}")

    try:
        # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
        summary_data = generate_summary_report(image_id)

        logger.info(f"âœ… ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {image_id}")

        return {
            "success": True,
            "image_id": image_id,
            "report": summary_data,
            "generated_at": datetime.now().isoformat()
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": "summary_report_generation_failed",
                "message": f"ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}",
                "image_id": image_id
            }
        )

@app.post("/batch-upload")
async def batch_upload_images(files: List[UploadFile] = File(...)):
    """
    è¤‡æ•°ã®ç”»åƒã‚’ä¸€æ‹¬ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
    """
    logger.info(f"ğŸ“¤ ãƒãƒƒãƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {len(files)}ãƒ•ã‚¡ã‚¤ãƒ«")

    # ãƒ•ã‚¡ã‚¤ãƒ«æ•°åˆ¶é™ãƒã‚§ãƒƒã‚¯
    if len(files) > 10:
        raise HTTPException(
            status_code=400,
            detail={
                "error": "too_many_files",
                "message": "ãƒ•ã‚¡ã‚¤ãƒ«æ•°ãŒä¸Šé™ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚æœ€å¤§10ãƒ•ã‚¡ã‚¤ãƒ«ã¾ã§ã§ã™ã€‚",
                "max_files": 10,
                "received_files": len(files)
            }
        )

    total_size = 0
    uploaded_files = []
    errors = []

    for i, file in enumerate(files):
        try:
            logger.info(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ ({i+1}/{len(files)}): {file.filename}")

            # ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼
            if not validate_file(file):
                errors.append({
                    "filename": file.filename,
                    "error": "invalid_file_format",
                    "message": f"ç„¡åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {file.content_type}"
                })
                continue

            # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            content = await file.read()
            file_size = len(content)
            total_size += file_size

            # åˆè¨ˆã‚µã‚¤ã‚ºåˆ¶é™ãƒã‚§ãƒƒã‚¯ï¼ˆ50MBï¼‰
            if total_size > 50 * 1024 * 1024:
                errors.append({
                    "filename": file.filename,
                    "error": "total_size_exceeded",
                    "message": "åˆè¨ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒ50MBã‚’è¶…ãˆã¦ã„ã¾ã™"
                })
                break

            # å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™ãƒã‚§ãƒƒã‚¯ï¼ˆ10MBï¼‰
            if file_size > 10 * 1024 * 1024:
                errors.append({
                    "filename": file.filename,
                    "error": "file_too_large",
                    "message": f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã¾ã™: {file_size / (1024*1024):.1f}MB"
                })
                continue

            # ãƒ•ã‚¡ã‚¤ãƒ«ç¨®åˆ¥ã«ã‚ˆã‚‹æ¤œè¨¼
            is_pdf = is_pdf_file(file.content_type, file.filename or "")

            if is_pdf:
                # PDFæ¤œè¨¼
                if not PDF_SUPPORT:
                    errors.append({
                        "filename": file.filename,
                        "error": "pdf_not_supported",
                        "message": "PDFå‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“"
                    })
                    continue

                try:
                    # PDFã®æœ‰åŠ¹æ€§ã‚’ç¢ºèª
                    test_images = convert_pdf_to_images(content)
                    if not test_images:
                        raise Exception("PDFã‹ã‚‰ç”»åƒã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
                except Exception as e:
                    errors.append({
                        "filename": file.filename,
                        "error": "corrupted_pdf",
                        "message": f"ç ´æã—ãŸPDFãƒ•ã‚¡ã‚¤ãƒ«: {str(e)}"
                    })
                    continue
            else:
                # ç”»åƒæ¤œè¨¼
                try:
                    image = Image.open(BytesIO(content))
                    image.verify()
                except Exception as e:
                    errors.append({
                        "filename": file.filename,
                        "error": "corrupted_image",
                        "message": f"ç ´æã—ãŸç”»åƒãƒ•ã‚¡ã‚¤ãƒ«: {str(e)}"
                    })
                    continue

            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            file_id = str(uuid.uuid4())
            file_extension = os.path.splitext(file.filename or "image")[1].lower() or ".jpg"
            safe_filename = f"{file_id}{file_extension}"
            file_path = os.path.join(UPLOAD_DIR, safe_filename)

            with open(file_path, "wb") as f:
                f.write(content)

            # è¨˜éŒ²ä¿å­˜
            upload_record = {
                "id": file_id,
                "original_filename": file.filename,
                "saved_filename": safe_filename,
                "file_path": file_path,
                "content_type": file.content_type,
                "file_size": file_size,
                "upload_time": datetime.now().isoformat(),
                "status": "uploaded",
                "batch_upload": True,
                "file_type": "pdf" if is_pdf else "image"
            }

            upload_records[file_id] = upload_record
            uploaded_files.append({
                "file_id": file_id,
                "filename": file.filename,
                "size": file_size,
                "status": "success"
            })

            logger.info(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {file.filename} -> {file_id}")

        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼ {file.filename}: {str(e)}")
            errors.append({
                "filename": file.filename,
                "error": "processing_failed",
                "message": str(e)
            })

    # è¨˜éŒ²ã‚’ä¿å­˜
    save_records()

    logger.info(f"âœ… ãƒãƒƒãƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: æˆåŠŸ={len(uploaded_files)}ä»¶, ã‚¨ãƒ©ãƒ¼={len(errors)}ä»¶")

    return {
        "success": True,
        "total_files": len(files),
        "uploaded_count": len(uploaded_files),
        "error_count": len(errors),
        "total_size": total_size,
        "files": uploaded_files,
        "errors": errors,
        "upload_time": datetime.now().isoformat()
    }

@app.post("/batch-search")
async def batch_search_images(
    background_tasks: BackgroundTasks,
    request: dict,
    batch_id: Optional[str] = None
):
    """
    è¤‡æ•°ã®ç”»åƒã‚’ä¸€æ‹¬ã§æ¤œç´¢ã™ã‚‹
    """
    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£ã‹ã‚‰ file_ids ã‚’å–å¾—
    file_ids = request.get("file_ids", [])
    if not file_ids:
        raise HTTPException(
            status_code=422,
            detail="file_ids is required in request body"
        )

    if not batch_id:
        batch_id = str(uuid.uuid4())

    logger.info(f"ğŸ” ãƒãƒƒãƒæ¤œç´¢é–‹å§‹: batch_id={batch_id}, {len(file_ids)}ãƒ•ã‚¡ã‚¤ãƒ«")

    # ãƒãƒƒãƒã‚¸ãƒ§ãƒ–åˆæœŸåŒ–
    batch_jobs[batch_id] = {
        "batch_id": batch_id,
        "total_files": len(file_ids),
        "completed_files": 0,
        "status": "processing",
        "start_time": datetime.now().isoformat(),
        "files": []
    }

    # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆæœŸçŠ¶æ…‹ã‚’è¨­å®šï¼ˆã™ã¹ã¦ã®file_idsã«å¯¾å¿œï¼‰
    for file_id in file_ids:
        if file_id in upload_records:
            batch_jobs[batch_id]["files"].append({
                "file_id": file_id,
                "filename": upload_records[file_id].get("original_filename", "ä¸æ˜"),
                "status": "pending",
                "progress": 0
            })
        else:
            # å­˜åœ¨ã—ãªã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚é…åˆ—ã«è¿½åŠ ï¼ˆã‚¨ãƒ©ãƒ¼çŠ¶æ…‹ã§ï¼‰
            batch_jobs[batch_id]["files"].append({
                "file_id": file_id,
                "filename": "ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“",
                "status": "error",
                "progress": 0,
                "error": "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            })

    # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å‡¦ç†é–‹å§‹
    background_tasks.add_task(process_batch_search, batch_id, file_ids)

    return {
        "success": True,
        "batch_id": batch_id,
        "message": f"ãƒãƒƒãƒæ¤œç´¢ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚{len(file_ids)}ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¾ã™ã€‚",
        "total_files": len(file_ids)
    }

def process_batch_search(batch_id: str, file_ids: List[str]):
    """
    ãƒãƒƒãƒæ¤œç´¢ã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œ
    """
    try:
        for i, file_id in enumerate(file_ids):
            if batch_id not in batch_jobs:
                return

            # æ—¢ã«ã‚¨ãƒ©ãƒ¼çŠ¶æ…‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if batch_jobs[batch_id]["files"][i]["status"] == "error":
                logger.info(f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ— ({i+1}/{len(file_ids)}): {file_id} - æ—¢ã«ã‚¨ãƒ©ãƒ¼çŠ¶æ…‹")
                batch_jobs[batch_id]["completed_files"] = i + 1
                continue

            # ãƒ•ã‚¡ã‚¤ãƒ«çŠ¶æ…‹ã‚’æ›´æ–°
            batch_jobs[batch_id]["files"][i]["status"] = "processing"
            batch_jobs[batch_id]["files"][i]["progress"] = 0

            logger.info(f"ğŸ”„ ãƒãƒƒãƒæ¤œç´¢å‡¦ç†ä¸­ ({i+1}/{len(file_ids)}): {file_id}")

            try:
                # æ—¢å­˜ã®åˆ†æãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨
                if file_id not in upload_records:
                    batch_jobs[batch_id]["files"][i]["status"] = "error"
                    batch_jobs[batch_id]["files"][i]["error"] = "ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
                    continue

                record = upload_records[file_id]
                file_path = record["file_path"]
                file_type = record.get("file_type", "image")

                # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
                with open(file_path, 'rb') as file:
                    file_content = file.read()

                # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°
                batch_jobs[batch_id]["files"][i]["progress"] = 10

                # ãƒ•ã‚¡ã‚¤ãƒ«ç¨®åˆ¥ã«å¿œã˜ã¦å‡¦ç†ã‚’åˆ†å²
                if file_type == "pdf":
                    # PDFã®å ´åˆï¼šå„ãƒšãƒ¼ã‚¸ã‚’ç”»åƒã«å¤‰æ›ã—ã¦å‡¦ç†
                    pdf_images = convert_pdf_to_images(file_content)
                    if not pdf_images:
                        raise Exception("PDFã‹ã‚‰ç”»åƒã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")

                    # å„ãƒšãƒ¼ã‚¸ã®ç”»åƒãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—ï¼ˆæœ€åˆã®ãƒšãƒ¼ã‚¸ã‚’ãƒ¡ã‚¤ãƒ³ãƒãƒƒã‚·ãƒ¥ã¨ã™ã‚‹ï¼‰
                    image_hash = calculate_image_hash(pdf_images[0])

                    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°
                    batch_jobs[batch_id]["files"][i]["progress"] = 25

                    # å„ãƒšãƒ¼ã‚¸ã‚’å€‹åˆ¥ã«åˆ†æ
                    all_url_lists = []
                    for page_i, page_image_content in enumerate(pdf_images):
                        page_urls = search_web_for_image(page_image_content)
                        all_url_lists.extend(page_urls)

                        # ãƒšãƒ¼ã‚¸ã”ã¨ã®ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°
                        page_progress = 25 + (page_i + 1) * 35 // len(pdf_images)
                        batch_jobs[batch_id]["files"][i]["progress"] = min(page_progress, 60)

                    # é‡è¤‡URLã‚’é™¤å»
                    url_list = list(dict.fromkeys(all_url_lists))

                else:
                    # ç”»åƒã®å ´åˆï¼šå¾“æ¥ã®å‡¦ç†
                    image_content = file_content
                    image_hash = calculate_image_hash(image_content)

                    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°
                    batch_jobs[batch_id]["files"][i]["progress"] = 20

                    # Webæ¤œç´¢å®Ÿè¡Œ
                    url_list = search_web_for_image(image_content)

                # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°
                batch_jobs[batch_id]["files"][i]["progress"] = 60

                # URLåˆ†æ
                processed_results = []
                for j, url in enumerate(url_list[:10]):
                    result = analyze_url_efficiently(url)
                    if result:
                        processed_results.append(result)

                    # å°åˆ»ã¿ãªé€²æ—æ›´æ–°
                    progress = 60 + (j + 1) * 3  # 60% + 30%åˆ†ã‚’ URLåˆ†æã§ä½¿ç”¨
                    batch_jobs[batch_id]["files"][i]["progress"] = min(progress, 90)

                # çµæœä¿å­˜
                search_results[file_id] = processed_results

                # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è¨˜éŒ²æ›´æ–°
                record["analysis_status"] = "completed"
                record["analysis_time"] = datetime.now().isoformat()
                record["found_urls_count"] = len(url_list)
                record["processed_results_count"] = len(processed_results)
                record["image_hash"] = image_hash

                # å±¥æ­´ä¿å­˜
                save_analysis_to_history(file_id, image_hash, processed_results)

                # å®Œäº†çŠ¶æ…‹æ›´æ–°
                batch_jobs[batch_id]["files"][i]["status"] = "completed"
                batch_jobs[batch_id]["files"][i]["progress"] = 100
                batch_jobs[batch_id]["files"][i]["results_count"] = len(processed_results)

                logger.info(f"âœ… ãƒãƒƒãƒæ¤œç´¢å®Œäº† ({i+1}/{len(file_ids)}): {file_id}")

            except Exception as e:
                logger.error(f"âŒ ãƒãƒƒãƒæ¤œç´¢ã‚¨ãƒ©ãƒ¼ {file_id}: {str(e)}")
                batch_jobs[batch_id]["files"][i]["status"] = "error"
                batch_jobs[batch_id]["files"][i]["error"] = str(e)

            # å®Œäº†ãƒ•ã‚¡ã‚¤ãƒ«æ•°æ›´æ–°
            batch_jobs[batch_id]["completed_files"] = i + 1

        # å…¨ä½“å®Œäº†
        batch_jobs[batch_id]["status"] = "completed"
        batch_jobs[batch_id]["end_time"] = datetime.now().isoformat()
        save_records()

        logger.info(f"âœ… ãƒãƒƒãƒæ¤œç´¢å…¨ä½“å®Œäº†: batch_id={batch_id}")

    except Exception as e:
        import traceback
        logger.error(f"âŒ ãƒãƒƒãƒæ¤œç´¢å…¨ä½“ã‚¨ãƒ©ãƒ¼: {str(e)}")
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")
        if batch_id in batch_jobs:
            batch_jobs[batch_id]["status"] = "error"
            batch_jobs[batch_id]["error"] = str(e)
            batch_jobs[batch_id]["end_time"] = datetime.now().isoformat()

@app.get("/batch-status/{batch_id}")
async def get_batch_status(batch_id: str):
    """
    ãƒãƒƒãƒå‡¦ç†ã®é€²æ—çŠ¶æ³ã‚’å–å¾—
    """
    if batch_id not in batch_jobs:
        raise HTTPException(
            status_code=404,
            detail="æŒ‡å®šã•ã‚ŒãŸãƒãƒƒãƒIDãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        )

    return {
        "success": True,
        "batch": batch_jobs[batch_id]
    }

@app.get("/image/{file_id}")
async def get_image(file_id: str):
    """
    ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    """
    if file_id not in upload_records:
        raise HTTPException(
            status_code=404,
            detail="æŒ‡å®šã•ã‚ŒãŸç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
        )

    record = upload_records[file_id]
    file_path = record["file_path"]

    if not os.path.exists(file_path):
        raise HTTPException(
            status_code=404,
            detail="ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“"
        )

    # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã‹ã‚‰é©åˆ‡ãªãƒ¡ãƒ‡ã‚£ã‚¢ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
    _, ext = os.path.splitext(file_path)
    media_type_map = {
        '.jpg': 'image/jpeg',
        '.jpeg': 'image/jpeg',
        '.png': 'image/png',
        '.gif': 'image/gif',
        '.webp': 'image/webp'
    }
    media_type = media_type_map.get(ext.lower(), 'image/jpeg')

    return FileResponse(
        file_path,
        media_type=media_type,
        filename=record.get("original_filename", f"image{ext}")
    )

@app.get("/file-info/{file_id}")
async def get_file_info(file_id: str):
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã®æƒ…å ±ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã€ã‚¿ã‚¤ãƒ—ç­‰ï¼‰ã‚’å–å¾—ã™ã‚‹
    """
    if file_id not in upload_records:
        raise HTTPException(
            status_code=404,
            detail="æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
        )

    record = upload_records[file_id]

    return {
        "file_id": file_id,
        "filename": record.get("original_filename", "ä¸æ˜"),
        "fileType": record.get("file_type", "image"),
        "fileSize": record.get("file_size", 0),
        "uploadTime": record.get("upload_time", ""),
        "analysisStatus": record.get("analysis_status", "pending")
    }

@app.get("/pdf-preview/{file_id}")
async def get_pdf_preview(file_id: str):
    """
    PDFãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€åˆã®ãƒšãƒ¼ã‚¸ã‚’ç”»åƒã¨ã—ã¦å–å¾—ã™ã‚‹
    """
    if file_id not in upload_records:
        raise HTTPException(
            status_code=404,
            detail="æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
        )

    record = upload_records[file_id]
    file_path = record["file_path"]

    if not os.path.exists(file_path):
        raise HTTPException(
            status_code=404,
            detail="ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“"
        )

    # PDFãƒ•ã‚¡ã‚¤ãƒ«ã‹ãƒã‚§ãƒƒã‚¯
    if record.get("file_type") != "pdf":
        raise HTTPException(
            status_code=400,
            detail="æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã¯PDFã§ã¯ã‚ã‚Šã¾ã›ã‚“"
        )

    try:
        # PDFã®æœ€åˆã®ãƒšãƒ¼ã‚¸ã‚’ç”»åƒã«å¤‰æ›
        with open(file_path, 'rb') as file:
            pdf_content = file.read()

        pdf_images = convert_pdf_to_images(pdf_content)
        if not pdf_images:
            raise HTTPException(
                status_code=500,
                detail="PDFã‹ã‚‰ç”»åƒã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸ"
            )

        # æœ€åˆã®ãƒšãƒ¼ã‚¸ã®ç”»åƒã‚’è¿”ã™
        first_page_image = pdf_images[0]

        return Response(
            content=first_page_image,
            media_type="image/png",
            headers={"Content-Disposition": f"inline; filename=\"{file_id}_preview.png\""}
        )

    except Exception as e:
        logger.error(f"âŒ PDFãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼ {file_id}: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail="PDFãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ"
        )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)