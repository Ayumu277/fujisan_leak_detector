from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
import os
import json
import uuid
# base64 ã¯ä¸è¦ï¼ˆVision API WEB_DETECTIONã‚’ä½¿ç”¨ï¼‰
import re
import logging
# requests ã¯ä¸è¦ï¼ˆhttpxã‚’ä½¿ç”¨ï¼‰
from datetime import datetime
from typing import Dict, List, Optional
from io import BytesIO
from dotenv import load_dotenv
from PIL import Image
# serpapi ã¯ä¸è¦ï¼ˆVision API WEB_DETECTIONã‚’ä½¿ç”¨ï¼‰
import httpx
from bs4 import BeautifulSoup
from google.cloud import vision
import google.generativeai as genai

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ãƒ­ã‚°ä¿å­˜ç”¨ï¼ˆãƒ¡ãƒ¢ãƒªå†…ï¼‰
system_logs = []
MAX_LOGS = 100  # æœ€å¤§ä¿å­˜ãƒ­ã‚°æ•°

class ListHandler(logging.Handler):
    """ãƒ­ã‚°ã‚’ãƒªã‚¹ãƒˆã«ä¿å­˜ã™ã‚‹ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    def emit(self, record):
        log_entry = {
            "timestamp": datetime.fromtimestamp(record.created).strftime("%Y-%m-%d %H:%M:%S"),
            "level": record.levelname,
            "message": record.getMessage()
        }
        system_logs.append(log_entry)
        # å¤ã„ãƒ­ã‚°ã‚’å‰Šé™¤
        if len(system_logs) > MAX_LOGS:
            system_logs.pop(0)

# ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¿½åŠ 
list_handler = ListHandler()
logger.addHandler(list_handler)

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

app = FastAPI(title="Book Leak Detector", version="1.0.0")

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å¿…è¦ãªAPI_KEYã‚’å–å¾—
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GOOGLE_APPLICATION_CREDENTIALS = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")

# Gemini APIã®è¨­å®š
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    logger.info("âœ… Gemini APIè¨­å®šå®Œäº†")
else:
    logger.warning("âš ï¸ GEMINI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

# API_KEYã®è¨­å®šçŠ¶æ³ã‚’ãƒã‚§ãƒƒã‚¯
missing_keys = []
if not GEMINI_API_KEY:
    missing_keys.append("GEMINI_API_KEY")
if not GOOGLE_APPLICATION_CREDENTIALS:
    missing_keys.append("GOOGLE_APPLICATION_CREDENTIALS")

if missing_keys:
    print(f"è­¦å‘Š: ä»¥ä¸‹ã®ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“: {', '.join(missing_keys)}")
    print("å®Œå…¨ãªæ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€.envãƒ•ã‚¡ã‚¤ãƒ«ã§ä»¥ä¸‹ã‚’è¨­å®šã—ã¦ãã ã•ã„:")
    print("- GEMINI_API_KEY: Gemini AIç”¨")
    print("- GOOGLE_APPLICATION_CREDENTIALS: Google Vision APIç”¨ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼")
else:
    print("âœ“ å¿…è¦ãªAPI_KEYãŒæ­£å¸¸ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã™")

# CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:5173", "http://localhost:5174"],  # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã®URL
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
UPLOAD_DIR = "uploads"
os.makedirs(UPLOAD_DIR, exist_ok=True)

# é™çš„ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®šï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”»åƒç”¨ï¼‰
app.mount("/uploads", StaticFiles(directory=UPLOAD_DIR), name="uploads")

# ä¸€æ™‚çš„ãªç”»åƒå…¬é–‹ç”¨ï¼ˆæ¤œç´¢æ™‚ã®ã¿ä½¿ç”¨ï¼‰
app.mount("/temp-images", StaticFiles(directory=UPLOAD_DIR), name="temp-images")

# ãƒ¡ãƒ¢ãƒªå†…ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼ˆæœ¬ç•ªç’°å¢ƒã§ã¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨ï¼‰
upload_records: Dict[str, Dict] = {}
search_results: Dict[str, List[Dict]] = {}

# JSONãƒ•ã‚¡ã‚¤ãƒ«ã§ã®æ°¸ç¶šåŒ–
RECORDS_FILE = "upload_records.json"

def load_records():
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨˜éŒ²ã‚’èª­ã¿è¾¼ã¿"""
    global upload_records
    try:
        if os.path.exists(RECORDS_FILE):
            with open(RECORDS_FILE, 'r', encoding='utf-8') as f:
                upload_records = json.load(f)
    except Exception as e:
        print(f"è¨˜éŒ²ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
        upload_records = {}

def save_records():
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜éŒ²ã‚’ä¿å­˜"""
    try:
        with open(RECORDS_FILE, 'w', encoding='utf-8') as f:
            json.dump(upload_records, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"è¨˜éŒ²ã®ä¿å­˜ã«å¤±æ•—: {e}")

# ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã«è¨˜éŒ²ã‚’èª­ã¿è¾¼ã¿
load_records()

# å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒªã‚¹ãƒˆã¯å‰Šé™¤ï¼ˆGemini AIã§å‹•çš„åˆ¤å®šï¼‰

# Vision APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã§åˆæœŸåŒ–
vision_client = vision.ImageAnnotatorClient()

# Geminiãƒ¢ãƒ‡ãƒ«ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã§åˆæœŸåŒ–
if GEMINI_API_KEY:
    gemini_model = genai.GenerativeModel('gemini-1.5-flash')
    logger.info("âœ… Gemini ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†")
else:
    gemini_model = None
    logger.warning("âš ï¸ Gemini ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã§ãã¾ã›ã‚“ã§ã—ãŸ")

def validate_image_file(file: UploadFile) -> bool:
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒæœ‰åŠ¹ãªç”»åƒã‹ã©ã†ã‹ã‚’æ¤œè¨¼"""
    allowed_types = ["image/jpeg", "image/png", "image/jpg", "image/gif", "image/webp"]
    return file.content_type in allowed_types

# Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰é–¢æ•°ã¯å‰Šé™¤ï¼ˆä¸è¦ï¼‰

def search_web_for_image(image_content: bytes) -> list[str]:
    """
    ç”»åƒã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å—ã‘å–ã‚Šã€Google Cloud Vision APIã®WEB_DETECTIONã‚’ä½¿ã£ã¦
    é¡ä¼¼ãƒ»åŒä¸€ç”»åƒãŒä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹URLã®ãƒªã‚¹ãƒˆã‚’è¿”ã™ã€‚
    """
    logger.info("ğŸ” Google Vision API WEB_DETECTIONæ¤œç´¢é–‹å§‹")

    try:
        image = vision.Image(content=image_content)
        response = vision_client.web_detection(image=image)
        web_detection = response.web_detection

        # URLã‚’é¸åˆ¥ã—ã€ãƒšãƒ¼ã‚¸ã®URLã‚’å„ªå…ˆã™ã‚‹
        page_urls = [page.url for page in web_detection.pages_with_matching_images if page.url] if web_detection.pages_with_matching_images else []

        # ç”»åƒURLã¯å‚è€ƒç¨‹åº¦ã«åé›†
        image_urls = []
        if web_detection.full_matching_images:
            image_urls.extend(img.url for img in web_detection.full_matching_images if img.url)
        if web_detection.partial_matching_images:
            image_urls.extend(img.url for img in web_detection.partial_matching_images if img.url)

        # é‡è¤‡ã‚’é™¤å»ã—ã€ãƒšãƒ¼ã‚¸URLã‚’å„ªå…ˆã—ãŸãƒªã‚¹ãƒˆã‚’ä½œæˆ
        seen = set()
        unique_urls = []

        # page_urls ã‚’å…ˆã«è¿½åŠ 
        for url in page_urls:
            if url not in seen:
                unique_urls.append(url)
                seen.add(url)

        # image_urls ã‚’è¿½åŠ ï¼ˆæ—¢ã«seenã«ã‚ã‚‹ã‚‚ã®ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        for url in image_urls:
            if url not in seen:
                unique_urls.append(url)
                seen.add(url)

        url_list = unique_urls
        logger.info(f"ğŸŒ ç™ºè¦‹ã•ã‚ŒãŸãƒ¦ãƒ‹ãƒ¼ã‚¯URL: {len(url_list)}ä»¶")
        for i, url in enumerate(url_list[:5]):  # æœ€åˆã®5ä»¶ã‚’ãƒ­ã‚°ã«è¡¨ç¤º
            logger.info(f"  {i+1}: {url}")

        return url_list

    except Exception as e:
        logger.error(f"âŒ WEB_DETECTION ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []

def scrape_page_content(url: str) -> str | None:
    """
    URLã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã¨æœ¬æ–‡ã®ä¸€éƒ¨ã‚’æŠ½å‡ºã™ã‚‹ã€‚
    ç”»åƒURLã‚„HTMLä»¥å¤–ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã€‚
    """
    # 1. æ‹¡å¼µå­ã¨ãƒ‰ãƒ¡ã‚¤ãƒ³ã§ç°¡æ˜“ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp']
    if any(url.lower().endswith(ext) for ext in image_extensions):
        logger.info(f"â­ï¸  ç”»åƒæ‹¡å¼µå­ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {url}")
        return None

    image_host_domains = ['pbs.twimg.com', 'm.media-amazon.com', 'img-cdn.theqoo.net']
    if any(domain in url for domain in image_host_domains):
        logger.info(f"â­ï¸  ç”»åƒãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {url}")
        return None

    logger.info(f"ğŸŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {url}")
    try:
        with httpx.Client(timeout=10.0, follow_redirects=True) as client:
            # 2. Content-Typeã‚’äº‹å‰ç¢ºèª
            try:
                head_response = client.head(url, headers={'User-Agent': 'Mozilla/5.0'})
                content_type = head_response.headers.get('content-type', '').lower()
                if 'text/html' not in content_type:
                    logger.info(f"â­ï¸  HTMLã§ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ— (Content-Type: {content_type}): {url}")
                    return None
            except httpx.RequestError as e:
                logger.warning(f"âš ï¸ HEADãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•— (GETã§ç¶šè¡Œ): {e}")

            # 3. GETãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—
            response = client.get(url, headers={'User-Agent': 'Mozilla/5.0'})
            response.raise_for_status()

        # 4. BeautifulSoupã§è§£æ
        soup = BeautifulSoup(response.text, 'html.parser')
        title = soup.title.string if soup.title else ""
        body_text = " ".join([p.get_text() for p in soup.find_all('p', limit=5)])

        content = f"Title: {title.strip()}\\n\\nBody: {body_text.strip()}"
        logger.info(f"ğŸ“ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: {len(content)} chars")
        return content

    except httpx.HTTPStatusError as e:
        logger.error(f"âŒ HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚¨ãƒ©ãƒ¼ {url}: {e.response.status_code} {e.response.reason_phrase}")
        return None
    except Exception as e:
        logger.error(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸€èˆ¬ã‚¨ãƒ©ãƒ¼ {url}: {e}")
        return None

def judge_content_with_gemini(content: str) -> dict:
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ãŸå†…å®¹ã‚’Geminiã§åˆ¤å®šã™ã‚‹"""
    logger.info("ğŸ¤– Gemini AIåˆ¤å®šé–‹å§‹")

    if not gemini_model:
        logger.error("âŒ Gemini ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return {"judgment": "ï¼Ÿ", "reason": "Gemini APIãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"}

    prompt = f"""
ä»¥ä¸‹ã®Webãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’åˆ†æã—ã€è‘—ä½œæ¨©çš„ã«å•é¡ŒãŒã‚ã‚‹æµ·è³Šç‰ˆã‚µã‚¤ãƒˆã‹ã€
ãã‚Œã¨ã‚‚æ­£è¦ã®ã‚µã‚¤ãƒˆã‹ã‚’åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚

ã€Webãƒšãƒ¼ã‚¸ã®å†…å®¹ã€‘
{content[:2000]}

ã€åˆ¤æ–­åŸºæº–ã€‘
- æ­£è¦ã‚µã‚¤ãƒˆ: å‡ºç‰ˆç¤¾ã€è‘—è€…ã€å…¬å¼æ›¸åº—ã€æ›¸è©•ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãªã©ã€‚
- æµ·è³Šç‰ˆã‚µã‚¤ãƒˆ: å…¨æ–‡æ²è¼‰ã€ç„¡æ–™ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€é•æ³•ã‚³ãƒ”ãƒ¼ã‚’ç¤ºå”†ã™ã‚‹æ–‡è¨€ãªã©ã€‚

        ã€å›ç­”å½¢å¼ã€‘
        ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
        åˆ¤å®šï¼š[â—‹ã€Ã—ã€ï¼Ÿ ã®ã„ãšã‚Œã‹]
        ç†ç”±ï¼š[åˆ¤æ–­ã®æ ¹æ‹ ã‚’20å­—ä»¥å†…ã§ç°¡æ½”ã«ã€‚åˆ¤æ–­ä¸èƒ½ãªå ´åˆã¯ã€Œæƒ…å ±ä¸è¶³ã®ãŸã‚åˆ¤æ–­ä¸èƒ½ã€ã¨è¨˜è¼‰]
"""
    try:
        response = gemini_model.generate_content(prompt)
        logger.info(f"ğŸ“‹ Geminiå¿œç­”: {response.text[:100]}...")

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰åˆ¤å®šã¨ç†ç”±ã‚’æŠ½å‡º
        lines = response.text.strip().split('\n')
        judgment_line = next((line for line in lines if 'åˆ¤å®šï¼š' in line), '')
        reason_line = next((line for line in lines if 'ç†ç”±ï¼š' in line), '')

        judgment = judgment_line.split('ï¼š')[1].replace('[','').replace(']','').strip() if 'ï¼š' in judgment_line else "ï¼Ÿ"
        reason = reason_line.split('ï¼š')[1].replace('[','').replace(']','').strip() if 'ï¼š' in reason_line else "AIå¿œç­”ã®è§£æã«å¤±æ•—"

        logger.info(f"âœ… Geminiåˆ¤å®šå®Œäº†: {judgment} - {reason}")
        return {"judgment": judgment, "reason": reason}
    except Exception as e:
        error_msg = str(e)

        # ã‚¨ãƒ©ãƒ¼ã®ç¨®é¡åˆ¥ã«ãƒ­ã‚°ã¨ç†ç”±ã‚’åˆ†ã‘ã‚‹
        if "404" in error_msg and "models/" in error_msg:
            logger.error(f"âŒ Gemini API ãƒ¢ãƒ‡ãƒ«ã‚¨ãƒ©ãƒ¼: {error_msg}")
            return {"judgment": "ï¼", "reason": "Geminiãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}
        elif "401" in error_msg or "403" in error_msg:
            logger.error(f"âŒ Gemini API èªè¨¼ã‚¨ãƒ©ãƒ¼: {error_msg}")
            return {"judgment": "ï¼", "reason": "Gemini APIèªè¨¼ã‚¨ãƒ©ãƒ¼"}
        elif "429" in error_msg or "quota" in error_msg.lower():
            logger.error(f"âŒ Gemini API ã‚¯ã‚©ãƒ¼ã‚¿ã‚¨ãƒ©ãƒ¼: {error_msg}")
            return {"judgment": "ï¼", "reason": "Gemini APIã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™"}
        elif "timeout" in error_msg.lower() or "connection" in error_msg.lower():
            logger.error(f"âŒ Gemini API ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {error_msg}")
            return {"judgment": "ï¼", "reason": "Geminiãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼"}
        else:
            logger.error(f"âŒ Gemini API ä¸æ˜ã‚¨ãƒ©ãƒ¼: {error_msg}")
            return {"judgment": "ï¼Ÿ", "reason": f"AIåˆ¤å®šã‚¨ãƒ©ãƒ¼: {error_msg[:50]}..."}







# analyze_domainé–¢æ•°ã¯å‰Šé™¤ï¼ˆVision API + Geminiåˆ¤å®šã‚’ä½¿ç”¨ï¼‰



# ä¸è¦ãªé–¢æ•°ã¯å‰Šé™¤ã•ã‚Œã¾ã—ãŸ

# Google Custom Search APIé–¢æ•°ã¯å‰Šé™¤ï¼ˆVision API WEB_DETECTIONã‚’ä½¿ç”¨ï¼‰

# ç”»åƒç‰¹å¾´ãƒ™ãƒ¼ã‚¹æ¤œç´¢é–¢æ•°ã¯å‰Šé™¤ï¼ˆVision API WEB_DETECTIONã‚’ä½¿ç”¨ï¼‰

# SerpAPIé–¢é€£ã®é–¢æ•°ã¯å‰Šé™¤ï¼ˆVision API WEB_DETECTIONã‚’ä½¿ç”¨ï¼‰

@app.get("/")
async def root():
    return {
        "message": "Book Leak Detector API",
        "api_keys": {
            "gemini_api_key_configured": GEMINI_API_KEY is not None,
            "google_vision_api_configured": GOOGLE_APPLICATION_CREDENTIALS is not None
        },
        "upload_count": len(upload_records),
        "search_results_count": len(search_results)
    }

@app.post("/upload")
async def upload_image(file: UploadFile = File(...)):
    """ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ä¿å­˜ã™ã‚‹"""

    logger.info(f"ğŸ“¤ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {file.filename}, content_type: {file.content_type}")

    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼
        if not validate_image_file(file):
            logger.error(f"âŒ ç„¡åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {file.content_type}")
            raise HTTPException(
                status_code=400,
                detail={
                    "error": "invalid_file_format",
                    "message": "ç„¡åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™ã€‚JPEGã€PNGã€GIFã€WebPå½¢å¼ã®ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
                    "allowed_types": ["image/jpeg", "image/png", "image/jpg", "image/gif", "image/webp"],
                    "received_type": file.content_type
                }
            )

        logger.info("âœ… ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼æ¤œè¨¼OK")

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™ï¼ˆ10MBï¼‰
        content = await file.read()
        file_size_mb = len(content) / (1024 * 1024)
        logger.info(f"ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size_mb:.2f}MB")

        if len(content) > 10 * 1024 * 1024:
            logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã¾ã™: {file_size_mb:.2f}MB")
            raise HTTPException(
                status_code=413,
                detail={
                    "error": "file_too_large",
                    "message": "ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã¾ã™ã€‚10MBä»¥ä¸‹ã®ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
                    "file_size_mb": file_size_mb,
                    "max_size_mb": 10
                }
            )

        try:
            # ç”»åƒã®æœ‰åŠ¹æ€§ã‚’ç¢ºèªï¼ˆãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ï¼‰
            image = Image.open(BytesIO(content))
            image.verify()
            logger.info("âœ… ç”»åƒæœ‰åŠ¹æ€§æ¤œè¨¼OK")
        except Exception as e:
            logger.error(f"âŒ ç”»åƒæ¤œè¨¼å¤±æ•—: {str(e)}")
            raise HTTPException(
                status_code=400,
                detail={
                    "error": "corrupted_image",
                    "message": "ç ´æã—ãŸç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ã€‚æœ‰åŠ¹ãªç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
                    "validation_error": str(e)
                }
            )

        # ä¸€æ„ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        file_id = str(uuid.uuid4())
        file_extension = os.path.splitext(file.filename or "image")[1].lower() or ".jpg"
        safe_filename = f"{file_id}{file_extension}"
        file_path = os.path.join(UPLOAD_DIR, safe_filename)

        logger.info(f"ğŸ’¾ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜é–‹å§‹: {file_path}")

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        try:
            with open(file_path, "wb") as f:
                f.write(content)
            logger.info("âœ… ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å¤±æ•—: {str(e)}")
            raise HTTPException(
                status_code=500,
                detail={
                    "error": "file_save_failed",
                    "message": f"ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}",
                    "file_path": file_path
                }
            )

        # è¨˜éŒ²ã‚’ä¿å­˜
        upload_record = {
            "id": file_id,
            "original_filename": file.filename,
            "saved_filename": safe_filename,
            "file_path": file_path,
            "content_type": file.content_type,
            "file_size": len(content),
            "upload_time": datetime.now().isoformat(),
            "status": "uploaded"
        }

        upload_records[file_id] = upload_record
        save_records()

        logger.info(f"âœ… ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: file_id={file_id}")

        return {
            "success": True,
            "file_id": file_id,
            "original_filename": file.filename,
            "saved_filename": safe_filename,
            "file_size": len(content),
            "upload_time": upload_record["upload_time"],
            "file_url": f"/uploads/{safe_filename}"
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": "unexpected_error",
                "message": f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            }
        )

@app.get("/uploads/history")
async def get_upload_history():
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å±¥æ­´ã‚’å–å¾—ã™ã‚‹"""
    # æ—¥ä»˜é †ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„ã‚‚ã®ãŒæœ€åˆï¼‰
    sorted_records = sorted(
        upload_records.values(),
        key=lambda x: x["upload_time"],
        reverse=True
    )

    return {
        "success": True,
        "count": len(sorted_records),
        "uploads": sorted_records
    }

@app.get("/uploads/{file_id}")
async def get_upload_details(file_id: str):
    """ç‰¹å®šã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°ã‚’å–å¾—ã™ã‚‹"""
    if file_id not in upload_records:
        raise HTTPException(
            status_code=404,
            detail="æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        )

    record = upload_records[file_id]

    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    if not os.path.exists(record["file_path"]):
        record["status"] = "file_missing"
        save_records()

    return {
        "success": True,
        "file": record
    }

@app.delete("/uploads/{file_id}")
async def delete_upload(file_id: str):
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã™ã‚‹"""
    if file_id not in upload_records:
        raise HTTPException(
            status_code=404,
            detail="æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        )

    record = upload_records[file_id]

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
    try:
        if os.path.exists(record["file_path"]):
            os.remove(record["file_path"])
    except Exception as e:
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")

    # è¨˜éŒ²ã‹ã‚‰å‰Šé™¤
    del upload_records[file_id]
    save_records()

    return {
        "success": True,
        "message": f"ãƒ•ã‚¡ã‚¤ãƒ« {record['original_filename']} ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚"
    }

@app.get("/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {
        "status": "healthy",
        "api_keys": {
            "gemini_api_key_configured": GEMINI_API_KEY is not None,
            "google_vision_api_configured": GOOGLE_APPLICATION_CREDENTIALS is not None
        },
        "system": {
            "upload_directory_exists": os.path.exists(UPLOAD_DIR),
            "records_file_exists": os.path.exists(RECORDS_FILE),
            "total_uploads": len(upload_records),
            "total_search_results": len(search_results)
        }
    }

@app.post("/search/{image_id}")
async def analyze_image(image_id: str):
    """æŒ‡å®šã•ã‚ŒãŸç”»åƒIDã«å¯¾ã—ã¦Webæ¤œç´¢ã‚’å®Ÿè¡Œã—ã€é¡ä¼¼ç”»åƒã®URLãƒªã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹"""

    logger.info(f"ğŸ” Webç”»åƒæ¤œç´¢é–‹å§‹: image_id={image_id}")

    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è¨˜éŒ²ã‚’ç¢ºèª
    if image_id not in upload_records:
        logger.error(f"âŒ image_id not found: {image_id}")
        raise HTTPException(
            status_code=404,
            detail={
                "error": "image_not_found",
                "message": "æŒ‡å®šã•ã‚ŒãŸimage_idãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚",
                "image_id": image_id
            }
        )

    record = upload_records[image_id]
    image_path = record["file_path"]

    logger.info(f"ğŸ“ æ¤œç´¢å¯¾è±¡ç”»åƒ: {image_path}")

    try:
        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã„ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’èª­ã¿è¾¼ã‚€
        with open(image_path, 'rb') as image_file:
            image_content = image_file.read()

        logger.info(f"ğŸ“¸ ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {len(image_content)} bytes")

                # Google Vision API WEB_DETECTIONã§URLæ¤œç´¢
        logger.info("ğŸŒ Google Vision API WEB_DETECTIONå®Ÿè¡Œä¸­...")
        url_list = search_web_for_image(image_content)

        logger.info(f"âœ… Webæ¤œç´¢å®Œäº†: {len(url_list)}ä»¶ã®URLã‚’ç™ºè¦‹")

        # å„URLã«å¯¾ã—ã¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° + Geminiåˆ¤å®šã‚’å®Ÿè¡Œ
        processed_results = []

        for i, url in enumerate(url_list[:10]):  # æœ€å¤§10ä»¶ã‚’å‡¦ç†
            logger.info(f"ğŸ”„ URLå‡¦ç†ä¸­ ({i+1}/{min(len(url_list), 10)}): {url}")

            # ãƒšãƒ¼ã‚¸å†…å®¹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
            content = scrape_page_content(url)

            if content:
                # Geminiã§åˆ¤å®š
                result = judge_content_with_gemini(content)

                processed_results.append({
                    "url": url,
                    "judgment": result['judgment'],
                    "reason": result['reason']
                })

                logger.info(f"  âœ… å‡¦ç†å®Œäº†: {result['judgment']} - {result['reason']}")
            else:
                # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—æ™‚
                processed_results.append({
                    "url": url,
                    "judgment": "ï¼Ÿ",
                    "reason": "ãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ"
                })
                logger.info(f"  âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—: {url}")

        # æœ€çµ‚çµæœã‚’ä¿å­˜
        search_results[image_id] = processed_results

        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è¨˜éŒ²ã‚’æ›´æ–°
        record["analysis_status"] = "completed"
        record["analysis_time"] = datetime.now().isoformat()
        record["found_urls_count"] = len(url_list)
        record["processed_results_count"] = len(processed_results)
        save_records()

        logger.info(f"âœ… åˆ†æå®Œäº†: image_id={image_id}, URLç™ºè¦‹={len(url_list)}ä»¶, å‡¦ç†å®Œäº†={len(processed_results)}ä»¶")

        return {
            "success": True,
            "image_id": image_id,
            "found_urls_count": len(url_list),
            "processed_results_count": len(processed_results),
            "results": processed_results,
            "analysis_time": record["analysis_time"],
            "message": f"Webæ¤œç´¢ãƒ»åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚{len(url_list)}ä»¶ã®URLãŒè¦‹ã¤ã‹ã‚Šã€{len(processed_results)}ä»¶ã‚’åˆ†æã—ã¾ã—ãŸã€‚"
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Webæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")

        # ã‚¨ãƒ©ãƒ¼çŠ¶æ…‹ã‚’è¨˜éŒ²
        record["analysis_status"] = "failed"
        record["analysis_error"] = str(e)
        record["analysis_time"] = datetime.now().isoformat()
        save_records()

        raise HTTPException(
            status_code=500,
            detail={
                "error": "search_failed",
                "message": f"Webæ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
                "image_id": image_id
            }
        )

@app.get("/results")
async def get_all_results():
    """ã™ã¹ã¦ã®æ¤œç´¢çµæœã‚’å–å¾—ã™ã‚‹"""
    return {
        "success": True,
        "total_searches": len(search_results),
        "results": search_results
    }

@app.get("/results/{image_id}")
async def get_search_results(image_id: str):
    """ç‰¹å®šã®image_idã®æ¤œç´¢çµæœã‚’å–å¾—ã™ã‚‹"""
    if image_id not in upload_records:
        raise HTTPException(
            status_code=404,
            detail={"error": "image_not_found", "message": "æŒ‡å®šã•ã‚ŒãŸimage_idã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è¨˜éŒ²ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"}
        )

    record = upload_records[image_id]

    # åˆ†æãŒã¾ã ã€ã¾ãŸã¯å¤±æ•—ã—ã¦ã„ã‚‹å ´åˆ
    if record.get("analysis_status") != "completed":
        return {
            "success": True,
            "image_id": image_id,
            "analysis_status": record.get("analysis_status", "not_started"),
            "message": "åˆ†æãŒå®Œäº†ã—ã¦ã„ã¾ã›ã‚“ã€‚å…ˆã«åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚",
            "details": record.get("analysis_error")
        }

    # åˆ†æã¯å®Œäº†ã—ãŸãŒã€æœ‰åŠ¹ãªçµæœãŒ0ä»¶ã ã£ãŸå ´åˆ
    if record.get("processed_results_count", 0) == 0:
        return {
            "success": True,
            "image_id": image_id,
            "analysis_status": "completed_no_results",
            "message": "åˆ†æã¯å®Œäº†ã—ã¾ã—ãŸãŒã€æœ‰åŠ¹ãªWebãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚",
            "found_urls_count": record.get("found_urls_count", 0),
            "processed_results_count": 0,
            "results": []
        }

    # æ­£å¸¸ãªçµæœã‚’è¿”ã™
    return {
        "success": True,
        "image_id": image_id,
        "analysis_status": "completed",
        "original_filename": record.get("original_filename", "ä¸æ˜"),
        "analysis_time": record.get("analysis_time", "ä¸æ˜"),
        "found_urls_count": record.get("found_urls_count", 0),
        "processed_results_count": record.get("processed_results_count", 0),
        "results": search_results.get(image_id, [])
    }

# ãƒ†ã‚¹ãƒˆç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.get("/test-search")
async def test_search():
    """ãƒ€ãƒŸãƒ¼ç”»åƒã§æ¤œç´¢ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã™ã‚‹"""
    logger.info("ğŸ§ª ãƒ†ã‚¹ãƒˆæ¤œç´¢é–‹å§‹")

    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
    test_image_id = "test-" + str(uuid.uuid4())

    # ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ€ãƒŸãƒ¼çµæœ
    dummy_results = [
        {
            "url": "https://amazon.co.jp/test-book",
            "domain": "amazon.co.jp",
            "title": "ãƒ†ã‚¹ãƒˆæ›¸ç± - Amazon",
            "source": "Amazon Japan",
            "is_official": True,
            "threat_level": "safe",
            "detailed_analysis": {
                "status": "safe",
                "reason": "å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ã§ã™",
                "content_analysis": None
            },
            "thumbnail": "https://example.com/thumb.jpg",
            "analysis_timestamp": datetime.now().isoformat()
        },
        {
            "url": "https://suspicious-site.com/free-download",
            "domain": "suspicious-site.com",
            "title": "ç„¡æ–™ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ - ç–‘ã‚ã—ã„ã‚µã‚¤ãƒˆ",
            "source": "Suspicious Site",
            "is_official": False,
            "threat_level": "suspicious",
            "detailed_analysis": {
                "status": "suspicious",
                "reason": "ç–‘ã‚ã—ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: ç„¡æ–™ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                "content_analysis": "åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆï¼ˆä¸€éƒ¨ï¼‰: ç„¡æ–™ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¯ã“ã¡ã‚‰..."
            },
            "thumbnail": "https://example.com/thumb2.jpg",
            "analysis_timestamp": datetime.now().isoformat()
        }
    ]

    # ãƒ†ã‚¹ãƒˆçµæœã‚’ãƒ¡ãƒ¢ãƒªã«ä¿å­˜
    search_results[test_image_id] = dummy_results

    logger.info(f"âœ… ãƒ†ã‚¹ãƒˆæ¤œç´¢å®Œäº†: {len(dummy_results)}ä»¶ã®çµæœ")

    return {
        "success": True,
        "test_image_id": test_image_id,
        "results_count": len(dummy_results),
        "message": f"ãƒ†ã‚¹ãƒˆæ¤œç´¢ãŒå®Œäº†ã—ã¾ã—ãŸã€‚{len(dummy_results)}ä»¶ã®çµæœãŒã‚ã‚Šã¾ã™ã€‚",
        "test_results": dummy_results
    }

@app.get("/test-domain/{domain}")
async def test_domain_analysis(domain: str):
    """æŒ‡å®šã•ã‚ŒãŸãƒ‰ãƒ¡ã‚¤ãƒ³ã®åˆ¤å®šãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã™ã‚‹ï¼ˆæ–°ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å¯¾å¿œï¼‰"""
    logger.info(f"ğŸ§ª ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆé–‹å§‹: {domain}")

    # ãƒ†ã‚¹ãƒˆç”¨URL
    test_url = f"https://{domain}"

    try:
        # ãƒšãƒ¼ã‚¸å†…å®¹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        content = scrape_page_content(test_url)

        if content:
            # Geminiã§åˆ¤å®š
            result = judge_content_with_gemini(content)

            logger.info(f"âœ… ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆå®Œäº†: {domain} -> {result['judgment']}")

            return {
                "success": True,
                "domain": domain,
                "test_url": test_url,
                "judgment": result['judgment'],
                "reason": result['reason'],
                "scraped_content_length": len(content),
                "test_time": datetime.now().isoformat()
            }
        else:
            logger.warning(f"âš ï¸ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—: {domain}")
            return {
                "success": False,
                "domain": domain,
                "error": "ãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ",
                "test_time": datetime.now().isoformat()
            }

    except Exception as e:
        logger.error(f"âŒ ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {
            "success": False,
            "domain": domain,
            "error": str(e),
            "test_time": datetime.now().isoformat()
        }

@app.get("/debug/logs")
async def get_debug_info():
    """ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å–å¾—ã™ã‚‹"""
    return {
        "system_status": "running",
        "total_uploads": len(upload_records),
        "total_search_results": len(search_results),
        "recent_uploads": list(upload_records.keys())[-5:] if upload_records else [],
        "api_keys_status": {
            "gemini_api_key": GEMINI_API_KEY is not None,
            "google_vision_api": GOOGLE_APPLICATION_CREDENTIALS is not None
        },
        "vision_api_status": "active",
        "timestamp": datetime.now().isoformat()
    }

@app.get("/logs")
async def get_system_logs():
    """ã‚·ã‚¹ãƒ†ãƒ ãƒ­ã‚°ã‚’å–å¾—ã™ã‚‹"""
    logger.info(f"ğŸ“‹ ãƒ­ã‚°å–å¾—è¦æ±‚: {len(system_logs)}ä»¶ã®ãƒ­ã‚°")
    return {
        "success": True,
        "total_logs": len(system_logs),
        "logs": system_logs[-50:],  # æœ€æ–°50ä»¶ã‚’è¿”ã™
        "timestamp": datetime.now().isoformat()
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)