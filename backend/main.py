from fastapi import FastAPI, File, UploadFile, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
import gc
import os
import json
import uuid
import re
import logging
from datetime import datetime
from typing import Dict, List, Optional
from io import BytesIO, StringIO
from dotenv import load_dotenv
from PIL import Image
import httpx
from bs4 import BeautifulSoup
from google.cloud import vision
import google.generativeai as genai
import hashlib
import csv
from urllib.parse import urlparse
from fastapi.responses import Response

# ãƒ­ã‚°è¨­å®šï¼ˆæœ€åˆã«è¨­å®šï¼‰
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# SerpAPIçµ±åˆç”¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import imagehash
    import requests
    from serpapi import GoogleSearch
    SERPAPI_SUPPORT = True
    logger.info("âœ… SerpAPIæ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
except ImportError:
    SERPAPI_SUPPORT = False
    logger.warning("âš ï¸ SerpAPIé–¢é€£ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚pip install google-search-results imagehash ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")

# PDFå‡¦ç†ç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    import fitz  # PyMuPDF
    PDF_SUPPORT = True
    logger.info("âœ… PDFå‡¦ç†æ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã§ã™ (PyMuPDF)")
except ImportError:
    PDF_SUPPORT = False
    logger.warning("âš ï¸ PDFå‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚pip install PyMuPDF ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")

# ãƒ­ã‚°ä¿å­˜ç”¨ï¼ˆãƒ¡ãƒ¢ãƒªå†…ï¼‰
system_logs = []
MAX_LOGS = 100  # æœ€å¤§ä¿å­˜ãƒ­ã‚°æ•°

class ListHandler(logging.Handler):
    """ãƒ­ã‚°ã‚’ãƒªã‚¹ãƒˆã«ä¿å­˜ã™ã‚‹ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    def emit(self, record):
        log_entry = {
            "timestamp": datetime.fromtimestamp(record.created).strftime("%Y-%m-%d %H:%M:%S"),
            "level": record.levelname,
            "message": record.getMessage()
        }
        system_logs.append(log_entry)
        # å¤ã„ãƒ­ã‚°ã‚’å‰Šé™¤
        if len(system_logs) > MAX_LOGS:
            system_logs.pop(0)

# ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¿½åŠ 
list_handler = ListHandler()
logger.addHandler(list_handler)

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

app = FastAPI(title="Book Leak Detector", version="1.0.0")

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å¿…è¦ãªAPI_KEYã‚’å–å¾—
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GOOGLE_APPLICATION_CREDENTIALS = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
X_BEARER_TOKEN = os.getenv("X_BEARER_TOKEN")
SERP_API_KEY = os.getenv("SERPAPI_KEY")

# Gemini APIã®è¨­å®š
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    logger.info("âœ… Gemini APIè¨­å®šå®Œäº†")
else:
    logger.warning("âš ï¸ GEMINI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

# API_KEYã®è¨­å®šçŠ¶æ³ã‚’ãƒã‚§ãƒƒã‚¯
missing_keys = []
if not GEMINI_API_KEY:
    missing_keys.append("GEMINI_API_KEY")
if not GOOGLE_APPLICATION_CREDENTIALS:
    missing_keys.append("GOOGLE_APPLICATION_CREDENTIALS")
if not X_BEARER_TOKEN:
    missing_keys.append("X_BEARER_TOKEN (Twitterå†…å®¹å–å¾—ç”¨)")
if not SERP_API_KEY:
    missing_keys.append("SERPAPI_KEY (SerpAPIé€†ç”»åƒæ¤œç´¢ç”¨)")

if missing_keys:
    required_missing = [k for k in missing_keys if "ç²¾åº¦å‘ä¸Šç”¨" not in k and "ã‚ªãƒ—ã‚·ãƒ§ãƒ³" not in k]
    if required_missing:
        print(f"è­¦å‘Š: ä»¥ä¸‹ã®å¿…é ˆç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“: {', '.join(required_missing)}")
    optional_missing = [k for k in missing_keys if "ç²¾åº¦å‘ä¸Šç”¨" in k or "ã‚ªãƒ—ã‚·ãƒ§ãƒ³" in k or "Twitterå†…å®¹å–å¾—ç”¨" in k]
    if optional_missing:
        print(f"æƒ…å ±: ä»¥ä¸‹ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“: {', '.join(optional_missing)}")
    print("å®Œå…¨ãªæ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€.envãƒ•ã‚¡ã‚¤ãƒ«ã§ä»¥ä¸‹ã‚’è¨­å®šã—ã¦ãã ã•ã„:")
    print("- GEMINI_API_KEY: Gemini AIç”¨")
    print("- GOOGLE_APPLICATION_CREDENTIALS: Google Vision APIç”¨ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼")
    print("- X_BEARER_TOKEN: X APIç”¨ï¼ˆTwitterå†…å®¹å–å¾—ï¼‰")
    print("- SERPAPI_KEY: SerpAPIç”¨ï¼ˆé€†ç”»åƒæ¤œç´¢ï¼‰")
else:
    print("âœ“ å¿…è¦ãªAPI_KEYãŒæ­£å¸¸ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã™")



# CORSè¨­å®š - æœ¬ç•ªç’°å¢ƒå¯¾å¿œ
allowed_origins = [
    "http://localhost:3000",
    "http://localhost:5173",
    "http://localhost:5174",
    "https://fujisan-leak-detector.onrender.com",  # Render ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ URLï¼ˆäºˆå‚™ï¼‰
    "https://fujisan-leak-detector-1.onrender.com",  # å®Ÿéš›ã®ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ URL
]

# ç’°å¢ƒå¤‰æ•°ã§CORSã‚ªãƒªã‚¸ãƒ³ã‚’è¿½åŠ å¯èƒ½
if cors_origins := os.getenv("CORS_ORIGINS"):
    allowed_origins.extend(cors_origins.split(","))

app.add_middleware(
    CORSMiddleware,
    allow_origins=allowed_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
UPLOAD_DIR = "uploads"
os.makedirs(UPLOAD_DIR, exist_ok=True)

# é™çš„ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®šï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”»åƒç”¨ï¼‰
app.mount("/uploads", StaticFiles(directory=UPLOAD_DIR), name="uploads")

# ä¸€æ™‚çš„ãªç”»åƒå…¬é–‹ç”¨ï¼ˆæ¤œç´¢æ™‚ã®ã¿ä½¿ç”¨ï¼‰
app.mount("/temp-images", StaticFiles(directory=UPLOAD_DIR), name="temp-images")

# ãƒ¡ãƒ¢ãƒªå†…ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼ˆæœ¬ç•ªç’°å¢ƒã§ã¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨ï¼‰
upload_records: Dict[str, Dict] = {}
search_results: Dict[str, Dict] = {}

# JSONãƒ•ã‚¡ã‚¤ãƒ«ã§ã®æ°¸ç¶šåŒ–
RECORDS_FILE = "upload_records.json"
HISTORY_FILE = "history.json"

# ãƒ¡ãƒ¢ãƒªå†…å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
analysis_history: List[Dict] = []

# ãƒãƒƒãƒå‡¦ç†çŠ¶æ³ç®¡ç†
batch_jobs: Dict[str, Dict] = {}

def load_records():
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨˜éŒ²ã‚’èª­ã¿è¾¼ã¿"""
    global upload_records
    try:
        if os.path.exists(RECORDS_FILE):
            with open(RECORDS_FILE, 'r', encoding='utf-8') as f:
                upload_records = json.load(f)
    except Exception as e:
        print(f"è¨˜éŒ²ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
        upload_records = {}

def save_records():
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜éŒ²ã‚’ä¿å­˜"""
    try:
        with open(RECORDS_FILE, 'w', encoding='utf-8') as f:
            json.dump(upload_records, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"è¨˜éŒ²ã®ä¿å­˜ã«å¤±æ•—: {e}")

def load_history():
    """å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å±¥æ­´ã‚’èª­ã¿è¾¼ã¿"""
    global analysis_history
    try:
        if os.path.exists(HISTORY_FILE):
            with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                analysis_history = json.load(f)
                logger.info(f"ğŸ“š å±¥æ­´èª­ã¿è¾¼ã¿å®Œäº†: {len(analysis_history)}ä»¶")
    except Exception as e:
        logger.error(f"å±¥æ­´ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
        analysis_history = []

def save_history():
    """å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã«å±¥æ­´ã‚’ä¿å­˜"""
    try:
        with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(analysis_history, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"å±¥æ­´ã®ä¿å­˜ã«å¤±æ•—: {e}")

def generate_search_method_summary(raw_urls: list) -> dict:
    """æ¤œç´¢æ–¹æ³•åˆ¥ã®çµ±è¨ˆæƒ…å ±ã‚’ç”Ÿæˆï¼ˆ3ã¤ã®å–å¾—çµŒè·¯ç‰ˆï¼‰"""
    summary = {
        "å®Œå…¨ä¸€è‡´": 0,
        "éƒ¨åˆ†ä¸€è‡´": 0,
        "Google Lenså®Œå…¨ä¸€è‡´": 0,
        "ä¸æ˜": 0
    }

    for url_data in raw_urls:
        if isinstance(url_data, dict):
            search_method = url_data.get("search_method", "ä¸æ˜")

            # æ¤œç´¢æ–¹æ³•ã‚’åˆ†é¡ï¼ˆ3ã¤ã®å–å¾—çµŒè·¯ï¼‰
            if search_method == "å®Œå…¨ä¸€è‡´":
                summary["å®Œå…¨ä¸€è‡´"] += 1
            elif search_method == "éƒ¨åˆ†ä¸€è‡´":
                summary["éƒ¨åˆ†ä¸€è‡´"] += 1
            elif search_method == "Google Lenså®Œå…¨ä¸€è‡´":
                summary["Google Lenså®Œå…¨ä¸€è‡´"] += 1
            else:
                summary["ä¸æ˜"] += 1
        else:
            summary["ä¸æ˜"] += 1

    return summary

def calculate_image_hash(image_content: bytes) -> str:
    """
    ç”»åƒã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰SHA-256ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—
    åŒã˜ç”»åƒã‚’è­˜åˆ¥ã™ã‚‹ãŸã‚ã«ä½¿ç”¨
    """
    return hashlib.sha256(image_content).hexdigest()

def save_analysis_to_history(image_id: str, image_hash: str, results: List[Dict]):
    """
    åˆ†æçµæœã‚’å±¥æ­´ã«ä¿å­˜
    """
    global analysis_history

    if image_id not in upload_records:
        return

    upload_record = upload_records[image_id]

    history_entry = {
        "history_id": str(uuid.uuid4()),
        "image_id": image_id,
        "image_hash": image_hash,
        "original_filename": upload_record.get("original_filename", "ä¸æ˜"),
        "analysis_date": datetime.now().isoformat(),
        "analysis_timestamp": int(datetime.now().timestamp()),
        "found_urls_count": upload_record.get("found_urls_count", 0),
        "processed_results_count": len(results),
        "results": results
    }

    analysis_history.append(history_entry)
    save_history()
    logger.info(f"ğŸ“š å±¥æ­´ã«ä¿å­˜: {image_id} ({len(results)}ä»¶ã®çµæœ)")

def get_previous_analysis(image_hash: str, exclude_history_id: Optional[str] = None) -> Dict | None:
    """
    åŒã˜ç”»åƒãƒãƒƒã‚·ãƒ¥ã®éå»ã®åˆ†æçµæœã‚’å–å¾—ï¼ˆæœ€æ–°ã®ã‚‚ã®ï¼‰
    """
    matching_histories = [
        h for h in analysis_history
        if h.get("image_hash") == image_hash and h.get("history_id") != exclude_history_id
    ]

    if not matching_histories:
        return None

    # æœ€æ–°ã®åˆ†æçµæœã‚’è¿”ã™
    return max(matching_histories, key=lambda x: x.get("analysis_timestamp", 0))

def calculate_diff(current_results: List[Dict], previous_results: List[Dict]) -> Dict:
    """
    ç¾åœ¨ã®çµæœã¨éå»ã®çµæœã®å·®åˆ†ã‚’è¨ˆç®—
    """
    # URLã‚’ã‚­ãƒ¼ã¨ã—ã¦ãƒãƒƒãƒ—ã‚’ä½œæˆ
    current_urls = {r["url"]: r for r in current_results}
    previous_urls = {r["url"]: r for r in previous_results}

    # æ–°è¦URLï¼ˆç¾åœ¨ã«ã‚ã‚‹ãŒéå»ã«ãªã„ï¼‰
    new_urls = []
    for url in current_urls:
        if url not in previous_urls:
            new_urls.append(current_urls[url])

    # æ¶ˆå¤±URLï¼ˆéå»ã«ã‚ã‚‹ãŒç¾åœ¨ã«ãªã„ï¼‰
    disappeared_urls = []
    for url in previous_urls:
        if url not in current_urls:
            disappeared_urls.append(previous_urls[url])

    # åˆ¤å®šå¤‰æ›´URLï¼ˆä¸¡æ–¹ã«ã‚ã‚‹ãŒåˆ¤å®šãŒå¤‰ã‚ã£ãŸï¼‰
    changed_urls = []
    for url in current_urls:
        if url in previous_urls:
            current_judgment = current_urls[url].get("judgment", "ï¼Ÿ")
            previous_judgment = previous_urls[url].get("judgment", "ï¼Ÿ")
            if current_judgment != previous_judgment:
                changed_urls.append({
                    "url": url,
                    "current": current_urls[url],
                    "previous": previous_urls[url]
                })

    return {
        "new_urls": new_urls,
        "disappeared_urls": disappeared_urls,
        "changed_urls": changed_urls,
        "has_changes": len(new_urls) > 0 or len(disappeared_urls) > 0 or len(changed_urls) > 0,
        "total_new": len(new_urls),
        "total_disappeared": len(disappeared_urls),
        "total_changed": len(changed_urls)
    }

# ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã«è¨˜éŒ²ã¨å±¥æ­´ã‚’èª­ã¿è¾¼ã¿
load_records()
load_history()

# å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒªã‚¹ãƒˆã¯å‰Šé™¤ï¼ˆGemini AIã§å‹•çš„åˆ¤å®šï¼‰

# Vision APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã§åˆæœŸåŒ–ï¼ˆRenderå¯¾å¿œï¼‰
try:
    import json
    from google.oauth2 import service_account

    # ã¾ãš GOOGLE_APPLICATION_CREDENTIALS_JSON ã‚’ç¢ºèª
    google_credentials_json = os.getenv("GOOGLE_APPLICATION_CREDENTIALS_JSON")
    if google_credentials_json:
        credentials_info = json.loads(google_credentials_json)
        credentials = service_account.Credentials.from_service_account_info(credentials_info)
        vision_client = vision.ImageAnnotatorClient(credentials=credentials)
        logger.info("âœ… Google Vision APIèªè¨¼å®Œäº†ï¼ˆGOOGLE_APPLICATION_CREDENTIALS_JSONï¼‰")
    else:
        # GOOGLE_APPLICATION_CREDENTIALS ã®å€¤ã‚’ç¢ºèª
        google_credentials = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
        if google_credentials:
            # JSONæ–‡å­—åˆ—ã‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚’åˆ¤å®š
            if google_credentials.strip().startswith('{'):
                # JSONæ–‡å­—åˆ—ã¨ã—ã¦å‡¦ç†
                credentials_info = json.loads(google_credentials)
                credentials = service_account.Credentials.from_service_account_info(credentials_info)
                vision_client = vision.ImageAnnotatorClient(credentials=credentials)
                logger.info("âœ… Google Vision APIèªè¨¼å®Œäº†ï¼ˆGOOGLE_APPLICATION_CREDENTIALS JSONå½¢å¼ï¼‰")
            else:
                # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¨ã—ã¦å‡¦ç†
                if os.path.exists(google_credentials):
                    vision_client = vision.ImageAnnotatorClient()
                    logger.info("âœ… Google Vision APIèªè¨¼å®Œäº†ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼‰")
                else:
                    logger.warning(f"âš ï¸ èªè¨¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {google_credentials}")
                    vision_client = None
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆèªè¨¼ã‚’è©¦è¡Œ
            vision_client = vision.ImageAnnotatorClient()
            logger.info("âœ… Google Vision APIèªè¨¼å®Œäº†ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆèªè¨¼ï¼‰")
except Exception as e:
    logger.warning(f"âš ï¸ Google Vision APIåˆæœŸåŒ–å¤±æ•—: {e}")
    vision_client = None

# Geminiãƒ¢ãƒ‡ãƒ«ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã§åˆæœŸåŒ–
if GEMINI_API_KEY:
    try:
        gemini_model = genai.GenerativeModel('gemini-2.5-flash')
        logger.info("âœ… Gemini ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†")
        logger.info("âœ… Gemini APIè¨­å®šç¢ºèªå®Œäº†")
    except Exception as e:
        logger.error(f"âŒ Gemini ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å¤±æ•—: {e}")
        gemini_model = None
else:
    logger.error("âŒ GEMINI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    gemini_model = None

def validate_file(file: UploadFile) -> bool:
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒæœ‰åŠ¹ãªç”»åƒã¾ãŸã¯PDFã‹ã©ã†ã‹ã‚’æ¤œè¨¼"""
    allowed_types = ["image/jpeg", "image/png", "image/jpg", "image/gif", "image/webp"]

    # PDFå¯¾å¿œ
    if PDF_SUPPORT:
        allowed_types.extend(["application/pdf"])

    return file.content_type in allowed_types

# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚
def validate_image_file(file: UploadFile) -> bool:
    """å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ã—ã¦ã„ã‚‹ï¼ˆéæ¨å¥¨ï¼‰"""
    return validate_file(file)

def convert_pdf_to_images(pdf_content: bytes) -> List[bytes]:
    """
    PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”»åƒã®ãƒªã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹
    å„ãƒšãƒ¼ã‚¸ã‚’å€‹åˆ¥ã®ç”»åƒã¨ã—ã¦è¿”ã™
    """
    images = []
    pdf_document = None

    try:
        # æ–¹æ³•1: PyMuPDF (fitz) ã‚’ä½¿ç”¨
        if 'fitz' in globals():
            logger.info("ğŸ”„ PyMuPDF ã§PDFã‚’ç”»åƒã«å¤‰æ›ä¸­...")
            pdf_document = fitz.open(stream=pdf_content, filetype="pdf")
            page_count = pdf_document.page_count  # closeå‰ã«å–å¾—
            logger.info(f"ğŸ“„ PDFç·ãƒšãƒ¼ã‚¸æ•°: {page_count}")

            for page_num in range(page_count):
                page = pdf_document[page_num]
                # é«˜å“è³ªã§PDFãƒšãƒ¼ã‚¸ã‚’ç”»åƒã«å¤‰æ› (PyMuPDF 1.26.3å¯¾å¿œ)
                pix = page.get_pixmap(dpi=200)  # type: ignore # DPIã§å“è³ªæŒ‡å®š
                img_data = pix.tobytes("png")
                images.append(img_data)
                logger.info(f"ğŸ“„ ãƒšãƒ¼ã‚¸ {page_num + 1} ã‚’ç”»åƒã«å¤‰æ›å®Œäº†")

            return images

    except Exception as e:
        logger.warning(f"âš ï¸ PyMuPDFå¤‰æ›å¤±æ•—: {e}")
        return []

    finally:
        # PDFæ–‡æ›¸ã‚’ç¢ºå®Ÿã«é–‰ã˜ã‚‹
        if pdf_document is not None:
            try:
                pdf_document.close()
                logger.debug("ğŸ”’ PDFæ–‡æ›¸ã‚¯ãƒ­ãƒ¼ã‚ºå®Œäº†")
            except Exception as e:
                logger.warning(f"âš ï¸ PDFæ–‡æ›¸ã‚¯ãƒ­ãƒ¼ã‚ºå¤±æ•—: {e}")

        # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
        gc.collect()

    logger.error("âŒ PDFã‚’ç”»åƒã«å¤‰æ›ã§ãã¾ã›ã‚“ã§ã—ãŸ")
    return []

def extract_pdf_text(pdf_content: bytes) -> str:
    """
    PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹ï¼ˆè£œåŠ©æƒ…å ±ã¨ã—ã¦ä½¿ç”¨ï¼‰
    """
    pdf_document = None

    try:
        # æ–¹æ³•1: PyMuPDF (fitz) ã‚’ä½¿ç”¨
        if 'fitz' in globals():
            logger.info("ğŸ”„ PyMuPDF ã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºä¸­...")
            pdf_document = fitz.open(stream=pdf_content, filetype="pdf")
            text = ""
            page_count = pdf_document.page_count  # closeå‰ã«å–å¾—

            for page_num in range(page_count):
                page = pdf_document[page_num]
                page_text = page.get_text()  # type: ignore
                text += f"[ãƒšãƒ¼ã‚¸ {page_num + 1}]\n{page_text}\n\n"

            return text.strip()

    except Exception as e:
        logger.warning(f"âš ï¸ PyMuPDF ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå¤±æ•—: {e}")
        return ""

    finally:
        # PDFæ–‡æ›¸ã‚’ç¢ºå®Ÿã«é–‰ã˜ã‚‹
        if pdf_document is not None:
            try:
                pdf_document.close()
                logger.debug("ğŸ”’ PDFãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º: æ–‡æ›¸ã‚¯ãƒ­ãƒ¼ã‚ºå®Œäº†")
            except Exception as e:
                logger.warning(f"âš ï¸ PDFãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º: æ–‡æ›¸ã‚¯ãƒ­ãƒ¼ã‚ºå¤±æ•—: {e}")

        # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
        gc.collect()

    return ""

def is_pdf_file(content_type: str, filename: str = "") -> bool:
    """ãƒ•ã‚¡ã‚¤ãƒ«ãŒPDFã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
    return (content_type == "application/pdf" or
            bool(filename and filename.lower().endswith('.pdf')))

# Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰é–¢æ•°ã¯å‰Šé™¤ï¼ˆä¸è¦ï¼‰

def validate_url_availability(url: str) -> bool:
    """
    URLã®æœ‰åŠ¹æ€§ã‚’äº‹å‰ã«ãƒã‚§ãƒƒã‚¯ã™ã‚‹ï¼ˆHEADãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼‰
    200ç•ªå°ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã®å ´åˆã®ã¿Trueã‚’è¿”ã™
    """
    try:
        with httpx.Client(timeout=5.0, follow_redirects=True) as client:
            response = client.head(url, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            return 200 <= response.status_code < 300
    except Exception as e:
        logger.warning(f"âš ï¸ URLæœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯å¤±æ•— {url}: {e}")
        return False

def is_reliable_domain(url: str) -> bool:
    """
    ãƒ‰ãƒ¡ã‚¤ãƒ³ãŒä¿¡é ¼ã§ãã‚‹ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹
    ç–‘ã‚ã—ã„ç”»åƒãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ã‚„æ€ªã—ã„ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’é™¤å¤–
    """
    try:
        from urllib.parse import urlparse
        parsed = urlparse(url)
        domain = parsed.netloc.lower()

        # é™¤å¤–ã™ã¹ãç”»åƒãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°/CDNãƒ‰ãƒ¡ã‚¤ãƒ³
        excluded_domains = [
            'pbs.twimg.com',
            'm.media-amazon.com',
            'img-cdn.theqoo.net',
            'i.imgur.com',
            'cdn.discordapp.com',
            'media.discordapp.net',
            'images.unsplash.com',
            'cdn.pixabay.com',
            'images.pexels.com',
            'img.freepik.com',
            'thumbs.dreamstime.com',
            'previews.123rf.com',
            'st.depositphotos.com',
            'c8.alamy.com',
            'media.gettyimages.com',
            'us.123rf.com',
            'image.shutterstock.com',
            't3.ftcdn.net',
            't4.ftcdn.net',
            'static.turbosquid.com',
            'render.fineartamerica.com'
        ]

        # é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯
        for excluded in excluded_domains:
            if excluded in domain:
                logger.info(f"â­ï¸ é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {domain}")
                return False

        # æ¥µç«¯ã«çŸ­ã„ãƒ‰ãƒ¡ã‚¤ãƒ³åã‚’é™¤å¤–ï¼ˆæ€ªã—ã„ãƒ‰ãƒ¡ã‚¤ãƒ³ã®å¯èƒ½æ€§ï¼‰
        if len(domain.replace('.', '')) < 5:
            logger.info(f"â­ï¸ çŸ­ã™ãã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {domain}")
            return False

        # æ•°å­—ã®ã¿ã®ã‚µãƒ–ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’é™¤å¤–
        if any(part.isdigit() for part in domain.split('.')):
            logger.info(f"â­ï¸ æ•°å­—ã‚µãƒ–ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {domain}")
            return False

        return True
    except Exception as e:
        logger.warning(f"âš ï¸ ãƒ‰ãƒ¡ã‚¤ãƒ³ä¿¡é ¼æ€§ãƒã‚§ãƒƒã‚¯å¤±æ•— {url}: {e}")
        return False

def estimate_urls_from_text(detected_text: str, confidence_score: float) -> list[dict]:
    """
    ãƒ†ã‚­ã‚¹ãƒˆæ¤œå‡ºçµæœã‹ã‚‰é–¢é€£URLã‚’æ¨å®šã™ã‚‹
    """
    estimated_urls = []

    # ãƒ†ã‚­ã‚¹ãƒˆã¨URLã®ãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸ï¼ˆå¤§å¹…æ‹¡å¼µï¼‰
    text_to_urls = {
        # ãƒ–ãƒ©ãƒ³ãƒ‰å
        'apple': ['https://www.apple.com'],
        'google': ['https://www.google.com'],
        'microsoft': ['https://www.microsoft.com'],
        'amazon': ['https://www.amazon.com'],
        'toyota': ['https://www.toyota.com'],
        'honda': ['https://www.honda.com'],
        'sony': ['https://www.sony.com'],
        'nintendo': ['https://www.nintendo.com'],
        'starbucks': ['https://www.starbucks.com'],
        'mcdonalds': ['https://www.mcdonalds.com'],

        # ä¸€èˆ¬çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        'iphone': ['https://www.apple.com'],
        'android': ['https://www.google.com'],
        'windows': ['https://www.microsoft.com'],
        'playstation': ['https://www.playstation.com'],
        'xbox': ['https://www.microsoft.com'],

        # æ—¥æœ¬ã®ãƒ–ãƒ©ãƒ³ãƒ‰
        'ãƒ‰ã‚³ãƒ¢': ['https://www.docomo.ne.jp'],
        'au': ['https://www.au.com'],
        'softbank': ['https://www.softbank.jp'],
        'ã‚»ãƒ–ãƒ³ã‚¤ãƒ¬ãƒ–ãƒ³': ['https://www.7-eleven.co.jp'],
        'ãƒ­ãƒ¼ã‚½ãƒ³': ['https://www.lawson.co.jp'],
        'ãƒ•ã‚¡ãƒŸãƒ': ['https://www.family.co.jp'],

        # æ—¥æœ¬ã®äººåãƒ»èŠ¸èƒ½äººï¼ˆé€†æ¤œç´¢å¯¾è±¡ï¼‰
        'å‰å³¶': ['https://www.google.com/search?q=å‰å³¶äºœç¾', 'https://seigura.com', 'https://natalie.mu'],
        'äºœç¾': ['https://www.google.com/search?q=å‰å³¶äºœç¾', 'https://seigura.com', 'https://natalie.mu'],
        'å‰å³¶äºœç¾': ['https://www.google.com/search?q=å‰å³¶äºœç¾', 'https://seigura.com', 'https://natalie.mu'],
        'ã¾ãˆã—ã¾': ['https://www.google.com/search?q=å‰å³¶äºœç¾', 'https://seigura.com'],
        'ã‚ã¿': ['https://www.google.com/search?q=å‰å³¶äºœç¾', 'https://seigura.com'],
        'maeshima': ['https://www.google.com/search?q=å‰å³¶äºœç¾', 'https://seigura.com'],
        'ami': ['https://www.google.com/search?q=å‰å³¶äºœç¾', 'https://seigura.com'],

        # ä½œå“åãƒ»ã‚¿ã‚¤ãƒˆãƒ«
        'å…¬å¥³': ['https://www.google.com/search?q=å…¬å¥³æ®¿ä¸‹ã®å®¶åº­æ•™å¸«', 'https://seigura.com'],
        'æ®¿ä¸‹': ['https://www.google.com/search?q=å…¬å¥³æ®¿ä¸‹ã®å®¶åº­æ•™å¸«', 'https://seigura.com'],
        'å®¶åº­æ•™å¸«': ['https://www.google.com/search?q=å…¬å¥³æ®¿ä¸‹ã®å®¶åº­æ•™å¸«', 'https://seigura.com'],
        'ã‚«ãƒ¬ãƒ³': ['https://www.google.com/search?q=å…¬å¥³æ®¿ä¸‹ã®å®¶åº­æ•™å¸«+ã‚«ãƒ¬ãƒ³', 'https://seigura.com'],
        'karen': ['https://www.google.com/search?q=å…¬å¥³æ®¿ä¸‹ã®å®¶åº­æ•™å¸«+ã‚«ãƒ¬ãƒ³', 'https://seigura.com'],

        # éŸ³æ¥½é–¢é€£
        'wish': ['https://www.google.com/search?q=Wish+for+you', 'https://natalie.mu', 'https://www.oricon.co.jp'],
        'ã‚¢ãƒŸãƒ¥ãƒ¬ãƒƒãƒˆ': ['https://www.google.com/search?q=ã‚¢ãƒŸãƒ¥ãƒ¬ãƒƒãƒˆ+å‰å³¶äºœç¾', 'https://natalie.mu'],
        'åŠ‡è–¬': ['https://www.google.com/search?q=åŠ‡è–¬+å‰å³¶äºœç¾', 'https://natalie.mu'],
        'amulet': ['https://www.google.com/search?q=ã‚¢ãƒŸãƒ¥ãƒ¬ãƒƒãƒˆ+å‰å³¶äºœç¾', 'https://natalie.mu'],

        # å£°å„ªãƒ»ã‚¢ãƒ‹ãƒ¡é–¢é€£ã®è©³ç´°
        'bang': ['https://www.google.com/search?q=BanG+Dream', 'https://seigura.com'],
        'dream': ['https://www.google.com/search?q=BanG+Dream', 'https://seigura.com'],
        'bangdream': ['https://www.google.com/search?q=BanG+Dream', 'https://seigura.com'],
        'ã±ã™ã¦ã‚‹': ['https://www.google.com/search?q=ã±ã™ã¦ã‚‹ã‚‰ã„ãµ', 'https://seigura.com'],
        'ã‚‰ã„ãµ': ['https://www.google.com/search?q=ã±ã™ã¦ã‚‹ã‚‰ã„ãµ', 'https://seigura.com'],
        'ãƒ—ãƒªãƒ†ã‚£': ['https://www.google.com/search?q=ãƒ—ãƒªãƒ†ã‚£ãƒªã‚ºãƒ ', 'https://seigura.com'],
        'ãƒªã‚ºãƒ ': ['https://www.google.com/search?q=ãƒ—ãƒªãƒ†ã‚£ãƒªã‚ºãƒ ', 'https://seigura.com'],
        'ã‚ªãƒ¼ãƒ­ãƒ©': ['https://www.google.com/search?q=ãƒ—ãƒªãƒ†ã‚£ãƒªã‚ºãƒ +ã‚ªãƒ¼ãƒ­ãƒ©ãƒ‰ãƒªãƒ¼ãƒ ', 'https://seigura.com'],
        'ãƒ‰ãƒªãƒ¼ãƒ ': ['https://www.google.com/search?q=ãƒ—ãƒªãƒ†ã‚£ãƒªã‚ºãƒ +ã‚ªãƒ¼ãƒ­ãƒ©ãƒ‰ãƒªãƒ¼ãƒ ', 'https://seigura.com'],
        'å¤è¦‹': ['https://www.google.com/search?q=å¤è¦‹ã•ã‚“ã¯+ã‚³ãƒŸãƒ¥ç—‡ã§ã™', 'https://seigura.com'],
        'ã‚³ãƒŸãƒ¥': ['https://www.google.com/search?q=å¤è¦‹ã•ã‚“ã¯+ã‚³ãƒŸãƒ¥ç—‡ã§ã™', 'https://seigura.com'],
        'ç—‡': ['https://www.google.com/search?q=å¤è¦‹ã•ã‚“ã¯+ã‚³ãƒŸãƒ¥ç—‡ã§ã™', 'https://seigura.com'],
        'ã‚¢ã‚µãƒ«ãƒˆ': ['https://www.google.com/search?q=ã‚¢ã‚µãƒ«ãƒˆãƒªãƒªã‚£', 'https://seigura.com'],
        'ãƒªãƒªã‚£': ['https://www.google.com/search?q=ã‚¢ã‚µãƒ«ãƒˆãƒªãƒªã‚£', 'https://seigura.com'],
        'bouquet': ['https://www.google.com/search?q=ã‚¢ã‚µãƒ«ãƒˆãƒªãƒªã‚£+BOUQUET', 'https://seigura.com'],

        # æ—¥ä»˜ãƒ»æ™‚é–“é–¢é€£
        '11æœˆ': ['https://www.google.com/search?q=11æœˆ22æ—¥+å‰å³¶äºœç¾', 'https://seigura.com'],
        '22æ—¥': ['https://www.google.com/search?q=11æœˆ22æ—¥+å‰å³¶äºœç¾', 'https://seigura.com'],
        'ç”Ÿã¾ã‚Œ': ['https://www.google.com/search?q=å‰å³¶äºœç¾+èª•ç”Ÿæ—¥', 'https://seigura.com'],
        'èª•ç”Ÿ': ['https://www.google.com/search?q=å‰å³¶äºœç¾+èª•ç”Ÿæ—¥', 'https://seigura.com'],

        # æ¥­ç•Œãƒ»è·æ¥­é–¢é€£
        'ãƒœã‚¤ã‚¹': ['https://www.google.com/search?q=ãƒœã‚¤ã‚¹ã‚­ãƒƒãƒˆ', 'https://seigura.com'],
        'ã‚­ãƒƒãƒˆ': ['https://www.google.com/search?q=ãƒœã‚¤ã‚¹ã‚­ãƒƒãƒˆ', 'https://seigura.com'],
        'æ‰€å±': ['https://www.google.com/search?q=ãƒœã‚¤ã‚¹ã‚­ãƒƒãƒˆ+æ‰€å±', 'https://seigura.com'],

        # ä¸€èˆ¬çš„ãªæ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        'æ­Œ': ['https://www.google.com/search?q=æ­Œæ‰‹'],
        'æ¥½æ›²': ['https://www.google.com/search?q=æ¥½æ›²'],
        'éŸ³æ¥½': ['https://www.google.com/search?q=éŸ³æ¥½'],
        'ãƒ©ã‚¤ãƒ–': ['https://www.google.com/search?q=ãƒ©ã‚¤ãƒ–'],
        'ã‚³ãƒ³ã‚µãƒ¼ãƒˆ': ['https://www.google.com/search?q=ã‚³ãƒ³ã‚µãƒ¼ãƒˆ'],

        # å£°å„ªãƒ»ã‚¢ãƒ‹ãƒ¡é–¢é€£
        'å£°å„ª': ['https://www.google.com/search?q=å£°å„ª'],
        'ã‚¢ãƒ‹ãƒ¡': ['https://www.google.com/search?q=ã‚¢ãƒ‹ãƒ¡'],
        'ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼': ['https://www.google.com/search?q=ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼'],
        'ãƒœã‚¤ã‚¹': ['https://www.google.com/search?q=ãƒœã‚¤ã‚¹'],

        # ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ»å‡ºç‰ˆé–¢é€£
        'é›‘èªŒ': ['https://www.google.com/search?q=é›‘èªŒ'],
        'è¨˜äº‹': ['https://www.google.com/search?q=è¨˜äº‹'],
        'ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼': ['https://www.google.com/search?q=ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼'],
        'å–æ': ['https://www.google.com/search?q=å–æ'],
    }

    # ãƒ†ã‚­ã‚¹ãƒˆã®å°æ–‡å­—åŒ–
    text_lower = detected_text.lower()

    # ãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸ã‹ã‚‰é–¢é€£URLã‚’æ¤œç´¢
    for keyword, urls in text_to_urls.items():
        if keyword.lower() in text_lower:
            for url in urls:
                # ä¿¡é ¼åº¦ã«åŸºã¥ã„ã¦åˆ†é¡
                if confidence_score >= 0.9:
                    search_method = "é«˜ä¿¡é ¼åº¦ãƒ†ã‚­ã‚¹ãƒˆ"
                    confidence = "é«˜"
                elif confidence_score >= 0.7:
                    search_method = "ä¸­ä¿¡é ¼åº¦ãƒ†ã‚­ã‚¹ãƒˆ"
                    confidence = "ä¸­"
                else:
                    search_method = "ä½ä¿¡é ¼åº¦ãƒ†ã‚­ã‚¹ãƒˆ"
                    confidence = "ä½"

                estimated_urls.append({
                    "url": url,
                    "search_method": search_method,
                    "search_source": "Text Detection",
                    "score": confidence_score,
                    "confidence": confidence,
                    "detected_text": detected_text
                })

    return estimated_urls

def reverse_search_from_detected_urls(detected_urls: list[dict]) -> list[dict]:
    """
    æ¤œå‡ºã•ã‚ŒãŸURLã‹ã‚‰é€†æ¤œç´¢ã‚’è¡Œã„ã€é–¢é€£URLã‚’ç™ºè¦‹ã™ã‚‹
    """
    reverse_results = []

    logger.info("ğŸ”„ é€†æ¤œç´¢æ©Ÿèƒ½é–‹å§‹...")

    for url_data in detected_urls:
        original_url = url_data.get("url", "")

        # Googleæ¤œç´¢URLã®å ´åˆã€æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æŠ½å‡ºã—ã¦é–¢é€£ã‚µã‚¤ãƒˆã‚’æ¨å®š
        if "google.com/search" in original_url:
            try:
                from urllib.parse import urlparse, parse_qs
                parsed = urlparse(original_url)
                query_params = parse_qs(parsed.query)
                search_query = query_params.get('q', [''])[0]

                if search_query:
                    logger.info(f"ğŸ” é€†æ¤œç´¢ã‚¯ã‚¨ãƒªç™ºè¦‹: {search_query}")

                    # æ¤œç´¢ã‚¯ã‚¨ãƒªã«åŸºã¥ã„ã¦é–¢é€£ã‚µã‚¤ãƒˆã‚’æ¨å®š
                    related_urls = estimate_related_sites_from_query(search_query)

                    for related_url in related_urls:
                        reverse_results.append({
                            "url": related_url,
                            "search_method": "é€†å¼•ãæ¤œç´¢",
                            "search_source": "Reverse Search",
                            "score": 0.7,
                            "confidence": "ä¸­",
                            "original_query": search_query
                        })
                        logger.info(f"  âœ… é€†æ¤œç´¢çµæœè¿½åŠ : {related_url}")

            except Exception as e:
                logger.warning(f"âš ï¸ é€†æ¤œç´¢å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")

    logger.info(f"âœ… é€†æ¤œç´¢å®Œäº†: {len(reverse_results)}ä»¶ã®é–¢é€£URLç™ºè¦‹")
    return reverse_results

def estimate_related_sites_from_query(search_query: str) -> list[str]:
    """
    æ¤œç´¢ã‚¯ã‚¨ãƒªã‹ã‚‰é–¢é€£ã‚µã‚¤ãƒˆã‚’æ¨å®šã™ã‚‹
    """
    related_sites = []
    query_lower = search_query.lower()

    # ã‚¯ã‚¨ãƒªãƒ™ãƒ¼ã‚¹ã®é–¢é€£ã‚µã‚¤ãƒˆæ¨å®š
    site_mappings = {
        # äººåãƒ»èŠ¸èƒ½äººé–¢é€£
        'å‰å³¶äºœç¾': [
            'https://www.oricon.co.jp',
            'https://natalie.mu',
            'https://www.animenewsnetwork.com',
            'https://seigura.com',
            'https://www.famitsu.com'
        ],
        'å£°å„ª': [
            'https://seigura.com',
            'https://www.animenewsnetwork.com',
            'https://natalie.mu',
            'https://www.oricon.co.jp'
        ],
        'éŸ³æ¥½': [
            'https://natalie.mu',
            'https://www.oricon.co.jp',
            'https://www.billboard-japan.com'
        ],
        'ã‚¢ãƒ‹ãƒ¡': [
            'https://www.animenewsnetwork.com',
            'https://natalie.mu',
            'https://www.famitsu.com'
        ],
        'ã‚²ãƒ¼ãƒ ': [
            'https://www.famitsu.com',
            'https://www.4gamer.net',
            'https://natalie.mu'
        ]
    }

    # éƒ¨åˆ†ãƒãƒƒãƒãƒ³ã‚°ã§é–¢é€£ã‚µã‚¤ãƒˆã‚’æ¤œç´¢
    for keyword, sites in site_mappings.items():
        if keyword in query_lower:
            related_sites.extend(sites)

    # é‡è¤‡é™¤å»
    return list(set(related_sites))

def cleanup_old_temp_files():
    """
    å¤ã„Google Lensä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆ1æ™‚é–“ä»¥ä¸Šå‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
    """
    try:
        import time
        current_time = time.time()
        cutoff_time = current_time - 3600  # 1æ™‚é–“å‰
        
        if not os.path.exists(UPLOAD_DIR):
            return
        
        cleaned_count = 0
        for filename in os.listdir(UPLOAD_DIR):
            if filename.startswith("google_lens_temp_"):
                file_path = os.path.join(UPLOAD_DIR, filename)
                try:
                    # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æŠ½å‡º
                    timestamp_str = filename.split("_")[3]  # google_lens_temp_{timestamp}_{uuid}
                    file_timestamp = int(timestamp_str)
                    
                    if file_timestamp < cutoff_time:
                        os.remove(file_path)
                        cleaned_count += 1
                        logger.debug(f"ğŸ§¹ å¤ã„ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {filename}")
                except (ValueError, IndexError, OSError) as e:
                    logger.warning(f"âš ï¸ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼ {filename}: {e}")
        
        if cleaned_count > 0:
            logger.info(f"ğŸ§¹ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†: {cleaned_count}ä»¶å‰Šé™¤")
    except Exception as e:
        logger.warning(f"âš ï¸ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¤±æ•—: {e}")

def calculate_multi_hash_similarity(image1: Image.Image, image2: Image.Image) -> Dict:
    """
    è¤‡æ•°ã®ãƒãƒƒã‚·ãƒ¥ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨ã—ã¦ç”»åƒã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
    ã‚ˆã‚Šé«˜ç²¾åº¦ãªã€Œå®Œå…¨ä¸€è‡´ã€åˆ¤å®šã‚’å®Ÿç¾
    """
    try:
        # è¤‡æ•°ã®ãƒãƒƒã‚·ãƒ¥ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§æ¯”è¼ƒ
        phash_dist = imagehash.phash(image1) - imagehash.phash(image2)
        dhash_dist = imagehash.dhash(image1) - imagehash.dhash(image2)
        ahash_dist = imagehash.average_hash(image1) - imagehash.average_hash(image2)

        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆå…¨ã¦ã®ãƒãƒƒã‚·ãƒ¥ãŒä½è·é›¢ã®å ´åˆã®ã¿é«˜ã‚¹ã‚³ã‚¢ï¼‰
        total_distance = phash_dist + dhash_dist + ahash_dist
        max_distance = max(phash_dist, dhash_dist, ahash_dist)

        return {
            "phash_distance": int(phash_dist),
            "dhash_distance": int(dhash_dist),
            "ahash_distance": int(ahash_dist),
            "total_distance": int(total_distance),
            "max_distance": int(max_distance),
            "is_near_exact": phash_dist <= 2 and dhash_dist <= 3 and ahash_dist <= 3 and max_distance <= 3,
            "similarity_score": max(0, 1.0 - (total_distance / 30.0))  # 30ã¯çµŒé¨“çš„ãªæœ€å¤§å€¤
        }
    except Exception as e:
        logger.warning(f"âš ï¸ ãƒãƒƒã‚·ãƒ¥è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        return {
            "phash_distance": 999,
            "dhash_distance": 999,
            "ahash_distance": 999,
            "total_distance": 999,
            "max_distance": 999,
            "is_near_exact": False,
            "similarity_score": 0.0
        }


def google_lens_exact_search(input_image_bytes: bytes) -> List[Dict]:
    """
    SerpAPI Google Lens Exact Matches APIã§å®Œå…¨ä¸€è‡´ç”»åƒã‚’å–å¾—

    Args:
        input_image_bytes (bytes): å…¥åŠ›ç”»åƒã®ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿

    Returns:
        List[Dict]: Google Lenså®Œå…¨ä¸€è‡´ã®URLãƒªã‚¹ãƒˆ
    """
    if not SERP_API_KEY or not SERPAPI_SUPPORT:
        logger.warning("âš ï¸ SerpAPIæ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        return []

    temp_file_path = None
    try:
        logger.info("ğŸ” Google Lens Exact Matches APIæ¤œç´¢é–‹å§‹")

        # 1. å…¥åŠ›ç”»åƒã®å‰å‡¦ç†
        try:
            input_image = Image.open(BytesIO(input_image_bytes))
            if input_image.mode != 'RGB':
                input_image = input_image.convert('RGB')

            # ç”»åƒå“è³ªãƒã‚§ãƒƒã‚¯
            width, height = input_image.size
            if width < 50 or height < 50:
                logger.warning("âš ï¸ å…¥åŠ›ç”»åƒãŒå°ã•ã™ãã¾ã™ï¼ˆ50x50æœªæº€ï¼‰")
                return []

            logger.info(f"ğŸ“Š å…¥åŠ›ç”»åƒè§£æ: ã‚µã‚¤ã‚º={width}x{height}")

        except Exception as e:
            logger.error(f"âŒ å…¥åŠ›ç”»åƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return []

        # 2. æ°¸ç¶šåŒ–ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆï¼ˆãƒ¯ãƒ¼ã‚«ãƒ¼å†èµ·å‹•å¯¾å¿œï¼‰
        os.makedirs(UPLOAD_DIR, exist_ok=True)
        
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ•ã‚¡ã‚¤ãƒ«åï¼ˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ç”¨ï¼‰
        import time
        timestamp = int(time.time())
        temp_filename = f"google_lens_temp_{timestamp}_{uuid.uuid4().hex[:8]}.jpg"
        temp_file_path = os.path.join(UPLOAD_DIR, temp_filename)
        logger.info(f"ğŸ“ æ°¸ç¶šåŒ–ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆäºˆå®š: {temp_file_path}")
        
        # å¤ã„ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆ1æ™‚é–“ä»¥ä¸Šå‰ï¼‰
        cleanup_old_temp_files()

        # é«˜å“è³ªã§JPEGä¿å­˜ï¼ˆGoogle Lens APIã®ç²¾åº¦å‘ä¸Šã®ãŸã‚ï¼‰
        input_image.save(temp_file_path, 'JPEG', quality=95, optimize=False)
        logger.info(f"ğŸ’¾ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†: {temp_file_path} ({os.path.getsize(temp_file_path)} bytes)")

        # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
        if not os.path.exists(temp_file_path):
            logger.error(f"âŒ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå¤±æ•—: {temp_file_path}")
            return []

        # 3. ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’HTTPã§å…¬é–‹ï¼ˆRenderå¯¾å¿œï¼‰
        render_url = os.getenv("RENDER_EXTERNAL_URL")
        if render_url:
            # Renderæœ¬ç•ªç’°å¢ƒã®å ´åˆ
            base_url = render_url.rstrip('/')
            logger.info(f"ğŸŒ Renderç’°å¢ƒä½¿ç”¨: {base_url}")
        else:
            # ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç’°å¢ƒã®å ´åˆ
            base_url = os.getenv("VITE_API_BASE_URL", "http://localhost:8000")
            logger.info(f"ğŸ  ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒä½¿ç”¨: {base_url}")

        image_url = f"{base_url}/uploads/{temp_filename}"
        logger.info(f"ğŸ“ ä¸€æ™‚ç”»åƒURL: {image_url}")

        # 4. Google Lens Exact Matches APIå®Ÿè¡Œ
        # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã¯`image`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€æœ¬ç•ªç’°å¢ƒã§ã¯`url`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
        if render_url:
            # æœ¬ç•ªç’°å¢ƒï¼ˆRenderï¼‰: urlãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
            search_params = {
                "engine": "google_lens",
                "type": "exact_matches",
                "url": image_url,  # å¤–éƒ¨ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ãªURL
                "api_key": SERP_API_KEY,
                "no_cache": True,
                "safe": "off"
            }
            logger.info(f"ğŸŒ æœ¬ç•ªç’°å¢ƒ - URLä½¿ç”¨: {image_url}")
        else:
            # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒ: imageãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
            search_params = {
                "engine": "google_lens",
                "type": "exact_matches",
                "image": temp_file_path,  # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
                "api_key": SERP_API_KEY,
                "no_cache": True,
                "safe": "off"
            }
            logger.info(f"ğŸ  ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒ - ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ä½¿ç”¨: {temp_file_path}")

        logger.info(f"ğŸ” Google Lens APIãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {search_params}")
        
        # SerpAPIãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–ï¼‰
        try:
            search = GoogleSearch(search_params)
            logger.info("ğŸŒ SerpAPI Google Lens ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
            
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ
            import signal
            import threading
            
            results = None
            exception_occurred = None
            
            def serpapi_request():
                nonlocal results, exception_occurred
                try:
                    results = search.get_dict()
                except Exception as e:
                    exception_occurred = e
            
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ããƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆ120ç§’ï¼‰
            thread = threading.Thread(target=serpapi_request)
            thread.daemon = True
            thread.start()
            thread.join(timeout=120)
            
            if thread.is_alive():
                logger.error("âŒ SerpAPI ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (120ç§’)")
                logger.info("   ğŸ“Š Google Vision APIã®çµæœã®ã¿ä½¿ç”¨ã—ã¾ã™")
                return []
            
            if exception_occurred:
                raise exception_occurred
            
            if results is None:
                logger.error("âŒ SerpAPI ãƒªã‚¯ã‚¨ã‚¹ãƒˆçµæœãŒç©º")
                logger.info("   ğŸ“Š Google Vision APIã®çµæœã®ã¿ä½¿ç”¨ã—ã¾ã™")
                return []
            
            logger.info(f"ğŸ“¡ SerpAPI ãƒ¬ã‚¹ãƒãƒ³ã‚¹å—ä¿¡: {type(results)} - ã‚­ãƒ¼: {list(results.keys()) if isinstance(results, dict) else 'Not a dict'}")
            
        except Exception as serpapi_error:
            logger.error(f"âŒ SerpAPI ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {str(serpapi_error)}")
            logger.info("   ğŸ“Š Google Vision APIã®çµæœã®ã¿ä½¿ç”¨ã—ã¾ã™")
            return []

        # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
        if "error" in results:
            error_msg = results["error"]
            logger.error(f"âŒ SerpAPI Google Lens ã‚¨ãƒ©ãƒ¼: {error_msg}")

            # ç‰¹å®šã®ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯è©³ç´°æƒ…å ±ã‚’æä¾›
            if "hasn't returned any results" in error_msg:
                logger.info("ğŸ’¡ SerpAPI Google Lensã§ä¸€è‡´ã™ã‚‹çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                logger.info("   âœ… ã“ã‚Œã¯æ­£å¸¸ãªå‹•ä½œã§ã™ï¼ˆã“ã®ç”»åƒã«å®Œå…¨ä¸€è‡´ãŒãªã„ï¼‰")
                logger.info("   ğŸ“Š Google Vision APIã®çµæœã‚’ä½¿ç”¨ã—ã¾ã™")
                # ã‚¨ãƒ©ãƒ¼ã§ã¯ãªãã€çµæœãŒç„¡ã„ã ã‘ãªã®ã§ç©ºã®é…åˆ—ã‚’è¿”ã™
                return []
            elif "quota" in error_msg.lower() or "limit" in error_msg.lower():
                logger.warning("âš ï¸ SerpAPI ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã«é”ã—ã¾ã—ãŸ")
                logger.info("   ğŸ“Š Google Vision APIã®çµæœã®ã¿ä½¿ç”¨ã—ã¾ã™")
                return []
            elif "invalid" in error_msg.lower() or "parameter" in error_msg.lower():
                logger.error("âŒ SerpAPI ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¨ãƒ©ãƒ¼ - APIè¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„")
                return []
            elif "couldn't get valid results" in error_msg.lower():
                logger.warning("âš ï¸ SerpAPI ç”»åƒå‡¦ç†å¤±æ•— - ä¸€æ™‚çš„ãªå•é¡Œã®å¯èƒ½æ€§")
                logger.info("   ğŸ’¡ åŸå› : ç”»åƒã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—ã€APIè² è·ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å•é¡Œ")
                logger.info("   ğŸ“Š Google Vision APIã®çµæœã®ã¿ä½¿ç”¨ã—ã¾ã™")
                return []
            elif "timeout" in error_msg.lower() or "slow" in error_msg.lower():
                logger.warning("âš ï¸ SerpAPI ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ - ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†æ™‚é–“è¶…é")
                return []
            else:
                logger.error(f"âŒ SerpAPI ä¸æ˜ãªã‚¨ãƒ©ãƒ¼: {error_msg}")
                logger.info("   ğŸ“Š Google Vision APIã®çµæœã®ã¿ä½¿ç”¨ã—ã¾ã™")
                return []

        # 5. exact_matchesã‚’å–å¾—
        exact_matches = results.get("exact_matches", [])
        logger.info(f"ğŸ¯ Google Lens Exact Matchesã‹ã‚‰ {len(exact_matches)} ä»¶ã®å€™è£œã‚’å–å¾—")

        # ãƒ‡ãƒãƒƒã‚°: ãƒ¬ã‚¹ãƒãƒ³ã‚¹å…¨ä½“ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆæ©Ÿå¯†æƒ…å ±ã‚’é™¤ãï¼‰
        if not exact_matches and "error" not in results:
            logger.warning(f"âš ï¸ Google Lens APIãƒ¬ã‚¹ãƒãƒ³ã‚¹è©³ç´°: {results}")
            # ä»–ã«ä½¿ç”¨å¯èƒ½ãªã‚­ãƒ¼ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            for key in results.keys():
                if key != "api_key":  # API_KEYã¯å‡ºåŠ›ã—ãªã„
                    logger.info(f"   ğŸ“‹ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚­ãƒ¼ '{key}': {type(results[key])}")

        if not exact_matches:
            logger.info("ğŸ’¡ Google Lensã§å®Œå…¨ä¸€è‡´ã™ã‚‹ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return []

        # 6. exact_matchesã‚’å‡¦ç†
        processed_results = []
        for i, match in enumerate(exact_matches):
            try:
                position = match.get("position", i + 1)
                title = match.get("title", "ã‚¿ã‚¤ãƒˆãƒ«ãªã—")
                source = match.get("source", "ã‚½ãƒ¼ã‚¹ä¸æ˜")
                link = match.get("link", "")
                thumbnail = match.get("thumbnail", "")

                # ä¾¡æ ¼æƒ…å ±ï¼ˆå•†å“ã®å ´åˆï¼‰
                price = match.get("price", "")
                extracted_price = match.get("extracted_price", 0)
                in_stock = match.get("in_stock", False)
                out_of_stock = match.get("out_of_stock", False)

                # æ—¥ä»˜æƒ…å ±
                date = match.get("date", "")

                # å®Ÿéš›ã®ç”»åƒã‚µã‚¤ã‚º
                actual_image_width = match.get("actual_image_width", 0)
                actual_image_height = match.get("actual_image_height", 0)

                if link:
                    result = {
                        "url": link,
                        "title": title,
                        "source": source,
                        "position": position,
                        "thumbnail": thumbnail,
                        "search_method": "Google Lenså®Œå…¨ä¸€è‡´",
                        "search_source": "Google Lens Exact Matches",
                        "confidence": "high",  # Google Lensã®å®Œå…¨ä¸€è‡´ã¯é«˜ä¿¡é ¼åº¦
                        "score": 1.0,  # å®Œå…¨ä¸€è‡´ãªã®ã§æœ€é«˜ã‚¹ã‚³ã‚¢
                        "actual_image_width": actual_image_width,
                        "actual_image_height": actual_image_height
                    }

                    # ä¾¡æ ¼æƒ…å ±ãŒã‚ã‚Œã°è¿½åŠ 
                    if price:
                        result["price"] = price
                        result["extracted_price"] = extracted_price
                        result["in_stock"] = in_stock
                        result["out_of_stock"] = out_of_stock

                    # æ—¥ä»˜æƒ…å ±ãŒã‚ã‚Œã°è¿½åŠ 
                    if date:
                        result["date"] = date

                    processed_results.append(result)
                    logger.info(f"âœ… Google Lenså®Œå…¨ä¸€è‡´ {position}: {title[:50]}...")

            except Exception as e:
                logger.debug(f"  âš ï¸ Google Lenså€™è£œ {i+1} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
                continue

        logger.info(f"âœ… Google Lensæ¤œç´¢å®Œäº†: {len(processed_results)}ä»¶ã®å®Œå…¨ä¸€è‡´ã‚’ç™ºè¦‹")

        return processed_results

    except Exception as e:
        logger.error(f"âŒ Google Lensæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []

    finally:
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ï¼ˆSerpAPIå®Œäº†å¾Œã«é…å»¶å‰Šé™¤ï¼‰
        if temp_file_path and os.path.exists(temp_file_path):
            try:
                # å°‘ã—å¾…ã£ã¦ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ï¼ˆSerpAPIãŒã‚¢ã‚¯ã‚»ã‚¹å®Œäº†ã™ã‚‹ã¾ã§ï¼‰
                import time
                time.sleep(1)  # 1ç§’å¾…æ©Ÿ
                os.remove(temp_file_path)
                logger.debug(f"ğŸ—‘ï¸ Google Lensä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {temp_file_path}")
            except Exception as e:
                logger.warning(f"âš ï¸ Google Lensä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤å¤±æ•—: {str(e)}")
                # å‰Šé™¤å¤±æ•—ã§ã‚‚ç¶šè¡Œï¼ˆæ¬¡å›èµ·å‹•æ™‚ã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã•ã‚Œã‚‹ï¼‰

def enhanced_image_search_with_reverse(image_content: bytes) -> list[dict]:
    """
    3ã¤ã®å–å¾—çµŒè·¯ã«ã‚ˆã‚‹ç”»åƒæ¤œç´¢
    1. Google Vision API: å®Œå…¨ä¸€è‡´ã¨éƒ¨åˆ†ä¸€è‡´ã®ã¿
    2. Google Lens API: å®Œå…¨ä¸€è‡´
    3. ï¼ˆtextdetectionã¨é€†å¼•ãæ¤œç´¢ã¯é™¤å»ï¼‰
    """
    logger.info("ğŸš€ 3ã¤ã®å–å¾—çµŒè·¯ã«ã‚ˆã‚‹ç”»åƒæ¤œç´¢é–‹å§‹")

    # 1. Google Vision APIæ¤œç´¢ï¼ˆå®Œå…¨ä¸€è‡´ã¨éƒ¨åˆ†ä¸€è‡´ã®ã¿ï¼‰
    vision_results = search_web_for_image(image_content)

    # 2. Google Lens Exact Matches APIæ¤œç´¢
    google_lens_results = google_lens_exact_search(image_content)

    # 3. çµæœã‚’çµ±åˆï¼ˆé‡è¤‡URLé™¤å»ã€Google Lenså„ªå…ˆï¼‰
    all_results = []

    # Google Lensçµæœã‚’å…ˆã«è¿½åŠ ï¼ˆå„ªå…ˆåº¦é«˜ï¼‰
    seen_urls = set()
    for result in google_lens_results:
        url = result.get("url", "")
        if url and url not in seen_urls:
            seen_urls.add(url)
            all_results.append(result)

    # Vision APIçµæœã‚’è¿½åŠ ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼‰
    for result in vision_results:
        url = result.get("url", "")
        if url and url not in seen_urls:
            seen_urls.add(url)
            all_results.append(result)

    logger.info(f"ğŸ“Š ç”»åƒæ¤œç´¢çµæœçµ±è¨ˆ:")
    logger.info(f"  - Google Vision API: {len(vision_results)}ä»¶ï¼ˆå®Œå…¨ä¸€è‡´ãƒ»éƒ¨åˆ†ä¸€è‡´ï¼‰")
    logger.info(f"  - SerpAPI Google Lens: {len(google_lens_results)}ä»¶ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰")
    logger.info(f"  - é‡è¤‡é™¤å»å¾Œåˆè¨ˆ: {len(all_results)}ä»¶")

    return all_results

def search_web_for_image(image_content: bytes) -> list[dict]:
    """
    ç”»åƒã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å—ã‘å–ã‚Šã€Google Vision APIã§
    åŒä¸€ç”»åƒãŒä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹URLã®ãƒªã‚¹ãƒˆã‚’è¿”ã™ã€‚
    å„URLã«ã¯æ¤œç´¢æ–¹æ³•ï¼ˆå®Œå…¨ä¸€è‡´/éƒ¨åˆ†ä¸€è‡´/å…ƒè¨˜äº‹æ¤œç´¢ï¼‰ã®åˆ†é¡æƒ…å ±ã‚’ä»˜ä¸ã€‚
    """
    logger.info("ğŸ” ç”»åƒæ¤œç´¢é–‹å§‹ï¼ˆVision API WEB+TEXTï¼‰")

    all_results = []

    try:
        # 1. Google Vision API
        logger.info("ğŸ“Š ã€Phase 1ã€‘Google Vision APIï¼ˆWEB+TEXTï¼‰")

        # Vision APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if not vision_client:
            logger.error("âŒ Google Vision APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            logger.error("   è¨­å®šç¢ºèª: GOOGLE_APPLICATION_CREDENTIALS ã¾ãŸã¯ GOOGLE_APPLICATION_CREDENTIALS_JSON")
            return []

        # å®Œå…¨ä¸€è‡´æ¤œå‡ºã®ãŸã‚ã®ç”»åƒå‰å‡¦ç†æœ€é©åŒ–
        logger.info(f"ğŸ–¼ï¸ ç”»åƒã‚µã‚¤ã‚º: {len(image_content)} bytes")

        # ç”»åƒå½¢å¼ã‚’ç¢ºèª
        try:
            from PIL import Image as PILImage
            import io

            pil_image = PILImage.open(io.BytesIO(image_content))
            logger.info(f"ğŸ–¼ï¸ ç”»åƒå½¢å¼: {pil_image.format}, ã‚µã‚¤ã‚º: {pil_image.size}, ãƒ¢ãƒ¼ãƒ‰: {pil_image.mode}")

            # å®Œå…¨ä¸€è‡´æ¤œå‡ºã®ãŸã‚ã®æœ€é©ã‚µã‚¤ã‚ºèª¿æ•´
            min_dimension = 800  # æœ€å°ã‚µã‚¤ã‚ºã‚’800pxã«è¨­å®š
            max_dimension = 4096  # æœ€å¤§ã‚µã‚¤ã‚ºã‚’4Kã«è¨­å®š

            current_min = min(pil_image.size)
            current_max = max(pil_image.size)

            # å°ã•ã™ãã‚‹ç”»åƒã®ã‚¢ãƒƒãƒ—ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            if current_min < min_dimension:
                scale_factor = min_dimension / current_min
                new_size = (int(pil_image.size[0] * scale_factor), int(pil_image.size[1] * scale_factor))
                pil_image = pil_image.resize(new_size, PILImage.Resampling.LANCZOS)
                logger.info(f"ğŸ”§ å®Œå…¨ä¸€è‡´ç”¨ã‚¢ãƒƒãƒ—ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°: {pil_image.size[0]}x{pil_image.size[1]} -> {new_size[0]}x{new_size[1]}")

                # ã‚¢ãƒƒãƒ—ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¾Œã®ç”»åƒã‚’ä¿å­˜
                output_buffer = io.BytesIO()
                if pil_image.mode in ('RGBA', 'LA', 'P'):
                    pil_image = pil_image.convert('RGB')
                pil_image.save(output_buffer, format='JPEG', quality=100, optimize=False, subsampling=0)
                image_content = output_buffer.getvalue()
                logger.info(f"ğŸ”§ ã‚¢ãƒƒãƒ—ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å®Œäº†: {len(image_content)} bytes")

            # å¤§ãã™ãã‚‹ç”»åƒã®ãƒ€ã‚¦ãƒ³ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            elif current_max > max_dimension:
                scale_factor = max_dimension / current_max
                new_size = (int(pil_image.size[0] * scale_factor), int(pil_image.size[1] * scale_factor))
                pil_image = pil_image.resize(new_size, PILImage.Resampling.LANCZOS)
                logger.info(f"ğŸ”§ å®Œå…¨ä¸€è‡´ç”¨ãƒ€ã‚¦ãƒ³ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°: {pil_image.size[0]}x{pil_image.size[1]} -> {new_size[0]}x{new_size[1]}")

                # ãƒ€ã‚¦ãƒ³ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¾Œã®ç”»åƒã‚’ä¿å­˜
                output_buffer = io.BytesIO()
                if pil_image.mode in ('RGBA', 'LA', 'P'):
                    pil_image = pil_image.convert('RGB')
                pil_image.save(output_buffer, format='JPEG', quality=100, optimize=False, subsampling=0)
                image_content = output_buffer.getvalue()
                logger.info(f"ğŸ”§ ãƒ€ã‚¦ãƒ³ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å®Œäº†: {len(image_content)} bytes")
            else:
                logger.info(f"âœ… ç”»åƒã‚µã‚¤ã‚ºã¯å®Œå…¨ä¸€è‡´æ¤œå‡ºã«æœ€é©: {pil_image.size[0]}x{pil_image.size[1]}")

            # Vision APIå®Œå…¨ä¸€è‡´ç²¾åº¦æœ€é©åŒ–ï¼ˆå…ƒç”»åƒå„ªå…ˆï¼‰
            original_size = len(image_content)
            max_size = 10 * 1024 * 1024  # 10MBã«æ‹¡å¤§

            # å…ƒã®ç”»åƒã‚’ãã®ã¾ã¾è©¦è¡Œï¼ˆæœ€é«˜ã®å®Œå…¨ä¸€è‡´ç²¾åº¦ã®ãŸã‚ï¼‰
            if original_size <= max_size:
                logger.info(f"âœ… å…ƒç”»åƒã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼ˆå®Œå…¨ä¸€è‡´æœ€å„ªå…ˆï¼‰: {original_size} bytes")
            else:
                logger.info(f"ğŸ”§ ç”»åƒã‚µã‚¤ã‚ºæœ€é©åŒ–ä¸­... ({original_size} -> ç›®æ¨™: < {max_size})")

                # å®Œå…¨ä¸€è‡´æ¤œå‡ºã®ãŸã‚å¯èƒ½ãªé™ã‚Šé«˜è§£åƒåº¦ã‚’ç¶­æŒ
                max_dimension = 4096  # 4Kè§£åƒåº¦ã¾ã§è¨±å¯
                if max(pil_image.size) > max_dimension:
                    ratio = max_dimension / max(pil_image.size)
                    new_size = (int(pil_image.size[0] * ratio), int(pil_image.size[1] * ratio))
                    pil_image = pil_image.resize(new_size, PILImage.Resampling.LANCZOS)
                    logger.info(f"ğŸ”§ é«˜è§£åƒåº¦ãƒªã‚µã‚¤ã‚ºå®Œäº†: {new_size}")

                # å®Œå…¨ä¸€è‡´æ¤œå‡ºã®ãŸã‚æœ€é«˜å“è³ªã§ä¿å­˜
                output_buffer = io.BytesIO()
                if pil_image.mode in ('RGBA', 'LA', 'P'):
                    pil_image = pil_image.convert('RGB')

                # å®Œå…¨ä¸€è‡´æ¤œå‡ºã«æœ€é©åŒ–ã•ã‚ŒãŸè¨­å®š
                pil_image.save(output_buffer, format='JPEG', quality=100, optimize=False,
                                 subsampling=0, progressive=False)
                image_content = output_buffer.getvalue()
                logger.info(f"ğŸ”§ å®Œå…¨ä¸€è‡´æœ€é©åŒ–å®Œäº†: {len(image_content)} bytes")

            # PNGå½¢å¼ã®å ´åˆã€JPEGå¤‰æ›ã‚‚è©¦è¡Œï¼ˆå®Œå…¨ä¸€è‡´ç²¾åº¦å‘ä¸Šï¼‰
            if pil_image.format == 'PNG' and original_size <= max_size:
                logger.info(f"ğŸ”§ PNG->JPEGå¤‰æ›ã§å®Œå…¨ä¸€è‡´ç²¾åº¦å‘ä¸Šã‚’è©¦è¡Œ...")
                jpeg_buffer = io.BytesIO()
                rgb_image = pil_image
                if pil_image.mode in ('RGBA', 'LA', 'P'):
                    # é€æ˜åº¦ã‚’ç™½èƒŒæ™¯ã§å‡¦ç†
                    rgb_image = PILImage.new('RGB', pil_image.size, (255, 255, 255))
                    if pil_image.mode == 'RGBA':
                        rgb_image.paste(pil_image, mask=pil_image.split()[-1])
                    else:
                        rgb_image.paste(pil_image)

                rgb_image.save(jpeg_buffer, format='JPEG', quality=100, optimize=False, subsampling=0)
                jpeg_content = jpeg_buffer.getvalue()

                # JPEGã®æ–¹ãŒå°ã•ã„å ´åˆã¯æ¡ç”¨
                if len(jpeg_content) < len(image_content):
                    image_content = jpeg_content
                    logger.info(f"ğŸ”§ JPEGå¤‰æ›æ¡ç”¨: {len(image_content)} bytes")
                else:
                    logger.info(f"ğŸ”§ å…ƒPNGä¿æŒ: {len(image_content)} bytes")

        except Exception as img_error:
            logger.warning(f"âš ï¸ ç”»åƒå‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {img_error}")

        image = vision.Image(content=image_content)

        # Vision API æ¤œå‡ºå®Ÿè¡Œï¼ˆWEB_DETECTIONã®ã¿ï¼‰
        logger.info("ğŸ¯ Vision API æ¤œå‡ºé–‹å§‹ï¼ˆWEB_DETECTIONç‰¹åŒ–ï¼‰")

        try:
            # WEB_DETECTIONå°‚ç”¨ã§æœ€å¤§ç²¾åº¦ã‚’è¿½æ±‚
            logger.info("ğŸŒ WEB_DETECTION å®Ÿè¡Œä¸­ï¼ˆæœ€å¤§çµæœæ•°ã§ç²¾åº¦é‡è¦–ï¼‰...")
            features = [
                vision.Feature(type_=vision.Feature.Type.WEB_DETECTION, max_results=2000)  # æœ€å¤§çµæœæ•°ã‚’å¢—åŠ 
            ]
            request = vision.AnnotateImageRequest(image=image, features=features)
            response = vision_client.annotate_image(request=request)
            logger.info("âœ… æ¤œå‡ºå®Œäº†")

            logger.info(f"ğŸ“¡ Vision API ãƒ¬ã‚¹ãƒãƒ³ã‚¹å—ä¿¡å®Œäº†")
            logger.info(f"ğŸ“‹ ãƒ¬ã‚¹ãƒãƒ³ã‚¹è©³ç´°: type={type(response)}")
            if hasattr(response, 'error'):
                error_attr = getattr(response, 'error', None)
                logger.info(f"ğŸ“‹ ã‚¨ãƒ©ãƒ¼å±æ€§å­˜åœ¨: {error_attr is not None}")
        except Exception as api_error:
            logger.error(f"âŒ Vision API å‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {api_error}")
            logger.error(f"   ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(api_error).__name__}")

            # å…·ä½“çš„ãªã‚¨ãƒ©ãƒ¼å†…å®¹ã‚’ãƒã‚§ãƒƒã‚¯
            error_str = str(api_error).lower()
            if 'quota' in error_str or 'limit' in error_str:
                logger.error("   åŸå› : APIã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã«é”ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
            elif 'permission' in error_str or 'auth' in error_str:
                logger.error("   åŸå› : èªè¨¼ã‚¨ãƒ©ãƒ¼ã¾ãŸã¯æ¨©é™ä¸è¶³ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
            elif 'billing' in error_str:
                logger.error("   åŸå› : èª²é‡‘è¨­å®šã«å•é¡ŒãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
            else:
                logger.error(f"   è©³ç´°: {api_error}")

            return []

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒæ­£å¸¸ã‹ç¢ºèª
        if not response:
            logger.error("âŒ Vision API ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒç©ºã§ã™")
            return []

        # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯ï¼ˆã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰ãŒ0ä»¥å¤–ã®å ´åˆã®ã¿ã‚¨ãƒ©ãƒ¼ã¨ã—ã¦æ‰±ã†ï¼‰
        if hasattr(response, 'error') and response.error:
            # gRPC Status ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®è©³ç´°ã‚’å–å¾—
            error_code = getattr(response.error, 'code', 'UNKNOWN')
            error_message = getattr(response.error, 'message', 'è©³ç´°ä¸æ˜')
            error_details = getattr(response.error, 'details', [])

            # ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰ãŒ0ï¼ˆOKï¼‰ä»¥å¤–ã®å ´åˆã®ã¿ã‚¨ãƒ©ãƒ¼ã¨ã—ã¦å‡¦ç†
            if error_code != 0:
                logger.error(f"âŒ Vision API ã‚¨ãƒ©ãƒ¼:")
                logger.error(f"   ã‚³ãƒ¼ãƒ‰: {error_code}")
                logger.error(f"   ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {error_message}")
                logger.error(f"   è©³ç´°: {error_details}")
                logger.error(f"   ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(response.error)}")

                # ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰ã«åŸºã¥ãå¯¾å‡¦æ³•ã®æç¤º
                if error_code == 3:  # INVALID_ARGUMENT
                    logger.error("   åŸå› : ç„¡åŠ¹ãªå¼•æ•°ï¼ˆç”»åƒå½¢å¼ã‚„å†…å®¹ã«å•é¡ŒãŒã‚ã‚‹å¯èƒ½æ€§ï¼‰")
                elif error_code == 7:  # PERMISSION_DENIED
                    logger.error("   åŸå› : æ¨©é™ä¸è¶³ï¼ˆã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®æ¨©é™ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼‰")
                elif error_code == 8:  # RESOURCE_EXHAUSTED
                    logger.error("   åŸå› : ãƒªã‚½ãƒ¼ã‚¹ä¸è¶³ï¼ˆAPIã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã«é”ã—ã¦ã„ã‚‹å¯èƒ½æ€§ï¼‰")
                elif error_code == 16:  # UNAUTHENTICATED
                    logger.error("   åŸå› : èªè¨¼ã‚¨ãƒ©ãƒ¼ï¼ˆèªè¨¼æƒ…å ±ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼‰")

                return []
            else:
                logger.info(f"âœ… Vision API ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ­£å¸¸ï¼ˆã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰: {error_code}ï¼‰")

        # WEB_DETECTIONçµæœã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        web_detection = response.web_detection if hasattr(response, 'web_detection') else None

        # WEBæ¤œå‡ºçµæœã®ä»¶æ•°ã‚’é›†è¨ˆ
        web_count = 0
        full_count = 0
        partial_count = 0
        similar_count = 0
        pages_count = 0

        if web_detection:
            full_count = len(web_detection.full_matching_images) if web_detection.full_matching_images else 0
            partial_count = len(web_detection.partial_matching_images) if web_detection.partial_matching_images else 0
            similar_count = len(web_detection.visually_similar_images) if web_detection.visually_similar_images else 0
            pages_count = len(web_detection.pages_with_matching_images) if web_detection.pages_with_matching_images else 0
            web_count = full_count + partial_count + similar_count

            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±: é¡ä¼¼ç”»åƒãŒå¤šã„ã®ã«å®Œå…¨ãƒ»éƒ¨åˆ†ä¸€è‡´ãŒ0ä»¶ã®å ´åˆ
            if similar_count > 0 and full_count == 0 and partial_count == 0:
                logger.info(f"ğŸ” ãƒ‡ãƒãƒƒã‚°: é¡ä¼¼ç”»åƒ{similar_count}ä»¶ã‚ã‚Šã€å®Œå…¨ãƒ»éƒ¨åˆ†ä¸€è‡´0ä»¶")
                logger.info("   - ç”»åƒã®å“è³ªã‚„è§£åƒåº¦ãŒå½±éŸ¿ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
                logger.info("   - ã¾ãŸã¯ã€ã“ã®ç”»åƒãŒéå¸¸ã«æ–°ã—ã„/ç‰¹æ®Šãªç”»åƒã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")

        logger.info(f"ğŸ“ˆ Vision APIæ¤œå‡ºçµæœï¼ˆWEB_DETECTIONç‰¹åŒ–ã€é¡ä¼¼ç”»åƒé™¤å¤–ï¼‰:")
        logger.info(f"  - å®Œå…¨ä¸€è‡´ç”»åƒ: {full_count}ä»¶")
        logger.info(f"  - éƒ¨åˆ†ä¸€è‡´ç”»åƒ: {partial_count}ä»¶")
        logger.info(f"  - é¡ä¼¼ç”»åƒ: {similar_count}ä»¶ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
        logger.info(f"  - é–¢é€£ãƒšãƒ¼ã‚¸: {pages_count}ä»¶")
        logger.info(f"  - æœ‰åŠ¹æ¤œå‡º: {full_count + partial_count + pages_count}ä»¶")

        # 1-1. WEB_DETECTION: å®Œå…¨ä¸€è‡´ç”»åƒã‹ã‚‰URLåé›†
        if web_detection and web_detection.full_matching_images:
            logger.info(f"ğŸ¯ å®Œå…¨ä¸€è‡´ç”»åƒã‹ã‚‰URLæŠ½å‡ºä¸­... ({len(web_detection.full_matching_images)}ä»¶ç™ºè¦‹)")
            for i, img in enumerate(web_detection.full_matching_images):
                logger.info(f"   ğŸ“‹ å®Œå…¨ä¸€è‡´ç”»åƒ {i+1}: URL={getattr(img, 'url', 'ãªã—')}, Score={getattr(img, 'score', 'ãªã—')}")
                if img.url and img.url.startswith(('http://', 'https://')):
                    result = {
                        "url": img.url,
                        "search_method": "å®Œå…¨ä¸€è‡´",
                        "search_source": "Vision API",
                        "score": getattr(img, 'score', 1.0),
                        "confidence": "é«˜"
                    }
                    all_results.append(result)
                    logger.info(f"  âœ… å®Œå…¨ä¸€è‡´ç”»åƒè¿½åŠ : {img.url}")

                    # seigura.comã‚„NTTãƒ‰ã‚³ãƒ¢ã®æ¤œå‡ºç¢ºèª
                    if "seigura.com" in img.url.lower():
                        logger.info(f"  ğŸ¯ seigura.comæ¤œå‡ºæˆåŠŸï¼: {img.url}")
                    elif "ntt" in img.url.lower() or "docomo" in img.url.lower():
                        logger.info(f"  ğŸ¯ NTTãƒ‰ã‚³ãƒ¢æ¤œå‡ºæˆåŠŸï¼: {img.url}")
                else:
                    logger.warning(f"  âš ï¸ å®Œå…¨ä¸€è‡´ç”»åƒã®URLãŒç„¡åŠ¹: {getattr(img, 'url', 'ãªã—')}")
        else:
            logger.info("ğŸ’¡ å®Œå…¨ä¸€è‡´ç”»åƒãŒ0ä»¶ã§ã—ãŸ")

        # 1-2. WEB_DETECTION: éƒ¨åˆ†ä¸€è‡´ç”»åƒã‹ã‚‰URLåé›†ï¼ˆé©å¿œçš„ã‚¹ã‚³ã‚¢é–¾å€¤ï¼‰
        if web_detection and web_detection.partial_matching_images:
            logger.info(f"ğŸ¯ éƒ¨åˆ†ä¸€è‡´ç”»åƒã‹ã‚‰URLæŠ½å‡ºä¸­... ({len(web_detection.partial_matching_images)}ä»¶ç™ºè¦‹)")

            # ã‚¹ã‚³ã‚¢åˆ†å¸ƒã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
            scores = [getattr(img, 'score', 0.0) for img in web_detection.partial_matching_images if img.url]
            if scores:
                max_score = max(scores)
                min_score = min(scores)
                avg_score = sum(scores) / len(scores)
                logger.info(f"  ğŸ“Š éƒ¨åˆ†ä¸€è‡´ã‚¹ã‚³ã‚¢åˆ†å¸ƒ: æœ€é«˜={max_score:.4f}, æœ€ä½={min_score:.4f}, å¹³å‡={avg_score:.4f}")

            # é©å¿œçš„é–¾å€¤è¨­å®šï¼ˆçµæœãŒ0ä»¶ã«ãªã‚‰ãªã„ã‚ˆã†èª¿æ•´ï¼‰
            adaptive_threshold = 0.01  # åŸºæœ¬é–¾å€¤ã‚’å¤§å¹…ã«ä¸‹ã’ã‚‹
            if scores and max(scores) < 0.05:
                adaptive_threshold = min_score  # æœ€ä½ã‚¹ã‚³ã‚¢ã§ã‚‚æ¡ç”¨
                logger.info(f"  ğŸ”§ é©å¿œçš„é–¾å€¤é©ç”¨: {adaptive_threshold:.4f} (å…¨çµæœæ¡ç”¨ãƒ¢ãƒ¼ãƒ‰)")

            filtered_count = 0
            for i, img in enumerate(web_detection.partial_matching_images):
                if img.url and img.url.startswith(('http://', 'https://')):
                    score = getattr(img, 'score', 0.0)
                    logger.info(f"  ğŸ” éƒ¨åˆ†ä¸€è‡´å€™è£œ {i+1}: score={score:.4f}, url={img.url}")

                    if score >= adaptive_threshold:
                        img_confidence = "é«˜" if score >= 0.5 else "ä¸­" if score >= 0.1 else "ä½"
                        img_result = {
                            "url": img.url,
                            "search_method": "éƒ¨åˆ†ä¸€è‡´",
                            "search_source": "Vision API",
                            "score": score,
                            "confidence": img_confidence
                        }
                        all_results.append(img_result)
                        logger.info(f"  âœ… éƒ¨åˆ†ä¸€è‡´ç”»åƒè¿½åŠ  (score: {score:.4f}): {img.url}")
                    else:
                        filtered_count += 1
                        logger.info(f"  âŒ ã‚¹ã‚³ã‚¢ä¸è¶³ã§ã‚¹ã‚­ãƒƒãƒ— (score: {score:.4f}): {img.url}")

            logger.info(f"  ğŸ“Š éƒ¨åˆ†ä¸€è‡´çµæœ: æ¡ç”¨={len(web_detection.partial_matching_images)-filtered_count}ä»¶, é™¤å¤–={filtered_count}ä»¶")
        else:
            logger.info("ğŸ’¡ éƒ¨åˆ†ä¸€è‡´ç”»åƒãŒ0ä»¶ã§ã—ãŸ")

        # 1-3. é¡ä¼¼ç”»åƒã¯å‰Šé™¤ï¼ˆä½¿ã„ç‰©ã«ãªã‚‰ãªã„ãŸã‚ï¼‰
        if web_detection and web_detection.visually_similar_images:
            logger.info(f"â­ï¸ é¡ä¼¼ç”»åƒã‚’ã‚¹ã‚­ãƒƒãƒ— ({len(web_detection.visually_similar_images)}ä»¶ç™ºè¦‹ã€å“è³ªãŒä½ã„ãŸã‚é™¤å¤–)")
        else:
            logger.info("ğŸ’¡ é¡ä¼¼ç”»åƒãŒ0ä»¶ã§ã—ãŸ")

        # 1-4. WEB_DETECTION: é–¢é€£ãƒšãƒ¼ã‚¸ã‹ã‚‰URLåé›†ï¼ˆé©å¿œçš„ã‚¹ã‚³ã‚¢é–¾å€¤ï¼‰
        if web_detection and web_detection.pages_with_matching_images:
            logger.info(f"ğŸ¯ é–¢é€£ãƒšãƒ¼ã‚¸ã‹ã‚‰URLæŠ½å‡ºä¸­... ({len(web_detection.pages_with_matching_images)}ä»¶ç™ºè¦‹)")

            # ã‚¹ã‚³ã‚¢åˆ†å¸ƒã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
            page_scores = [getattr(page, 'score', 0.0) for page in web_detection.pages_with_matching_images if page.url]
            if page_scores:
                max_score = max(page_scores)
                min_score = min(page_scores)
                avg_score = sum(page_scores) / len(page_scores)
                logger.info(f"  ğŸ“Š é–¢é€£ãƒšãƒ¼ã‚¸ã‚¹ã‚³ã‚¢åˆ†å¸ƒ: æœ€é«˜={max_score:.4f}, æœ€ä½={min_score:.4f}, å¹³å‡={avg_score:.4f}")

            # é©å¿œçš„é–¾å€¤è¨­å®šï¼ˆä¸Šä½10ä»¶ç¨‹åº¦ã‚’ç›®æ¨™ï¼‰
            page_threshold = 0.001  # éå¸¸ã«ä½ã„é–¾å€¤
            if page_scores:
                sorted_scores = sorted(page_scores, reverse=True)
                if len(sorted_scores) >= 10:
                    page_threshold = sorted_scores[9]  # ä¸Šä½10ä»¶ç›®ã®ã‚¹ã‚³ã‚¢
                    logger.info(f"  ğŸ”§ é–¢é€£ãƒšãƒ¼ã‚¸é©å¿œçš„é–¾å€¤: {page_threshold:.4f} (ä¸Šä½10ä»¶æ¡ç”¨)")
                else:
                    page_threshold = min_score
                    logger.info(f"  ğŸ”§ é–¢é€£ãƒšãƒ¼ã‚¸é©å¿œçš„é–¾å€¤: {page_threshold:.4f} (å…¨çµæœæ¡ç”¨)")

            pages_filtered_count = 0
            for i, page in enumerate(web_detection.pages_with_matching_images):
                if page.url and page.url.startswith(('http://', 'https://')):
                    score = getattr(page, 'score', 0.0)
                    logger.info(f"  ğŸ” é–¢é€£ãƒšãƒ¼ã‚¸å€™è£œ {i+1}: score={score:.4f}, url={page.url}")

                    if score >= page_threshold:
                        page_confidence = "é«˜" if score >= 0.3 else "ä¸­" if score >= 0.1 else "ä½"
                        page_result = {
                            "url": page.url,
                            "search_method": "é–¢é€£ãƒšãƒ¼ã‚¸",
                            "search_source": "Vision API",
                            "score": score,
                            "confidence": page_confidence
                        }
                        all_results.append(page_result)
                        logger.info(f"  âœ… é–¢é€£ãƒšãƒ¼ã‚¸è¿½åŠ  (score: {score:.4f}): {page.url}")
                    else:
                        pages_filtered_count += 1
                        logger.info(f"  âŒ é–¢é€£ãƒšãƒ¼ã‚¸ã‚¹ã‚³ã‚¢ä¸è¶³ (score: {score:.4f}): {page.url}")

            logger.info(f"  ğŸ“Š é–¢é€£ãƒšãƒ¼ã‚¸çµæœ: æ¡ç”¨={len(web_detection.pages_with_matching_images)-pages_filtered_count}ä»¶, é™¤å¤–={pages_filtered_count}ä»¶")
        else:
            logger.info("ğŸ’¡ é–¢é€£ãƒšãƒ¼ã‚¸ãŒ0ä»¶ã§ã—ãŸ")

        # 1-3. TEXT_DETECTIONæ©Ÿèƒ½ã¯å‰Šé™¤ï¼ˆç²¾åº¦ãŒä½ã„ãŸã‚ï¼‰
        logger.info(f"ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆæ¤œå‡ºæ©Ÿèƒ½ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆç²¾åº¦å‘ä¸Šã®ãŸã‚ç„¡åŠ¹åŒ–ï¼‰")


        # çµæœæ•°åˆ¶å¾¡ï¼ˆ5-10ä»¶ç¨‹åº¦ã«èª¿æ•´ï¼‰
        target_result_count = 8  # ç›®æ¨™çµæœæ•°
        if len(all_results) > target_result_count:
            logger.info(f"ğŸ”§ çµæœæ•°åˆ¶å¾¡: {len(all_results)}ä»¶ -> {target_result_count}ä»¶ã«èª¿æ•´")

            # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
            all_results.sort(key=lambda x: x.get('score', 0.0), reverse=True)

            # ä¸Šä½çµæœã‚’é¸æŠï¼ˆå®Œå…¨ä¸€è‡´ã¯å¿…ãšå«ã‚ã‚‹ï¼‰
            filtered_results = []
            complete_matches = [r for r in all_results if r['search_method'] == 'å®Œå…¨ä¸€è‡´']
            other_results = [r for r in all_results if r['search_method'] != 'å®Œå…¨ä¸€è‡´']

            # å®Œå…¨ä¸€è‡´ã‚’å…¨ã¦è¿½åŠ 
            filtered_results.extend(complete_matches)

            # æ®‹ã‚Šæ ã«ä»–ã®çµæœã‚’è¿½åŠ 
            remaining_slots = target_result_count - len(complete_matches)
            if remaining_slots > 0:
                filtered_results.extend(other_results[:remaining_slots])

            all_results = filtered_results
            logger.info(f"  ğŸ¯ æœ€çµ‚é¸æŠ: å®Œå…¨ä¸€è‡´={len(complete_matches)}ä»¶, ãã®ä»–={len(filtered_results)-len(complete_matches)}ä»¶")

        # æœ€çµ‚çµ±è¨ˆï¼ˆVision APIç‰¹åŒ–ã€é¡ä¼¼ç”»åƒé™¤å¤–ï¼‰
        final_results_count = len(all_results)
        logger.info(f"âœ… Vision APIæ¤œå‡ºå®Œäº†: {final_results_count}ä»¶ã®URLå–å¾—")
        logger.info(f"  - å®Œå…¨ä¸€è‡´: {len([r for r in all_results if r['search_method'] == 'å®Œå…¨ä¸€è‡´'])}ä»¶")
        logger.info(f"  - éƒ¨åˆ†ä¸€è‡´: {len([r for r in all_results if r['search_method'] == 'éƒ¨åˆ†ä¸€è‡´'])}ä»¶")
        logger.info(f"  - é–¢é€£ãƒšãƒ¼ã‚¸: {len([r for r in all_results if r['search_method'] == 'é–¢é€£ãƒšãƒ¼ã‚¸'])}ä»¶")

        # é‡è¤‡é™¤å»ã®ã¿ï¼ˆä¿¡é ¼æ€§ãƒ»æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯ã¯å‰Šé™¤ï¼‰
        logger.info("ğŸ”§ URLé‡è¤‡é™¤å»é–‹å§‹...")
        logger.info(f"ğŸ” é‡è¤‡é™¤å»å‰ã®ç·URLæ•°: {len(all_results)}ä»¶")

        filtered_results = []
        seen_urls = set()
        duplicate_count = 0

        for result in all_results:
            url = result["url"]

            if url in seen_urls:
                duplicate_count += 1
                continue
            seen_urls.add(url)

            # å…¨URLã‚’å–å¾—URLä¸€è¦§ã«å«ã‚ã‚‹ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãªã—ï¼‰
            filtered_results.append(result)
            logger.info(f"  âœ… URLè¿½åŠ  [{result['search_method']}]: {url}")

            # æœ€å¤§100ä»¶ã«æ‹¡å¼µï¼ˆå…¨ã¦å–å¾—ã™ã‚‹ãŸã‚ï¼‰
            if len(filtered_results) >= 100:
                break

        logger.info(f"ğŸ§¹ é‡è¤‡é™¤å»çµ±è¨ˆ: é‡è¤‡é™¤å»={duplicate_count}ä»¶")
        logger.info(f"ğŸŒ æœ€çµ‚çš„ã«å–å¾—ã•ã‚ŒãŸURL: {len(filtered_results)}ä»¶")

        # æ¤œç´¢æ–¹æ³•åˆ¥ã®çµ±è¨ˆ
        method_stats = {}
        for result in filtered_results:
            method = result["search_method"]
            method_stats[method] = method_stats.get(method, 0) + 1

        logger.info(f"ğŸ“Š æ¤œç´¢æ–¹æ³•åˆ¥å†…è¨³:")
        for method, count in method_stats.items():
            logger.info(f"  - {method}: {count}ä»¶")

        # ã‚ˆã‚Šè©³ç´°ãªçµ±è¨ˆ
        logger.info(f"  - å…¨æ¤œç´¢ç¯„å›²åˆè¨ˆ: {len(filtered_results)}ä»¶")

        # ä¸Šä½10ä»¶ã‚’ãƒ­ã‚°å‡ºåŠ›
        for i, result in enumerate(filtered_results[:10]):
            logger.info(f"  {i+1}: [{result['search_method']}] {result['url']}")

        if len(filtered_results) > 10:
            logger.info(f"  ... ä»– {len(filtered_results) - 10}ä»¶")

        return filtered_results

    except Exception as e:
        logger.error(f"âŒ ç”»åƒæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []

def is_reliable_domain_relaxed(url: str) -> bool:
    """
    ãƒ‰ãƒ¡ã‚¤ãƒ³ä¿¡é ¼æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€ä½é™ã®é™¤å¤–ã®ã¿ï¼‰
    æœ¬æ¥ã®è¶£æ—¨ï¼šæ€ªã—ã„ãƒ‰ãƒ¡ã‚¤ãƒ³ã“ãAIåˆ¤å®šã§æ‚ªç”¨ãƒã‚§ãƒƒã‚¯ã™ã‚‹ãŸã‚ã€é™¤å¤–ã¯æœ€å°é™ã«
    """
    try:
        from urllib.parse import urlparse
        parsed = urlparse(url)
        domain = parsed.netloc.lower()

        # æœ€ä½é™ã®é™¤å¤–ï¼šæ˜ã‚‰ã‹ã«ç”»åƒã‚µãƒ¼ãƒ“ã‚¹ã®ã¿
        image_only_domains = [
            'i.imgur.com',
            'cdn.discordapp.com',
            'media.discordapp.net',
            'images.unsplash.com',
            'cdn.pixabay.com',
            'images.pexels.com',
        ]

        # ç”»åƒã‚µãƒ¼ãƒ“ã‚¹ã®ã¿é™¤å¤–ï¼ˆä»–ã¯ã™ã¹ã¦AIåˆ¤å®šå¯¾è±¡ï¼‰
        for image_domain in image_only_domains:
            if image_domain in domain:
                logger.info(f"â­ï¸ ç”»åƒã‚µãƒ¼ãƒ“ã‚¹ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {domain}")
                return False

        # ãã®ä»–ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã¯ã™ã¹ã¦é€šã™ï¼ˆæ‚ªç”¨ãƒã‚§ãƒƒã‚¯ã®ãŸã‚ï¼‰
        return True
    except Exception as e:
        logger.warning(f"âš ï¸ ãƒ‰ãƒ¡ã‚¤ãƒ³ä¿¡é ¼æ€§ãƒã‚§ãƒƒã‚¯å¤±æ•— {url}: {e}")
        return True  # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯é€šã™

# search_with_serpapié–¢æ•°ã‚’å‰Šé™¤

def get_x_tweet_content(tweet_url: str) -> dict | None:
    """
    Xï¼ˆTwitterï¼‰ã®ãƒ„ã‚¤ãƒ¼ãƒˆURLã‹ã‚‰æŠ•ç¨¿å†…å®¹ã¨ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæƒ…å ±ã‚’å–å¾—
    X API v2ã®Bearer Tokenèªè¨¼ã‚’ä½¿ç”¨
    """
    if not X_BEARER_TOKEN:
        logger.warning("âš ï¸ X_BEARER_TOKEN ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ãƒ„ã‚¤ãƒ¼ãƒˆå†…å®¹å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—")
        return None

    try:
        import re
        from urllib.parse import urlparse

        # ãƒ„ã‚¤ãƒ¼ãƒˆIDã‚’æŠ½å‡º
        tweet_id_match = re.search(r'/status/(\d+)', tweet_url)
        if not tweet_id_match:
            logger.warning(f"âš ï¸ ãƒ„ã‚¤ãƒ¼ãƒˆIDã‚’æŠ½å‡ºã§ãã¾ã›ã‚“: {tweet_url}")
            return None

        tweet_id = tweet_id_match.group(1)
        logger.info(f"ğŸ¦ X API ãƒ„ã‚¤ãƒ¼ãƒˆå†…å®¹å–å¾—é–‹å§‹: ID={tweet_id}")

        # X API v2ã§ãƒ„ã‚¤ãƒ¼ãƒˆå†…å®¹ã‚’å–å¾—ï¼ˆBearer Tokenèªè¨¼ï¼‰
        headers = {
            'Authorization': f'Bearer {X_BEARER_TOKEN}',
            'Content-Type': 'application/json'
        }

        with httpx.Client(timeout=10.0) as client:
            response = client.get(
                f"https://api.twitter.com/2/tweets/{tweet_id}",
                headers=headers,
                params={
                    'tweet.fields': 'text,author_id,created_at,public_metrics',
                    'user.fields': 'username,name,description,public_metrics',
                    'expansions': 'author_id'
                }
            )
            response.raise_for_status()

            data = response.json()

            if 'data' not in data:
                logger.warning(f"âš ï¸ ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {tweet_id}")
                return None

            tweet_data = data['data']
            user_data = None

            # ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’å–å¾—
            if 'includes' in data and 'users' in data['includes']:
                user_data = data['includes']['users'][0]

            # çµæœã‚’æ§‹é€ åŒ–
            result = {
                'tweet_id': tweet_id,
                'tweet_text': tweet_data.get('text', ''),
                'author_id': tweet_data.get('author_id', ''),
                'created_at': tweet_data.get('created_at', ''),
                'public_metrics': tweet_data.get('public_metrics', {}),
                'username': user_data.get('username', '') if user_data else '',
                'display_name': user_data.get('name', '') if user_data else '',
                'user_description': user_data.get('description', '') if user_data else '',
                'user_metrics': user_data.get('public_metrics', {}) if user_data else {}
            }

            logger.info(f"âœ… X APIå–å¾—æˆåŠŸ: @{result['username']} - {result['tweet_text'][:50]}...")
            return result

    except httpx.HTTPStatusError as e:
        if e.response.status_code == 401:
            logger.error("âŒ X APIèªè¨¼ã‚¨ãƒ©ãƒ¼: Bearer TokenãŒç„¡åŠ¹ã¾ãŸã¯æœŸé™åˆ‡ã‚Œã§ã™")
        elif e.response.status_code == 403:
            logger.error("âŒ X APIæ¨©é™ã‚¨ãƒ©ãƒ¼: ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“")
        elif e.response.status_code == 404:
            logger.error("âŒ ãƒ„ã‚¤ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆå‰Šé™¤æ¸ˆã¿ã¾ãŸã¯éå…¬é–‹ï¼‰")
        else:
            logger.error(f"âŒ X API HTTPã‚¨ãƒ©ãƒ¼: {e.response.status_code} - {e.response.text}")
        return None
    except Exception as e:
        logger.error(f"âŒ X APIä¸€èˆ¬ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

def judge_x_content_with_gemini(x_data: dict) -> dict:
    """
    Xï¼ˆTwitterï¼‰ã®æŠ•ç¨¿å†…å®¹ã¨ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæƒ…å ±ã‚’Gemini AIã§åˆ¤å®š
    """
    if not gemini_model:
        logger.warning("âš ï¸ Gemini ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return {
            "judgment": "ï¼Ÿ",
            "reason": "Gemini AIãŒåˆ©ç”¨ã§ãã¾ã›ã‚“",
            "confidence": "ä¸æ˜"
        }

    try:
        # XæŠ•ç¨¿ã®è©³ç´°æƒ…å ±ã‚’æ§‹ç¯‰
        username = x_data.get('username', 'ä¸æ˜')
        display_name = x_data.get('display_name', 'ä¸æ˜')
        tweet_text = x_data.get('tweet_text', '')
        user_description = x_data.get('user_description', '')

        # ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ãªã©ã®æŒ‡æ¨™
        user_metrics = x_data.get('user_metrics', {})
        followers_count = user_metrics.get('followers_count', 0)
        following_count = user_metrics.get('following_count', 0)
        tweet_count = user_metrics.get('tweet_count', 0)

        # æŠ•ç¨¿ã®æŒ‡æ¨™
        public_metrics = x_data.get('public_metrics', {})
        retweet_count = public_metrics.get('retweet_count', 0)
        like_count = public_metrics.get('like_count', 0)
        reply_count = public_metrics.get('reply_count', 0)

        # Geminiç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ï¼ˆçŸ­ç¸®ç‰ˆï¼‰
        prompt = f"""
ã€XæŠ•ç¨¿åˆ†æã€‘
ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ: @{username} ({display_name})
ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼: {followers_count:,}äºº
æŠ•ç¨¿å†…å®¹: {tweet_text[:500]}

è‘—ä½œæ¨©ä¾µå®³ãƒ»é•æ³•ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

åˆ¤å®šåŸºæº–ï¼š
â—‹ï¼ˆå®‰å…¨ï¼‰: å…¬å¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã€æ­£å½“ãªæŠ•ç¨¿
Ã—ï¼ˆå±é™ºï¼‰: è‘—ä½œæ¨©ä¾µå®³ã€é•æ³•ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€æµ·è³Šç‰ˆ
ï¼Ÿï¼ˆä¸æ˜ï¼‰: åˆ¤å®šå›°é›£

å›ç­”å½¢å¼: "åˆ¤å®š:[â—‹/Ã—/?] ç†ç”±:[150å­—ä»¥å†…ã®ç°¡æ½”ãªç†ç”±]"
å¿…ãš150å­—ä»¥å†…ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
"""

        logger.info("ğŸ¤– Gemini AI XæŠ•ç¨¿åˆ¤å®šé–‹å§‹")
        response = gemini_model.generate_content(prompt)

        if not response or not response.text:
            logger.warning("âš ï¸ Gemini AIã‹ã‚‰ã®å¿œç­”ãŒç©ºã§ã™")
            return {
                "judgment": "ï¼Ÿ",
                "reason": "AIå¿œç­”ãŒç©ºã§ã—ãŸ",
                "confidence": "ä¸æ˜"
            }

        response_text = response.text.strip()
        logger.info(f"ğŸ“‹ Gemini XæŠ•ç¨¿åˆ¤å®šå¿œç­”: {response_text}")

        # å¿œç­”ã‚’è§£æ
        judgment = "ï¼Ÿ"
        reason = "åˆ¤å®šã§ãã¾ã›ã‚“ã§ã—ãŸ"

        if "åˆ¤å®š:" in response_text and "ç†ç”±:" in response_text:
            parts = response_text.split("ç†ç”±:")
            judgment_part = parts[0].replace("åˆ¤å®š:", "").strip()
            reason = parts[1].strip()

            if "â—‹" in judgment_part:
                judgment = "â—‹"
            elif "Ã—" in judgment_part:
                judgment = "Ã—"
            else:
                judgment = "ï¼Ÿ"
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è§£æ
            if "â—‹" in response_text:
                judgment = "â—‹"
            elif "Ã—" in response_text:
                judgment = "Ã—"
            reason = response_text

        # ç†ç”±ã‚’300å­—ä»¥å†…ã«åˆ¶é™
        if len(reason) > 300:
            reason = reason[:297] + "..."
            logger.info(f"ğŸ“ XæŠ•ç¨¿åˆ¤å®šç†ç”±ã‚’300å­—ã«çŸ­ç¸®ã—ã¾ã—ãŸ")

        # ä¿¡é ¼åº¦ã‚’è¨­å®š
        confidence = "é«˜" if judgment in ["â—‹", "Ã—"] else "ä½"

        logger.info(f"âœ… Gemini XæŠ•ç¨¿åˆ¤å®šå®Œäº†: {judgment} - {reason[:50]}...")

        return {
            "judgment": judgment,
            "reason": reason,
            "confidence": confidence,
            "x_data": x_data  # å…ƒãƒ‡ãƒ¼ã‚¿ã‚‚ä¿æŒ
        }

    except Exception as e:
        logger.error(f"âŒ Gemini XæŠ•ç¨¿åˆ¤å®šã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {
            "judgment": "ï¼Ÿ",
            "reason": f"åˆ¤å®šã‚¨ãƒ©ãƒ¼: {str(e)}",
            "confidence": "ä¸æ˜"
        }

def validate_url_availability_fast(url: str) -> bool:
    """
    URLã®æœ‰åŠ¹æ€§ã‚’é«˜é€Ÿãƒã‚§ãƒƒã‚¯ï¼ˆå³æ ¼ç‰ˆï¼‰
    ç™½ç´™ãƒšãƒ¼ã‚¸ã‚„ç„¡åŠ¹ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’äº‹å‰ã«é™¤å¤–
    Twitterç”»åƒURLã¯ç‰¹åˆ¥å‡¦ç†ã®ãŸã‚é€šã™
    """
    try:
        # Twitterç”»åƒURLã¯ç‰¹åˆ¥å‡¦ç†ã®ãŸã‚ã€æ¤œè¨¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦é€šã™
        if 'pbs.twimg.com' in url:
            logger.info(f"ğŸ¦ Twitterç”»åƒURLæ¤œå‡º - ç‰¹åˆ¥å‡¦ç†ã®ãŸã‚é€šé: {url}")
            return True

        with httpx.Client(timeout=5.0, follow_redirects=True) as client:
            # 1. HEADãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª
            try:
                head_response = client.head(url, headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                })

                # 4xx/5xxã‚¨ãƒ©ãƒ¼ã¯å³åº§ã«é™¤å¤–
                if head_response.status_code >= 400:
                    logger.info(f"âŒ HTTPã‚¨ãƒ©ãƒ¼ {head_response.status_code}: {url}")
                    return False

                # Content-Typeãƒã‚§ãƒƒã‚¯
                content_type = head_response.headers.get('content-type', '').lower()
                if content_type and 'text/html' not in content_type:
                    logger.info(f"âŒ éHTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ ({content_type}): {url}")
                    return False

            except httpx.RequestError:
                # HEADãŒå¤±æ•—ã—ãŸå ´åˆã¯GETã§å†è©¦è¡Œ
                pass

            # 2. GETãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æœ‰åŠ¹æ€§ã‚’ç¢ºèª
            response = client.get(url, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })

            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
            if not (200 <= response.status_code < 300):
                logger.info(f"âŒ ç„¡åŠ¹ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ {response.status_code}: {url}")
                return False

            # Content-Typeã®æœ€çµ‚ç¢ºèª
            content_type = response.headers.get('content-type', '').lower()
            if 'text/html' not in content_type:
                logger.info(f"âŒ éHTMLãƒ¬ã‚¹ãƒãƒ³ã‚¹ ({content_type}): {url}")
                return False

            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å®Ÿè³ªæ€§ãƒã‚§ãƒƒã‚¯
            content_length = len(response.text.strip())
            if content_length < 100:  # 100æ–‡å­—æœªæº€ã¯ç©ºç™½ãƒšãƒ¼ã‚¸ã¨ã¿ãªã™
                logger.info(f"âŒ ç©ºç™½ãƒšãƒ¼ã‚¸ (é•·ã•: {content_length}): {url}")
                return False

            # ç©ºç™½ãƒšãƒ¼ã‚¸ã‚„ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ã®å…¸å‹çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
            content_lower = response.text.lower()
            error_indicators = [
                'page not found',
                'not found',
                '404',
                'error',
                'page does not exist',
                'pÃ¡gina no encontrada',  # ã‚¹ãƒšã‚¤ãƒ³èªã®ã€Œãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€
                'no se encontrÃ³',
                'sin contenido',
                'empty page',
                'blank page'
            ]

            for indicator in error_indicators:
                if indicator in content_lower and content_length < 1000:
                    logger.info(f"âŒ ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸æ¤œå‡º ('{indicator}'): {url}")
                    return False

            logger.info(f"âœ… æœ‰åŠ¹ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç¢ºèª: {url}")
            return True

    except httpx.TimeoutException:
        logger.info(f"âŒ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {url}")
        return False
    except httpx.RequestError as e:
        logger.info(f"âŒ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {url} - {e}")
        return False
    except Exception as e:
        logger.warning(f"âš ï¸ URLæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {url} - {e}")
        return False

def is_trusted_news_domain(url: str) -> bool:
    """
    ä¿¡é ¼ã§ãã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»å‡ºç‰ˆç³»ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ãƒã‚§ãƒƒã‚¯
    ã“ã‚Œã‚‰ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã¯Geminiåˆ¤å®šã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ç›´æ¥â—‹åˆ¤å®š
    """
    try:
        from urllib.parse import urlparse
        parsed = urlparse(url)
        domain = parsed.netloc.lower()

                # ä¿¡é ¼ã§ãã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»å‡ºç‰ˆãƒ»å…¬å¼ã‚µã‚¤ãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³
        trusted_domains = [
            # ä¸»è¦ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ»æ–°è
            'news.yahoo.co.jp', 'www.nhk.or.jp', 'nhk.or.jp', 'www3.nhk.or.jp',
            'mainichi.jp', 'www.mainichi.jp', 'www.asahi.com', 'asahi.com',
            'www.yomiuri.co.jp', 'yomiuri.co.jp', 'www.sankei.com', 'sankei.com',
            'www.nikkei.com', 'nikkei.com', 'www.jiji.com', 'jiji.com',
            'www.kyodo.co.jp', 'kyodo.co.jp', 'www.tokyo-np.co.jp', 'tokyo-np.co.jp',

            # çµŒæ¸ˆãƒ»ãƒ“ã‚¸ãƒã‚¹
            'toyokeizai.net', 'www.toyokeizai.net', 'diamond.jp', 'www.diamond.jp',
            'gendai.media', 'www.gendai.media', 'president.jp', 'www.president.jp',

            # å‡ºç‰ˆãƒ»ãƒ¡ãƒ‡ã‚£ã‚¢
            'bunshun.jp', 'www.bunshun.jp', 'shinchosha.co.jp', 'www.shinchosha.co.jp',
            'kadokawa.co.jp', 'www.kadokawa.co.jp', 'www.shogakukan.co.jp', 'shogakukan.co.jp',
            'www.shueisha.co.jp', 'shueisha.co.jp', 'www.kodansha.co.jp', 'kodansha.co.jp',

            # ITãƒ»ãƒ†ãƒƒã‚¯
            'www.itmedia.co.jp', 'itmedia.co.jp', 'www.impress.co.jp', 'impress.co.jp',
            'ascii.jp', 'www.ascii.jp', 'internet.watch.impress.co.jp', 'gigazine.net',
            'www.gigazine.net', 'techcrunch.com', 'jp.techcrunch.com',

            # ã‚²ãƒ¼ãƒ ãƒ»ã‚¨ãƒ³ã‚¿ãƒ¡
            'www.4gamer.net', '4gamer.net', 'www.famitsu.com', 'famitsu.com',
            'www.dengeki.com', 'dengeki.com', 'natalie.mu', 'www.natalie.mu',
            'comic-natalie.natalie.mu', 'music-natalie.natalie.mu', 'game-natalie.natalie.mu',
            'www.oricon.co.jp', 'oricon.co.jp', 'www.animeanime.jp', 'animeanime.jp',

            # æ›¸åº—ãƒ»EC
            'www.amazon.co.jp', 'amazon.co.jp', 'books.rakuten.co.jp', 'rakuten.co.jp',
            'honto.jp', 'www.honto.jp', 'www.kinokuniya.co.jp', 'kinokuniya.co.jp',
            'www.tsutaya.co.jp', 'tsutaya.co.jp', 'www.yodobashi.com', 'yodobashi.com',

            # ãƒ©ã‚¤ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«ãƒ»ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³
            'more.hpplus.jp', 'www.vogue.co.jp', 'vogue.co.jp', 'www.elle.com', 'elle.com',
            'www.cosmopolitan.com', 'cosmopolitan.com', 'mi-mollet.com', 'www.25ans.jp',
            'cancam.jp', 'www.cancam.jp', 'ray-web.jp', 'www.biteki.com', 'biteki.com'
        ]

        # å®Œå…¨ä¸€è‡´ãƒã‚§ãƒƒã‚¯
        if domain in trusted_domains:
            return True

        # ã‚µãƒ–ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’å«ã‚€éƒ¨åˆ†ä¸€è‡´ãƒã‚§ãƒƒã‚¯
        for trusted in trusted_domains:
            if domain.endswith('.' + trusted) or domain == trusted:
                return True

        # æ¥½å¤©ãƒ»Amazonã®éƒ¨åˆ†ä¸€è‡´ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ˆã‚Šåºƒç¯„å›²ã«å¯¾å¿œï¼‰
        trusted_patterns = [
            'rakuten.co.jp',  # search.rakuten.co.jp, books.rakuten.co.jp ãªã©
            'amazon.co.jp',  # www.amazon.co.jp ãªã©
            'amazon.com',  # www.amazon.com ãªã©
        ]

        for pattern in trusted_patterns:
            if pattern in domain:
                logger.info(f"âœ… ä¿¡é ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ä¸€è‡´: {pattern} in {domain}")
                return True

        return False
    except Exception as e:
        logger.warning(f"âš ï¸ ãƒ‰ãƒ¡ã‚¤ãƒ³ä¿¡é ¼æ€§ãƒã‚§ãƒƒã‚¯å¤±æ•— {url}: {e}")
        return False

def convert_twitter_image_to_tweet_url(url: str) -> dict | None:
    """
    Twitterç”»åƒURLï¼ˆpbs.twimg.comï¼‰ã‹ã‚‰å…ƒãƒ„ã‚¤ãƒ¼ãƒˆã®URLã¨å†…å®¹ã‚’å–å¾—ã‚’è©¦ã¿ã‚‹
    pbs.twimg.comç”»åƒURLã‹ã‚‰ãƒ„ã‚¤ãƒ¼ãƒˆIDã‚’æ¨å®šã—ã€å…ƒã®ãƒ„ã‚¤ãƒ¼ãƒˆURLã‚’è¿”ã™
    """
    try:
        from urllib.parse import urlparse
        parsed = urlparse(url)

        # Twitterç”»åƒURLã®å ´åˆ
        if 'pbs.twimg.com' in parsed.netloc:
            logger.info(f"ğŸ¦ Twitterç”»åƒURLæ¤œå‡º: {url}")

            # X APIã¾ãŸã¯SerpAPIãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã€ãƒ„ã‚¤ãƒ¼ãƒˆæ¤œç´¢ã‚’è©¦è¡Œ
            # if X_BEARER_TOKEN or (SERPAPI_KEY and SerpAPI_available): # SERPAPI_KEY ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
            if X_BEARER_TOKEN:
                tweet_result = get_x_tweet_url_and_content_by_image(url)
                if tweet_result:
                    return tweet_result

                # æ¤œç´¢ã§è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆã§ã‚‚ã€Geminiã«ç”»åƒã®æ€§è³ªã‚’ä¼ãˆã‚‹
                logger.info("ğŸ¦ ãƒ„ã‚¤ãƒ¼ãƒˆå†…å®¹ã¯ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸãŒã€Twitterç”»åƒã¨ã—ã¦å‡¦ç†")
                return {
                    "tweet_url": None,
                    "content": f"TWITTER_IMAGE_UNKNOWN:{url}"
                }

            # APIåˆ©ç”¨ä¸å¯ã®å ´åˆã¯å¾“æ¥ã®å‡¦ç†
            return {
                "tweet_url": None,
                "content": f"TWITTER_IMAGE:{url}"
            }

        return None
    except Exception as e:
        logger.warning(f"âš ï¸ Twitter URLå¤‰æ›å¤±æ•— {url}: {e}")
        return None

def get_x_tweet_url_and_content_by_image(image_url: str) -> dict | None:
    """
    ç”»åƒURLã‹ã‚‰ãƒ„ã‚¤ãƒ¼ãƒˆURLã¨å†…å®¹ã‚’æ¢ç´¢ã™ã‚‹ï¼ˆé«˜åº¦ç‰ˆï¼‰
    Google Vision API + X API v2ã‚’çµ„ã¿åˆã‚ã›ã¦ãƒ„ã‚¤ãƒ¼ãƒˆã‚’ç‰¹å®š
    æˆ»ã‚Šå€¤: {"tweet_url": "https://x.com/...", "content": "ãƒ„ã‚¤ãƒ¼ãƒˆå†…å®¹"}
    """
    try:
        logger.info(f"ğŸ¦ ç”»åƒURLçµŒç”±ã§ãƒ„ã‚¤ãƒ¼ãƒˆURLæ¤œç´¢: {image_url}")

        # æ–¹æ³•1: Google Vision APIã®WEB_DETECTIONã‚’ä½¿ç”¨
        if vision_client:
            try:
                logger.info("ğŸ” Google Vision APIã§WEB_DETECTIONå®Ÿè¡Œä¸­...")

                # ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                import httpx
                with httpx.Client(timeout=10.0) as client:
                    response = client.get(image_url)
                    if response.status_code == 200:
                        image_content = response.content

                        # Vision APIå®Ÿè¡Œ
                        from google.cloud import vision
                        image = vision.Image(content=image_content)
                        response = vision_client.web_detection(image=image)  # type: ignore

                        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç¢ºèª
                        if not response or not response.web_detection:
                            logger.warning("âš ï¸ Vision APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒç„¡åŠ¹")
                            return None

                        # é–¢é€£ãƒšãƒ¼ã‚¸ã‹ã‚‰ X/Twitter URLã‚’æ¢ç´¢
                        if response.web_detection.pages_with_matching_images:
                            for page in response.web_detection.pages_with_matching_images[:15]:
                                if page.url and any(domain in page.url for domain in ['x.com', 'twitter.com']):
                                    logger.info(f"ğŸ¦ Vision APIã§ãƒ„ã‚¤ãƒ¼ãƒˆURLç™ºè¦‹: {page.url}")
                                    tweet_content = get_x_tweet_content(page.url)
                                    if tweet_content:
                                        return {
                                            "tweet_url": page.url,
                                            "content": tweet_content
                                        }

                        # ã‚ˆã‚Šè©³ç´°ãªé–¢é€£ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚‚ãƒã‚§ãƒƒã‚¯
                        if response.web_detection.web_entities:
                            for entity in response.web_detection.web_entities[:10]:
                                if entity.description:
                                    # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®èª¬æ˜ã‹ã‚‰Twitteré–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œç´¢
                                    description = entity.description.lower()
                                    if any(keyword in description for keyword in ['twitter', 'tweet', 'x.com']):
                                        logger.info(f"ğŸ” é–¢é€£ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ç™ºè¦‹: {entity.description}")

                                        # ã“ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ä½¿ã£ã¦ã•ã‚‰ã«æ¤œç´¢ï¼ˆSerpAPIç„¡åŠ¹åŒ–ï¼‰
                                        # if SERPAPI_KEY and SerpAPI_available:
                                        #     search = GoogleSearch({  # type: ignore
                                        #         "engine": "google",
                                        #         "q": f'site:x.com OR site:twitter.com "{entity.description}"',
                                        #         "api_key": SERPAPI_KEY,
                                        #         "num": 10
                                        #     })
                                        #     entity_results = search.get_dict()
                                        #     if "organic_results" in entity_results:
                                        #         for result in entity_results["organic_results"][:3]:
                                        #             if "link" in result and any(domain in result["link"] for domain in ['x.com', 'twitter.com']):
                                        #                 logger.info(f"ğŸ¦ ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æ¤œç´¢ã§ãƒ„ã‚¤ãƒ¼ãƒˆURLç™ºè¦‹: {result['link']}")
                                        #                 tweet_content = get_x_tweet_content(result["link"])
                                        #                 if tweet_content:
                                        #                     return {
                                        #                         "tweet_url": result["link"],
                                        #                         "content": tweet_content
                                        #                     }
                                        logger.info("âš ï¸ SerpAPIã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æ¤œç´¢ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™")

            except Exception as vision_error:
                logger.warning(f"âš ï¸ Vision APIæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {vision_error}")

        # æ–¹æ³•2: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰Snowflake IDã‚’æŠ½å‡ºã—ã¦ãƒ„ã‚¤ãƒ¼ãƒˆIDã‚’æ¨å®š
        import re
        filename_match = re.search(r'/media/([^?]+)', image_url)
        if filename_match:
            filename = filename_match.group(1).split('.')[0]  # æ‹¡å¼µå­ã‚’é™¤å»
            logger.info(f"ğŸ” ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«å: {filename}")

            # Base64URLãƒ‡ã‚³ãƒ¼ãƒ‰ã‚’è©¦è¡Œã—ã¦Snowflake IDã‚’å–å¾—
            try:
                import base64
                from datetime import datetime

                # Twitterã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åã¯é€šå¸¸Base64URLã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸSnowflake ID
                decoded_bytes = base64.urlsafe_b64decode(filename + '==')  # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°è¿½åŠ 
                snowflake_id = int.from_bytes(decoded_bytes, byteorder='big')

                # Snowflake IDã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’è¨ˆç®—ï¼ˆTwitter Epoch: 2010-11-04 01:42:54 UTCï¼‰
                twitter_epoch = 1288834974657  # Twitter epoch in milliseconds
                timestamp_ms = (snowflake_id >> 22) + twitter_epoch
                tweet_datetime = datetime.fromtimestamp(timestamp_ms / 1000)

                logger.info(f"ğŸ“… æ¨å®šæŠ•ç¨¿æ—¥æ™‚: {tweet_datetime}")

                # ã“ã®æƒ…å ±ã‚’ä½¿ã£ã¦ã‚ˆã‚Šç²¾å¯†ãªæ¤œç´¢ã‚’å®Ÿè¡Œï¼ˆSerpAPIç„¡åŠ¹åŒ–ï¼‰
                # if SERPAPI_KEY and SerpAPI_available:
                #     date_str = tweet_datetime.strftime("%Y-%m-%d")
                #     search = GoogleSearch({  # type: ignore
                #         "engine": "google",
                #         "q": f'site:x.com OR site:twitter.com "{filename}" after:{date_str}',
                #         "api_key": SERPAPI_KEY,
                #         "num": 15
                #     })
                #
                #     date_results = search.get_dict()
                #     if "organic_results" in date_results:
                #         for result in date_results["organic_results"][:5]:
                #             if "link" in result and any(domain in result["link"] for domain in ['x.com', 'twitter.com']):
                #                 logger.info(f"ğŸ¦ æ—¥ä»˜æ¤œç´¢ã§ãƒ„ã‚¤ãƒ¼ãƒˆURLç™ºè¦‹: {result['link']}")
                #                 tweet_content = get_x_tweet_content(result["link"])


            except Exception as decode_error:
                logger.warning(f"âš ï¸ Snowflake ID ãƒ‡ã‚³ãƒ¼ãƒ‰å¤±æ•—: {decode_error}")

        logger.warning("âš ï¸ ç”»åƒã‹ã‚‰ãƒ„ã‚¤ãƒ¼ãƒˆURLã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return None

    except Exception as e:
        logger.error(f"âŒ ç”»åƒçµŒç”±ãƒ„ã‚¤ãƒ¼ãƒˆURLæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

    try:
        if vision_client:
            try:
                logger.info("ğŸ” Google Vision APIã§WEB_DETECTIONå®Ÿè¡Œä¸­...")

                # ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                import httpx
                with httpx.Client(timeout=10.0) as client:
                    response = client.get(image_url)
                    if response.status_code == 200:
                        image_content = response.content

                        # Vision APIå®Ÿè¡Œ
                        from google.cloud import vision
                        image = vision.Image(content=image_content)
                        response = vision_client.web_detection(image=image)  # type: ignore

                        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç¢ºèª
                        if not response or not response.web_detection:
                            logger.warning("âš ï¸ Vision APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒç„¡åŠ¹")
                            return None

                        # é–¢é€£ãƒšãƒ¼ã‚¸ã‹ã‚‰ X/Twitter URLã‚’æ¢ç´¢
                        if response.web_detection.pages_with_matching_images:
                            for page in response.web_detection.pages_with_matching_images[:15]:
                                if page.url and any(domain in page.url for domain in ['x.com', 'twitter.com']):
                                    logger.info(f"ğŸ¦ Vision APIã§ãƒ„ã‚¤ãƒ¼ãƒˆURLç™ºè¦‹: {page.url}")
                                    tweet_content = get_x_tweet_content(page.url)
                                    if tweet_content:
                                        return tweet_content

                        # ã‚ˆã‚Šè©³ç´°ãªé–¢é€£ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚‚ãƒã‚§ãƒƒã‚¯
                        if response.web_detection.web_entities:
                            for entity in response.web_detection.web_entities[:10]:
                                if entity.description:
                                    # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®èª¬æ˜ã‹ã‚‰Twitteré–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œç´¢
                                    description = entity.description.lower()
                                    if any(keyword in description for keyword in ['twitter', 'tweet', 'x.com']):
                                        logger.info(f"ğŸ” é–¢é€£ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ç™ºè¦‹: {entity.description}")

                                        # ã“ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ä½¿ã£ã¦ã•ã‚‰ã«æ¤œç´¢ï¼ˆSerpAPIç„¡åŠ¹åŒ–ï¼‰
                                        # if SERPAPI_KEY and SerpAPI_available:
                                        #     search = GoogleSearch({  # type: ignore
                                        #         "engine": "google",
                                        #         "q": f'site:x.com OR site:twitter.com "{entity.description}"',
                                        #         "api_key": SERPAPI_KEY,
                                        #         "num": 10
                                        #     })
                                        #     entity_results = search.get_dict()
                                        #     if "organic_results" in entity_results:
                                        #         for result in entity_results["organic_results"][:3]:
                                        #             if "link" in result and any(domain in result["link"] for domain in ['x.com', 'twitter.com']):
                                        #                 logger.info(f"ğŸ¦ ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æ¤œç´¢ã§ãƒ„ã‚¤ãƒ¼ãƒˆURLç™ºè¦‹: {result['link']}")
                                        #                 tweet_content = get_x_tweet_content(result["link"])
                                        #                 if tweet_content:
                                        #                     return tweet_content
                                        logger.info("âš ï¸ SerpAPIã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æ¤œç´¢ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™")

            except Exception as vision_error:
                logger.warning(f"âš ï¸ Vision APIæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {vision_error}")

        # æ–¹æ³•2: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰Snowflake IDã‚’æŠ½å‡ºã—ã¦ãƒ„ã‚¤ãƒ¼ãƒˆIDã‚’æ¨å®š
        import re
        filename_match = re.search(r'/media/([^?]+)', image_url)
        if filename_match:
            filename = filename_match.group(1).split('.')[0]  # æ‹¡å¼µå­ã‚’é™¤å»
            logger.info(f"ğŸ” ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«å: {filename}")

            # Base64URLãƒ‡ã‚³ãƒ¼ãƒ‰ã‚’è©¦è¡Œã—ã¦Snowflake IDã‚’å–å¾—
            try:
                import base64
                from datetime import datetime

                # Twitterã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åã¯é€šå¸¸Base64URLã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸSnowflake ID
                decoded_bytes = base64.urlsafe_b64decode(filename + '==')  # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°è¿½åŠ 
                snowflake_id = int.from_bytes(decoded_bytes, byteorder='big')

                # Snowflake IDã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’è¨ˆç®—ï¼ˆTwitter Epoch: 2010-11-04 01:42:54 UTCï¼‰
                twitter_epoch = 1288834974657  # Twitter epoch in milliseconds
                timestamp_ms = (snowflake_id >> 22) + twitter_epoch
                tweet_datetime = datetime.fromtimestamp(timestamp_ms / 1000)

                logger.info(f"ğŸ“… æ¨å®šæŠ•ç¨¿æ—¥æ™‚: {tweet_datetime}")

                # ã“ã®æƒ…å ±ã‚’ä½¿ã£ã¦ã‚ˆã‚Šç²¾å¯†ãªæ¤œç´¢ã‚’å®Ÿè¡Œï¼ˆSerpAPIç„¡åŠ¹åŒ–ï¼‰
                # if SERPAPI_KEY and SerpAPI_available:
                #     date_str = tweet_datetime.strftime("%Y-%m-%d")
                #     search = GoogleSearch({  # type: ignore
                #         "engine": "google",
                #         "q": f'site:x.com OR site:twitter.com "{filename}" after:{date_str}',
                #         "api_key": SERPAPI_KEY,
                #         "num": 15
                #     })
                #
                #     date_results = search.get_dict()
                #     if "organic_results" in date_results:
                #         for result in date_results["organic_results"][:5]:
                #             if "link" in result and any(domain in result["link"] for domain in ['x.com', 'twitter.com']):
                #                 logger.info(f"ğŸ¦ æ—¥ä»˜æ¤œç´¢ã§ãƒ„ã‚¤ãƒ¼ãƒˆURLç™ºè¦‹: {result['link']}")
                #                 tweet_content = get_x_tweet_content(result["link"])
                #                 if tweet_content:
                #                     return tweet_content


            except Exception as decode_error:
                logger.warning(f"âš ï¸ Snowflake ID ãƒ‡ã‚³ãƒ¼ãƒ‰å¤±æ•—: {decode_error}")

        logger.warning("âš ï¸ ç”»åƒã‹ã‚‰ãƒ„ã‚¤ãƒ¼ãƒˆå†…å®¹ã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return None

    except Exception as e:
        logger.error(f"âŒ ç”»åƒçµŒç”±ãƒ„ã‚¤ãƒ¼ãƒˆæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

        logger.info(f"ğŸ“ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: {len(content)} chars")
        return content

    except httpx.HTTPStatusError as e:
        logger.error(f"âŒ HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚¨ãƒ©ãƒ¼ {url}: {e.response.status_code} {e.response.reason_phrase}")
        return None
    except Exception as e:
        logger.error(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸€èˆ¬ã‚¨ãƒ©ãƒ¼ {url}: {e}")
        return None



# analyze_domainé–¢æ•°ã¯å‰Šé™¤ï¼ˆVision API + Geminiåˆ¤å®šã‚’ä½¿ç”¨ï¼‰



# ä¸è¦ãªé–¢æ•°ã¯å‰Šé™¤ã•ã‚Œã¾ã—ãŸ

# Google Custom Search APIé–¢æ•°ã¯å‰Šé™¤ï¼ˆVision API WEB_DETECTIONã‚’ä½¿ç”¨ï¼‰

# ç”»åƒç‰¹å¾´ãƒ™ãƒ¼ã‚¹æ¤œç´¢é–¢æ•°ã¯å‰Šé™¤ï¼ˆVision API WEB_DETECTIONã‚’ä½¿ç”¨ï¼‰

# SerpAPIé–¢é€£ã®é–¢æ•°ã¯å‰Šé™¤ï¼ˆVision API WEB_DETECTIONã‚’ä½¿ç”¨ï¼‰

@app.get("/")
async def root():
    return {
        "message": "Book Leak Detector API",
        "api_keys": {
            "gemini_api_key_configured": GEMINI_API_KEY is not None,
            "google_vision_api_configured": GOOGLE_APPLICATION_CREDENTIALS is not None
        },
        "upload_count": len(upload_records),
        "search_results_count": len(search_results)
    }

@app.post("/upload")
async def upload_image(file: UploadFile = File(...)):
    """ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ä¿å­˜ã™ã‚‹"""

    logger.info(f"ğŸ“¤ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {file.filename}, content_type: {file.content_type}")

    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼
        if not validate_file(file):
            allowed_types = ["image/jpeg", "image/png", "image/jpg", "image/gif", "image/webp"]
            if PDF_SUPPORT:
                allowed_types.append("application/pdf")

            logger.error(f"âŒ ç„¡åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {file.content_type}")
            raise HTTPException(
                status_code=400,
                detail={
                    "error": "invalid_file_format",
                    "message": "ç„¡åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™ã€‚JPEGã€PNGã€GIFã€WebPã€PDFå½¢å¼ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚" if PDF_SUPPORT else "ç„¡åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™ã€‚JPEGã€PNGã€GIFã€WebPå½¢å¼ã®ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
                    "allowed_types": allowed_types,
                    "received_type": file.content_type
                }
            )

        logger.info("âœ… ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼æ¤œè¨¼OK")

        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆã‚µã‚¤ã‚ºåˆ¶é™ãªã—ï¼‰
        content = await file.read()
        file_size_mb = len(content) / (1024 * 1024)
        logger.info(f"ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size_mb:.2f}MB")

        # ãƒ•ã‚¡ã‚¤ãƒ«ç¨®åˆ¥ã«ã‚ˆã‚‹æ¤œè¨¼
        is_pdf = is_pdf_file(file.content_type or "", file.filename or "")

        if is_pdf:
            # PDFæ¤œè¨¼
            if not PDF_SUPPORT:
                raise HTTPException(
                    status_code=400,
                    detail={
                        "error": "pdf_not_supported",
                        "message": "PDFå‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚",
                        "install_instruction": "pip install PyMuPDF"
                    }
                )

            try:
                # PDFã®æœ‰åŠ¹æ€§ã‚’ç¢ºèª
                test_images = convert_pdf_to_images(content)
                if not test_images:
                    raise Exception("PDFã‹ã‚‰ç”»åƒã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
                logger.info(f"âœ… PDFæœ‰åŠ¹æ€§æ¤œè¨¼OK ({len(test_images)}ãƒšãƒ¼ã‚¸)")
            except Exception as e:
                logger.error(f"âŒ PDFæ¤œè¨¼å¤±æ•—: {str(e)}")
                raise HTTPException(
                    status_code=400,
                    detail={
                        "error": "corrupted_pdf",
                        "message": "ç ´æã—ãŸPDFãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ã€‚æœ‰åŠ¹ãªPDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
                        "validation_error": str(e)
                    }
                )
        else:
            # ç”»åƒæ¤œè¨¼
            try:
                image = Image.open(BytesIO(content))
                image.verify()
                logger.info("âœ… ç”»åƒæœ‰åŠ¹æ€§æ¤œè¨¼OK")
            except Exception as e:
                logger.error(f"âŒ ç”»åƒæ¤œè¨¼å¤±æ•—: {str(e)}")
                raise HTTPException(
                    status_code=400,
                    detail={
                        "error": "corrupted_image",
                        "message": "ç ´æã—ãŸç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ã€‚æœ‰åŠ¹ãªç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
                        "validation_error": str(e)
                    }
                )

        # ä¸€æ„ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        file_id = str(uuid.uuid4())
        file_extension = os.path.splitext(file.filename or "image")[1].lower() or ".jpg"
        safe_filename = f"{file_id}{file_extension}"
        file_path = os.path.join(UPLOAD_DIR, safe_filename)

        logger.info(f"ğŸ’¾ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜é–‹å§‹: {file_path}")

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        try:
            with open(file_path, "wb") as f:
                f.write(content)
            logger.info("âœ… ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å¤±æ•—: {str(e)}")
            raise HTTPException(
                status_code=500,
                detail={
                    "error": "file_save_failed",
                    "message": f"ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}",
                    "file_path": file_path
                }
            )

        # è¨˜éŒ²ã‚’ä¿å­˜
        upload_record = {
            "id": file_id,
            "original_filename": file.filename,
            "saved_filename": safe_filename,
            "file_path": file_path,
            "content_type": file.content_type,
            "file_size": len(content),
            "upload_time": datetime.now().isoformat(),
            "status": "uploaded",
            "file_type": "pdf" if is_pdf else "image"
        }

        upload_records[file_id] = upload_record
        save_records()

        logger.info(f"âœ… ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: file_id={file_id}")

        return {
            "success": True,
            "file_id": file_id,
            "original_filename": file.filename,
            "saved_filename": safe_filename,
            "file_size": len(content),
            "upload_time": upload_record["upload_time"],
            "file_url": f"/uploads/{safe_filename}"
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": "unexpected_error",
                "message": f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            }
        )

@app.get("/uploads/history")
async def get_upload_history():
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å±¥æ­´ã‚’å–å¾—ã™ã‚‹"""
    # æ—¥ä»˜é †ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„ã‚‚ã®ãŒæœ€åˆï¼‰
    sorted_records = sorted(
        upload_records.values(),
        key=lambda x: x["upload_time"],
        reverse=True
    )

    return {
        "success": True,
        "count": len(sorted_records),
        "uploads": sorted_records
    }

@app.get("/uploads/{file_id}")
async def get_upload_details(file_id: str):
    """ç‰¹å®šã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°ã‚’å–å¾—ã™ã‚‹"""
    if file_id not in upload_records:
        raise HTTPException(
            status_code=404,
            detail="æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        )

    record = upload_records[file_id]

    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    if not os.path.exists(record["file_path"]):
        record["status"] = "file_missing"
        save_records()

    return {
        "success": True,
        "file": record
    }

@app.delete("/uploads/{file_id}")
async def delete_upload(file_id: str):
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã™ã‚‹"""
    if file_id not in upload_records:
        raise HTTPException(
            status_code=404,
            detail="æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        )

    record = upload_records[file_id]

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
    try:
        if os.path.exists(record["file_path"]):
            os.remove(record["file_path"])
    except Exception as e:
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")

    # è¨˜éŒ²ã‹ã‚‰å‰Šé™¤
    del upload_records[file_id]
    save_records()

    return {
        "success": True,
        "message": f"ãƒ•ã‚¡ã‚¤ãƒ« {record['original_filename']} ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚"
    }

@app.get("/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    # Vision APIæ¥ç¶šãƒ†ã‚¹ãƒˆ
    vision_api_status = "not_configured"
    vision_api_error = None

    if vision_client:
        try:
            # å°ã•ãªãƒ†ã‚¹ãƒˆç”»åƒã§Vision APIã‚’ãƒ†ã‚¹ãƒˆ
            from PIL import Image as PILImage
            import io

            # 1x1ã®å°ã•ãªç™½ã„ç”»åƒã‚’ä½œæˆ
            test_image = PILImage.new('RGB', (1, 1), color='white')
            img_buffer = io.BytesIO()
            test_image.save(img_buffer, format='PNG')
            test_image_content = img_buffer.getvalue()

            # Vision APIãƒ†ã‚¹ãƒˆå‘¼ã³å‡ºã—
            image = vision.Image(content=test_image_content)
            response = vision_client.web_detection(image=image)  # type: ignore

            if hasattr(response, 'error') and response.error:
                error_code = getattr(response.error, 'code', 'UNKNOWN')
                error_message = getattr(response.error, 'message', 'è©³ç´°ä¸æ˜')

                # ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰ãŒ0ï¼ˆOKï¼‰ä»¥å¤–ã®å ´åˆã®ã¿ã‚¨ãƒ©ãƒ¼ã¨ã—ã¦å‡¦ç†
                if error_code != 0:
                    vision_api_status = "error"
                    vision_api_error = f"Code: {error_code}, Message: {error_message}"
                else:
                    vision_api_status = "healthy"
                    vision_api_error = None
            else:
                vision_api_status = "healthy"

        except Exception as e:
            vision_api_status = "error"
            vision_api_error = str(e)

    return {
        "status": "healthy" if vision_api_status in ["healthy", "not_configured"] else "degraded",
        "api_keys": {
            "gemini_api_key_configured": GEMINI_API_KEY is not None,
            "google_vision_api_configured": GOOGLE_APPLICATION_CREDENTIALS is not None,
            "vision_api_client_initialized": vision_client is not None,
            "vision_api_status": vision_api_status,
            "vision_api_error": vision_api_error
        },
        "system": {
            "upload_directory_exists": os.path.exists(UPLOAD_DIR),
            "records_file_exists": os.path.exists(RECORDS_FILE),
            "total_uploads": len(upload_records),
            "total_search_results": len(search_results)
        }
    }

@app.post("/search/{image_id}")
async def analyze_image(image_id: str):
    """æŒ‡å®šã•ã‚ŒãŸç”»åƒIDã«å¯¾ã—ã¦Webæ¤œç´¢ã‚’å®Ÿè¡Œã—ã€é–¢é€£ç”»åƒã®URLãƒªã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹"""

    logger.info(f"ğŸ” Webç”»åƒæ¤œç´¢é–‹å§‹: image_id={image_id}")

    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è¨˜éŒ²ã‚’ç¢ºèª
    if image_id not in upload_records:
        logger.error(f"âŒ image_id not found: {image_id}")
        raise HTTPException(
            status_code=404,
            detail={
                "error": "image_not_found",
                "message": "æŒ‡å®šã•ã‚ŒãŸimage_idãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚",
                "image_id": image_id
            }
        )

    record = upload_records[image_id]
    file_path = record["file_path"]
    file_type = record.get("file_type", "image")

    logger.info(f"ğŸ“ æ¤œç´¢å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {file_path} (type: {file_type})")

    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã„ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’èª­ã¿è¾¼ã‚€
        with open(file_path, 'rb') as file:
            file_content = file.read()

        logger.info(f"ğŸ“¸ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {len(file_content)} bytes")

        # ãƒ•ã‚¡ã‚¤ãƒ«ç¨®åˆ¥ã«å¿œã˜ã¦å‡¦ç†ã‚’åˆ†å²
        if file_type == "pdf":
            # PDFã®å ´åˆï¼šå„ãƒšãƒ¼ã‚¸ã‚’ç”»åƒã«å¤‰æ›ã—ã¦å‡¦ç†
            logger.info("ğŸ“„ PDFå‡¦ç†é–‹å§‹...")

            pdf_images = convert_pdf_to_images(file_content)
            if not pdf_images:
                raise Exception("PDFã‹ã‚‰ç”»åƒã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")

            logger.info(f"ğŸ“„ PDFå‡¦ç†å®Œäº†: {len(pdf_images)}ãƒšãƒ¼ã‚¸ã‚’æŠ½å‡º")

            # å„ãƒšãƒ¼ã‚¸ã®ç”»åƒãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—ï¼ˆæœ€åˆã®ãƒšãƒ¼ã‚¸ã‚’ãƒ¡ã‚¤ãƒ³ãƒãƒƒã‚·ãƒ¥ã¨ã™ã‚‹ï¼‰
            image_hash = calculate_image_hash(pdf_images[0])
            logger.info(f"ğŸ”‘ ç”»åƒãƒãƒƒã‚·ãƒ¥è¨ˆç®—å®Œäº†ï¼ˆãƒšãƒ¼ã‚¸1ï¼‰: {image_hash[:16]}...")

            # å„ãƒšãƒ¼ã‚¸ã‚’å€‹åˆ¥ã«åˆ†æï¼ˆæ‹¡å¼µæ¤œç´¢ï¼‰
            all_url_lists = []
            for i, page_image_content in enumerate(pdf_images):
                logger.info(f"ğŸŒ ãƒšãƒ¼ã‚¸ {i+1} ã®æ‹¡å¼µç”»åƒæ¤œç´¢å®Ÿè¡Œä¸­ï¼ˆé€†æ¤œç´¢æ©Ÿèƒ½ä»˜ãï¼‰...")
                page_urls = enhanced_image_search_with_reverse(page_image_content)
                all_url_lists.extend(page_urls)
                logger.info(f"âœ… ãƒšãƒ¼ã‚¸ {i+1} æ‹¡å¼µWebæ¤œç´¢å®Œäº†: {len(page_urls)}ä»¶ã®URLã‚’ç™ºè¦‹")

            # é‡è¤‡URLã‚’é™¤å»ï¼ˆè¾æ›¸å½¢å¼ãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰
            seen_urls = set()
            url_list = []
            for url_data in all_url_lists:
                url = url_data["url"] if isinstance(url_data, dict) else url_data
                if url not in seen_urls:
                    seen_urls.add(url)
                    url_list.append(url_data)
            logger.info(f"ğŸ“‹ å…¨ãƒšãƒ¼ã‚¸çµ±åˆçµæœ: {len(url_list)}ä»¶ã®ä¸€æ„ãªURLã‚’ç™ºè¦‹")

        else:
            # ç”»åƒã®å ´åˆï¼šå¾“æ¥ã®å‡¦ç†
            image_content = file_content

            # ç”»åƒãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—
            image_hash = calculate_image_hash(image_content)
            logger.info(f"ğŸ”‘ ç”»åƒãƒãƒƒã‚·ãƒ¥è¨ˆç®—å®Œäº†: {image_hash[:16]}...")

            # æ‹¡å¼µç”»åƒæ¤œç´¢ï¼ˆé€†æ¤œç´¢æ©Ÿèƒ½ä»˜ãï¼‰
            logger.info("ğŸŒ æ‹¡å¼µç”»åƒæ¤œç´¢å®Ÿè¡Œä¸­ï¼ˆé€†æ¤œç´¢æ©Ÿèƒ½ä»˜ãï¼‰...")
            url_list = enhanced_image_search_with_reverse(image_content)
            logger.info(f"âœ… æ‹¡å¼µWebæ¤œç´¢å®Œäº†: {len(url_list)}ä»¶ã®URLã‚’ç™ºè¦‹")

        # å„URLã‚’åŠ¹ç‡çš„ã«åˆ†æï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã¯äº‹å‰â—‹åˆ¤å®šã€Twitterã¯ç‰¹åˆ¥å‡¦ç†ï¼‰
        processed_results = []

        for i, url_data in enumerate(url_list[:50]):  # PDFã®å ´åˆã¯æœ€å¤§50ä»¶ã«æ‹¡å¼µ
            # url_dataãŒè¾æ›¸å½¢å¼ã®å ´åˆã¨stringå½¢å¼ã®å ´åˆã«å¯¾å¿œ
            if isinstance(url_data, dict):
                url = url_data["url"]
                search_method = url_data.get("search_method", "ä¸æ˜")
                search_source = url_data.get("search_source", "ä¸æ˜")
                confidence = url_data.get("confidence", "ä¸æ˜")
            else:
                # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã€stringå½¢å¼ã‚‚ã‚µãƒãƒ¼ãƒˆ
                url = url_data
                search_method = "ä¸æ˜"
                search_source = "ä¸æ˜"
                confidence = "ä¸æ˜"

            logger.info(f"ğŸ”„ URLå‡¦ç†ä¸­ ({i+1}/{min(len(url_list), 50)}): [{search_method}] {url}")

            # åŠ¹ç‡çš„ãªåˆ†æå®Ÿè¡Œ
            result = analyze_url_efficiently(url)

            if result:
                # æ¤œç´¢æ–¹æ³•ã®æƒ…å ±ã‚’çµæœã«è¿½åŠ 
                result["search_method"] = search_method
                result["search_source"] = search_source
                result["confidence"] = confidence
                processed_results.append(result)
                logger.info(f"  âœ… å‡¦ç†å®Œäº†: {result['judgment']} - {result['reason']}")
            else:
                # åˆ†æå¤±æ•—æ™‚
                processed_results.append({
                    "url": url,
                    "judgment": "ï¼Ÿ",
                    "reason": "åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ",
                    "search_method": search_method,
                    "search_source": search_source,
                    "confidence": confidence
                })
                logger.info(f"  âŒ åˆ†æå¤±æ•—: {url}")

        # æœ€çµ‚çµæœã‚’ä¿å­˜ï¼ˆç”Ÿã®æ¤œç´¢çµæœã‚‚å«ã‚ã‚‹ï¼‰
        search_results[image_id] = {
            "processed_results": processed_results,
            "raw_urls": url_list,  # ç”Ÿã®æ¤œç´¢çµæœï¼ˆsearch_method, search_source, confidenceä»˜ãï¼‰
            "total_found": len(url_list),
            "total_processed": len(processed_results)
        }

        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è¨˜éŒ²ã‚’æ›´æ–°
        record["analysis_status"] = "completed"
        record["analysis_time"] = datetime.now().isoformat()
        record["found_urls_count"] = len(url_list)
        record["processed_results_count"] = len(processed_results)
        record["image_hash"] = image_hash
        save_records()

        # å±¥æ­´ã«ä¿å­˜
        save_analysis_to_history(image_id, image_hash, processed_results)

        logger.info(f"âœ… åˆ†æå®Œäº†: image_id={image_id}, URLç™ºè¦‹={len(url_list)}ä»¶, å‡¦ç†å®Œäº†={len(processed_results)}ä»¶")

        return {
            "success": True,
            "image_id": image_id,
            "found_urls_count": len(url_list),
            "processed_results_count": len(processed_results),
            "results": processed_results,
            "analysis_time": record["analysis_time"],
            "message": f"Webæ¤œç´¢ãƒ»åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚{len(url_list)}ä»¶ã®URLãŒè¦‹ã¤ã‹ã‚Šã€{len(processed_results)}ä»¶ã‚’åˆ†æã—ã¾ã—ãŸã€‚"
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Webæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")

        # ã‚¨ãƒ©ãƒ¼çŠ¶æ…‹ã‚’è¨˜éŒ²
        record["analysis_status"] = "failed"
        record["analysis_error"] = str(e)
        record["analysis_time"] = datetime.now().isoformat()
        save_records()

        raise HTTPException(
            status_code=500,
            detail={
                "error": "search_failed",
                "message": f"Webæ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
                "image_id": image_id
            }
        )

@app.get("/results")
async def get_all_results():
    """ã™ã¹ã¦ã®æ¤œç´¢çµæœã‚’å–å¾—ã™ã‚‹"""
    return {
        "success": True,
        "total_searches": len(search_results),
        "results": search_results
    }

@app.get("/results/{image_id}")
async def get_search_results(image_id: str):
    """ç‰¹å®šã®image_idã®æ¤œç´¢çµæœã‚’å–å¾—ã™ã‚‹"""
    if image_id not in upload_records:
        raise HTTPException(
            status_code=404,
            detail={"error": "image_not_found", "message": "æŒ‡å®šã•ã‚ŒãŸimage_idã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è¨˜éŒ²ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"}
        )

    record = upload_records[image_id]

    # åˆ†æãŒã¾ã ã€ã¾ãŸã¯å¤±æ•—ã—ã¦ã„ã‚‹å ´åˆ
    if record.get("analysis_status") != "completed":
        return {
            "success": True,
            "image_id": image_id,
            "analysis_status": record.get("analysis_status", "not_started"),
            "message": "åˆ†æãŒå®Œäº†ã—ã¦ã„ã¾ã›ã‚“ã€‚å…ˆã«åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚",
            "details": record.get("analysis_error")
        }

    # åˆ†æã¯å®Œäº†ã—ãŸãŒã€æœ‰åŠ¹ãªçµæœãŒ0ä»¶ã ã£ãŸå ´åˆ
    if record.get("processed_results_count", 0) == 0:
        return {
            "success": True,
            "image_id": image_id,
            "analysis_status": "completed_no_results",
            "message": "åˆ†æã¯å®Œäº†ã—ã¾ã—ãŸãŒã€æœ‰åŠ¹ãªWebãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚",
            "found_urls_count": record.get("found_urls_count", 0),
            "processed_results_count": 0,
            "results": []
        }

    # æ¤œç´¢çµæœãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    search_data = search_results.get(image_id, {})

    # æ–°æ—§ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«å¯¾å¿œ
    if isinstance(search_data, list):
        # æ—§ãƒ‡ãƒ¼ã‚¿æ§‹é€ ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
        processed_results = search_data
        raw_urls = []
        total_found = len(search_data)
        total_processed = len(search_data)
    else:
        # æ–°ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
        processed_results = search_data.get("processed_results", [])
        raw_urls = search_data.get("raw_urls", [])
        total_found = search_data.get("total_found", 0)
        total_processed = search_data.get("total_processed", 0)

    # æ­£å¸¸ãªçµæœã‚’è¿”ã™
    return {
        "success": True,
        "image_id": image_id,
        "analysis_status": "completed",
        "original_filename": record.get("original_filename", "ä¸æ˜"),
        "analysis_time": record.get("analysis_time", "ä¸æ˜"),
        "found_urls_count": record.get("found_urls_count", total_found),
        "processed_results_count": record.get("processed_results_count", total_processed),
        "results": processed_results,
        "raw_urls": raw_urls,  # ç”Ÿã®æ¤œç´¢çµæœã‚’è¿½åŠ 
        "search_summary": {
            "total_found": total_found,
            "total_processed": total_processed,
            "search_methods": generate_search_method_summary(raw_urls)
        }
    }

# ãƒ†ã‚¹ãƒˆç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.get("/test-search")
async def test_search():
    """ãƒ€ãƒŸãƒ¼ç”»åƒã§æ¤œç´¢ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã™ã‚‹"""
    logger.info("ğŸ§ª ãƒ†ã‚¹ãƒˆæ¤œç´¢é–‹å§‹")

    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
    test_image_id = "test-" + str(uuid.uuid4())

    # ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ€ãƒŸãƒ¼çµæœ
    dummy_results = [
        {
            "url": "https://amazon.co.jp/test-book",
            "domain": "amazon.co.jp",
            "title": "ãƒ†ã‚¹ãƒˆæ›¸ç± - Amazon",
            "source": "Amazon Japan",
            "is_official": True,
            "threat_level": "safe",
            "detailed_analysis": {
                "status": "safe",
                "reason": "å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ã§ã™",
                "content_analysis": None
            },
            "thumbnail": "https://example.com/thumb.jpg",
            "analysis_timestamp": datetime.now().isoformat()
        },
        {
            "url": "https://suspicious-site.com/free-download",
            "domain": "suspicious-site.com",
            "title": "ç„¡æ–™ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ - ç–‘ã‚ã—ã„ã‚µã‚¤ãƒˆ",
            "source": "Suspicious Site",
            "is_official": False,
            "threat_level": "suspicious",
            "detailed_analysis": {
                "status": "suspicious",
                "reason": "ç–‘ã‚ã—ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: ç„¡æ–™ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                "content_analysis": "åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆï¼ˆä¸€éƒ¨ï¼‰: ç„¡æ–™ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¯ã“ã¡ã‚‰..."
            },
            "thumbnail": "https://example.com/thumb2.jpg",
            "analysis_timestamp": datetime.now().isoformat()
        }
    ]

    # ãƒ†ã‚¹ãƒˆçµæœã‚’ãƒ¡ãƒ¢ãƒªã«ä¿å­˜
    search_results[test_image_id] = {
        "processed_results": dummy_results,
        "raw_urls": [],
        "total_found": len(dummy_results),
        "total_processed": len(dummy_results)
    }

    logger.info(f"âœ… ãƒ†ã‚¹ãƒˆæ¤œç´¢å®Œäº†: {len(dummy_results)}ä»¶ã®çµæœ")

    return {
        "success": True,
        "test_image_id": test_image_id,
        "results_count": len(dummy_results),
        "message": f"ãƒ†ã‚¹ãƒˆæ¤œç´¢ãŒå®Œäº†ã—ã¾ã—ãŸã€‚{len(dummy_results)}ä»¶ã®çµæœãŒã‚ã‚Šã¾ã™ã€‚",
        "test_results": dummy_results
    }

@app.get("/test-domain/{domain}")
async def test_domain_analysis(domain: str):
    """æŒ‡å®šã•ã‚ŒãŸãƒ‰ãƒ¡ã‚¤ãƒ³ã®åˆ¤å®šãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã™ã‚‹ï¼ˆæ–°ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å¯¾å¿œï¼‰"""
    logger.info(f"ğŸ§ª ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆé–‹å§‹: {domain}")

    # ãƒ†ã‚¹ãƒˆç”¨URL
    test_url = f"https://{domain}"

    try:
        # ãƒšãƒ¼ã‚¸å†…å®¹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        content = scrape_page_content(test_url)

        if content:
            # Geminiã§åˆ¤å®š
            result = judge_content_with_gemini(content)

            logger.info(f"âœ… ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆå®Œäº†: {domain} -> {result['judgment']}")

            return {
                "success": True,
                "domain": domain,
                "test_url": test_url,
                "judgment": result['judgment'],
                "reason": result['reason'],
                "scraped_content_length": len(content),
                "test_time": datetime.now().isoformat()
            }
        else:
            logger.warning(f"âš ï¸ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—: {domain}")
            return {
                "success": False,
                "domain": domain,
                "error": "ãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ",
                "test_time": datetime.now().isoformat()
            }

    except Exception as e:
        logger.error(f"âŒ ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {
            "success": False,
            "domain": domain,
            "error": str(e),
            "test_time": datetime.now().isoformat()
        }

@app.get("/debug/logs")
async def get_debug_info():
    """ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å–å¾—ã™ã‚‹"""
    return {
        "system_status": "running",
        "total_uploads": len(upload_records),
        "total_search_results": len(search_results),
        "recent_uploads": list(upload_records.keys())[-5:] if upload_records else [],
        "api_keys_status": {
            "gemini_api_key": GEMINI_API_KEY is not None,
            "google_vision_api": GOOGLE_APPLICATION_CREDENTIALS is not None
        },
        "vision_api_status": "active",
        "timestamp": datetime.now().isoformat()
    }

@app.get("/logs")
async def get_system_logs():
    """ã‚·ã‚¹ãƒ†ãƒ ãƒ­ã‚°ã‚’å–å¾—ã™ã‚‹"""
    logger.info(f"ğŸ“‹ ãƒ­ã‚°å–å¾—è¦æ±‚: {len(system_logs)}ä»¶ã®ãƒ­ã‚°")
    return {
        "success": True,
        "total_logs": len(system_logs),
        "logs": system_logs[-50:],  # æœ€æ–°50ä»¶ã‚’è¿”ã™
        "timestamp": datetime.now().isoformat()
    }

def generate_evidence_hash(data: dict) -> str:
    """
    è¨¼æ‹ ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’ç”Ÿæˆï¼ˆæ”¹ã–ã‚“é˜²æ­¢ç”¨ï¼‰
    """
    # ãƒ‡ãƒ¼ã‚¿ã‚’æ–‡å­—åˆ—ã¨ã—ã¦æ­£è¦åŒ–ã—ã¦ãƒãƒƒã‚·ãƒ¥åŒ–
    json_str = json.dumps(data, sort_keys=True, ensure_ascii=False)
    return hashlib.sha256(json_str.encode('utf-8')).hexdigest()

def create_evidence_data(image_id: str) -> dict:
    """
    è¨¼æ‹ ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã™ã‚‹
    """
    if image_id not in upload_records:
        raise HTTPException(
            status_code=404,
            detail="æŒ‡å®šã•ã‚ŒãŸimage_idãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        )

    if image_id not in search_results:
        raise HTTPException(
            status_code=404,
            detail="ã“ã®ç”»åƒã®åˆ†æçµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã«åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
        )

    upload_record = upload_records[image_id]
    analysis_results = search_results[image_id]

    # ç¾åœ¨æ™‚åˆ»
    current_time = datetime.now()

    # è¨¼æ‹ ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
    evidence_data = {
        "evidence_info": {
            "creation_date": current_time.isoformat(),
            "creation_timestamp": int(current_time.timestamp()),
            "evidence_id": f"evidence_{image_id}_{int(current_time.timestamp())}",
            "system_info": "Book Leak Detector v1.0.0"
        },
        "image_info": {
            "image_id": image_id,
            "original_filename": upload_record.get("original_filename", "ä¸æ˜"),
            "file_size": upload_record.get("file_size", 0),
            "upload_time": upload_record.get("upload_time", "ä¸æ˜"),
            "content_type": upload_record.get("content_type", "ä¸æ˜")
        },
        "analysis_info": {
            "analysis_time": upload_record.get("analysis_time", "ä¸æ˜"),
            "analysis_status": upload_record.get("analysis_status", "ä¸æ˜"),
            "found_urls_count": upload_record.get("found_urls_count", 0),
            "processed_results_count": upload_record.get("processed_results_count", 0)
        },
        "detection_results": {
            "total_urls_detected": len(analysis_results),
            "url_analysis": []
        }
    }

    # å„URLã®åˆ¤å®šçµæœã‚’è¿½åŠ 
    for result in analysis_results:
        url_info = {
            "url": result.get("url", ""),
            "judgment": result.get("judgment", "ï¼Ÿ"),
            "reason": result.get("reason", "ç†ç”±ä¸æ˜"),
            "analysis_timestamp": current_time.isoformat()
        }
        evidence_data["detection_results"]["url_analysis"].append(url_info)

    # ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—ï¼ˆæ”¹ã–ã‚“é˜²æ­¢ç”¨ï¼‰
    evidence_data["integrity"] = {
        "hash_algorithm": "SHA-256",
        "data_hash": generate_evidence_hash(evidence_data),
        "note": "ã“ã®ãƒãƒƒã‚·ãƒ¥å€¤ã¯è¨¼æ‹ ãƒ‡ãƒ¼ã‚¿ã®æ”¹ã–ã‚“ã‚’æ¤œçŸ¥ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™"
    }

    return evidence_data

@app.get("/api/evidence/download/{image_id}")
async def download_evidence(image_id: str):
    """
    æ¤œå‡ºçµæœã‚’è¨¼æ‹ ã¨ã—ã¦ä¿å­˜ç”¨JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
    """
    logger.info(f"ğŸ“¥ è¨¼æ‹ ä¿å…¨è¦æ±‚: image_id={image_id}")

    try:
        # è¨¼æ‹ ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        evidence_data = create_evidence_data(image_id)

        # JSONãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        timestamp = int(datetime.now().timestamp())
        filename = f"evidence_{image_id}_{timestamp}.json"

        # JSONãƒ‡ãƒ¼ã‚¿ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
        json_content = json.dumps(evidence_data, ensure_ascii=False, indent=2)

        logger.info(f"âœ… è¨¼æ‹ ä¿å…¨ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {filename}")

        # ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™
        return Response(
            content=json_content,
            media_type="application/json",
            headers={
                "Content-Disposition": f"attachment; filename={filename}",
                "Content-Type": "application/json; charset=utf-8"
            }
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ è¨¼æ‹ ä¿å…¨ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": "evidence_creation_failed",
                "message": f"è¨¼æ‹ ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}",
                "image_id": image_id
            }
        )

@app.get("/api/history")
async def get_analysis_history():
    """
    éå»ã®æ¤œæŸ»å±¥æ­´ä¸€è¦§ã‚’å–å¾—ã™ã‚‹
    """
    logger.info(f"ğŸ“š å±¥æ­´å–å¾—è¦æ±‚: {len(analysis_history)}ä»¶")

    try:
        # å±¥æ­´ã‚’æ–°ã—ã„é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_history = sorted(
            analysis_history,
            key=lambda x: x.get("analysis_timestamp", 0),
            reverse=True
        )

        # è¡¨ç¤ºç”¨ã«å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
        formatted_history = []
        for entry in sorted_history:
            formatted_entry = {
                "history_id": entry.get("history_id"),
                "image_id": entry.get("image_id"),
                "image_hash": entry.get("image_hash"),
                "original_filename": entry.get("original_filename"),
                "analysis_date": entry.get("analysis_date"),
                "analysis_timestamp": entry.get("analysis_timestamp"),
                "found_urls_count": entry.get("found_urls_count", 0),
                "processed_results_count": entry.get("processed_results_count", 0),
                "summary": {
                    "safe_count": len([r for r in entry.get("results", []) if r.get("judgment") == "â—‹"]),
                    "suspicious_count": len([r for r in entry.get("results", []) if r.get("judgment") == "Ã—"]),
                    "unknown_count": len([r for r in entry.get("results", []) if r.get("judgment") in ["ï¼Ÿ", "ï¼"]])
                }
            }
            formatted_history.append(formatted_entry)

        return {
            "success": True,
            "total_history_count": len(analysis_history),
            "history": formatted_history
        }

    except Exception as e:
        logger.error(f"âŒ å±¥æ­´å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": "history_retrieval_failed",
                "message": f"å±¥æ­´ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}"
            }
        )

@app.delete("/api/history/{history_id}")
async def delete_analysis_history(history_id: str):
    """
    æŒ‡å®šã•ã‚ŒãŸå±¥æ­´IDã®æ¤œæŸ»å±¥æ­´ã‚’å‰Šé™¤ã™ã‚‹
    """
    try:
        # æŒ‡å®šã•ã‚ŒãŸhistory_idã®å±¥æ­´ã‚’æ¤œç´¢
        history_to_delete = None
        for i, entry in enumerate(analysis_history):
            if entry.get("history_id") == history_id:
                history_to_delete = analysis_history.pop(i)
                break

        if not history_to_delete:
            raise HTTPException(
                status_code=404,
                detail={
                    "error": "history_not_found",
                    "message": "æŒ‡å®šã•ã‚ŒãŸå±¥æ­´ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
                }
            )

        # å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
        save_history()

        logger.info(f"ğŸ—‘ï¸ å±¥æ­´å‰Šé™¤å®Œäº†: {history_id}")

        return {
            "success": True,
            "message": "å±¥æ­´ã‚’å‰Šé™¤ã—ã¾ã—ãŸ",
            "deleted_history_id": history_id,
            "deleted_filename": history_to_delete.get("original_filename", "ä¸æ˜")
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ å±¥æ­´å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": "history_deletion_failed",
                "message": f"å±¥æ­´ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}"
            }
        )

@app.get("/api/history/details/{history_id}")
async def get_history_details(history_id: str):
    """
    æŒ‡å®šã•ã‚ŒãŸå±¥æ­´IDã®è©³ç´°ï¼ˆæ¤œå‡ºã•ã‚ŒãŸURLã¨åˆ¤å®šçµæœï¼‰ã‚’å–å¾—ã™ã‚‹
    """
    try:
        # æŒ‡å®šã•ã‚ŒãŸhistory_idã®å±¥æ­´ã‚’æ¤œç´¢
        target_history = None
        for entry in analysis_history:
            if entry.get("history_id") == history_id:
                target_history = entry
                break

        if not target_history:
            raise HTTPException(
                status_code=404,
                detail={
                    "error": "history_not_found",
                    "message": "æŒ‡å®šã•ã‚ŒãŸå±¥æ­´ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
                }
            )

        # è©³ç´°æƒ…å ±ã‚’æ•´å½¢
        results = target_history.get("results", [])

        return {
            "success": True,
            "history_id": history_id,
            "image_id": target_history.get("image_id"),
            "original_filename": target_history.get("original_filename"),
            "analysis_date": target_history.get("analysis_date"),
            "found_urls_count": target_history.get("found_urls_count", 0),
            "processed_results_count": target_history.get("processed_results_count", 0),
            "results": results,
            "summary": {
                "safe_count": len([r for r in results if r.get("judgment") == "â—‹"]),
                "suspicious_count": len([r for r in results if r.get("judgment") == "Ã—"]),
                "warning_count": len([r for r in results if r.get("judgment") == "ï¼"]),
                "unknown_count": len([r for r in results if r.get("judgment") == "ï¼Ÿ"])
            }
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ å±¥æ­´è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": "history_details_retrieval_failed",
                "message": f"å±¥æ­´è©³ç´°ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}"
            }
        )

@app.get("/api/history/diff/{image_id}")
async def get_analysis_diff(image_id: str):
    """
    æŒ‡å®šã•ã‚ŒãŸç”»åƒIDã®å‰å›æ¤œæŸ»ã¨ã®å·®åˆ†ã‚’å–å¾—ã™ã‚‹
    """
    logger.info(f"ğŸ”„ å·®åˆ†å–å¾—è¦æ±‚: image_id={image_id}")

    try:
        # ç¾åœ¨ã®çµæœã‚’å–å¾—
        if image_id not in upload_records:
            raise HTTPException(
                status_code=404,
                detail="æŒ‡å®šã•ã‚ŒãŸimage_idãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
            )

        record = upload_records[image_id]
        search_data = search_results.get(image_id, {})
        current_results = search_data.get("processed_results", []) if isinstance(search_data, dict) else []
        image_hash = record.get("image_hash")

        if not image_hash:
            return {
                "success": True,
                "has_previous": False,
                "message": "ã“ã®ç”»åƒã«å¯¾ã™ã‚‹éå»ã®åˆ†æçµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
            }

        # åŒã˜ãƒãƒƒã‚·ãƒ¥ã®éå»ã®åˆ†æçµæœã‚’å–å¾—
        previous_analysis = get_previous_analysis(image_hash)

        if not previous_analysis:
            return {
                "success": True,
                "has_previous": False,
                "message": "ã“ã®ç”»åƒã«å¯¾ã™ã‚‹éå»ã®åˆ†æçµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
            }

        # å·®åˆ†ã‚’è¨ˆç®—
        diff_result = calculate_diff(current_results, previous_analysis.get("results", []))

        # å‰å›åˆ†ææ—¥æ™‚ã‚’å«ã‚ã¦è¿”ã™
        response_data = {
            "success": True,
            "has_previous": True,
            "image_id": image_id,
            "image_hash": image_hash,
            "current_analysis": {
                "analysis_date": record.get("analysis_time"),
                "results_count": len(current_results)
            },
            "previous_analysis": {
                "analysis_date": previous_analysis.get("analysis_date"),
                "results_count": len(previous_analysis.get("results", []))
            },
            "diff": diff_result
        }

        logger.info(f"âœ… å·®åˆ†è¨ˆç®—å®Œäº†: æ–°è¦={diff_result['total_new']}, æ¶ˆå¤±={diff_result['total_disappeared']}, å¤‰æ›´={diff_result['total_changed']}")

        return response_data

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ å·®åˆ†å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": "diff_calculation_failed",
                "message": f"å·®åˆ†ã®è¨ˆç®—ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}",
                "image_id": image_id
            }
        )

def generate_csv_report(image_id: str) -> str:
    """
    CSVå½¢å¼ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹
    """
    if image_id not in upload_records:
        raise HTTPException(
            status_code=404,
            detail="æŒ‡å®šã•ã‚ŒãŸimage_idãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        )

    record = upload_records[image_id]
    results = search_results.get(image_id, [])

    # StringIOã‚’ä½¿ã£ã¦CSVãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    output = StringIO()

    # BOMä»˜ãUTF-8ã®ãŸã‚ã®BOMã‚’è¿½åŠ 
    output.write('\ufeff')

    writer = csv.writer(output)

    # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œï¼ˆæ—¥æœ¬èªï¼‰
    headers = [
        "æ¤œæŸ»æ—¥æ™‚",
        "ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«å",
        "URL",
        "ãƒ‰ãƒ¡ã‚¤ãƒ³",
        "åˆ¤å®šçµæœ",
        "åˆ¤å®šç†ç”±"
    ]
    writer.writerow(headers)

    # ãƒ‡ãƒ¼ã‚¿è¡Œ
    analysis_time = record.get("analysis_time", "ä¸æ˜")
    filename = record.get("original_filename", "ä¸æ˜")

    for result in results:
        url = result.get("url", "")
        judgment = result.get("judgment", "ï¼Ÿ")
        reason = result.get("reason", "ç†ç”±ä¸æ˜")

        # ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’æŠ½å‡º
        try:
            domain = urlparse(url).netloc
        except:
            domain = "ä¸æ˜"

        writer.writerow([
            analysis_time,
            filename,
            url,
            domain,
            judgment,
            reason
        ])

    return output.getvalue()

def generate_summary_report(image_id: str) -> dict:
    """
    çµŒå–¶å±¤å‘ã‘ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹
    """
    if image_id not in upload_records:
        raise HTTPException(
            status_code=404,
            detail="æŒ‡å®šã•ã‚ŒãŸimage_idãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        )

    record = upload_records[image_id]
    results = search_results.get(image_id, [])

    # çµ±è¨ˆã‚’è¨ˆç®—
    total_count = len(results)
    safe_count = len([r for r in results if r.get("judgment") == "â—‹"])
    dangerous_count = len([r for r in results if r.get("judgment") == "Ã—"])
    warning_count = len([r for r in results if r.get("judgment") in ["ï¼Ÿ", "ï¼"]])

    # å±é™ºãªãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’é›†è¨ˆ
    dangerous_domains = {}
    for result in results:
        if result.get("judgment") == "Ã—":
            try:
                domain = urlparse(result.get("url", "")).netloc
                if domain:
                    dangerous_domains[domain] = dangerous_domains.get(domain, 0) + 1
            except:
                pass

    # TOP5å±é™ºãƒ‰ãƒ¡ã‚¤ãƒ³
    top_dangerous = sorted(dangerous_domains.items(), key=lambda x: x[1], reverse=True)[:5]

    # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
    if dangerous_count > 0:
        if dangerous_count >= 3:
            recommended_action = "è‡³æ€¥å¯¾å¿œãŒå¿…è¦"
            action_details = f"{dangerous_count}ä»¶ã®å±é™ºã‚µã‚¤ãƒˆãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚æ³•çš„å¯¾å¿œã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
        else:
            recommended_action = "è¦æ³¨æ„ãƒ»ç›£è¦–ç¶™ç¶š"
            action_details = f"{dangerous_count}ä»¶ã®å±é™ºã‚µã‚¤ãƒˆãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚ç¶™ç¶šçš„ãªç›£è¦–ãŒå¿…è¦ã§ã™ã€‚"
    elif warning_count > 0:
        recommended_action = "çµŒéè¦³å¯Ÿ"
        action_details = f"{warning_count}ä»¶ã®ä¸æ˜ã‚µã‚¤ãƒˆãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚å®šæœŸçš„ãªå†æ¤œæŸ»ã‚’æ¨å¥¨ã—ã¾ã™ã€‚"
    else:
        recommended_action = "å®‰å…¨"
        action_details = "å±é™ºãªã‚µã‚¤ãƒˆã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚"

    return {
        "summary": {
            "analysis_date": record.get("analysis_time", "ä¸æ˜"),
            "image_filename": record.get("original_filename", "ä¸æ˜"),
            "total_detected": total_count,
            "safe_sites": safe_count,
            "dangerous_sites": dangerous_count,
            "warning_sites": warning_count
        },
        "risk_assessment": {
            "level": "é«˜" if dangerous_count >= 3 else "ä¸­" if dangerous_count > 0 else "ä½",
            "recommended_action": recommended_action,
            "action_details": action_details
        },
        "top_dangerous_domains": [
            {"domain": domain, "count": count} for domain, count in top_dangerous
        ],
        "recommendations": [
            "å®šæœŸçš„ãªå†æ¤œæŸ»ã®å®Ÿæ–½",
            "æ¤œå‡ºã•ã‚ŒãŸå±é™ºã‚µã‚¤ãƒˆã¸ã®æ³•çš„å¯¾å¿œ",
            "ç¤¾å†…ã¸ã®æ³¨æ„å–šèµ·ã¨æ•™è‚²",
            "æ¤œå‡ºçµæœã®ç¤¾å†…å…±æœ‰"
        ]
    }

@app.get("/api/report/csv/{image_id}")
async def download_csv_report(image_id: str):
    """
    CSVå½¢å¼ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
    """
    logger.info(f"ğŸ“Š CSVãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆè¦æ±‚: image_id={image_id}")

    try:
        # CSVãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        csv_content = generate_csv_report(image_id)

        # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        timestamp = int(datetime.now().timestamp())
        filename = f"leak_detection_report_{image_id}_{timestamp}.csv"

        logger.info(f"âœ… CSVãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {filename}")

        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™
        return Response(
            content=csv_content.encode('utf-8'),
            media_type="text/csv",
            headers={
                "Content-Disposition": f"attachment; filename={filename}",
                "Content-Type": "text/csv; charset=utf-8"
            }
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ CSVãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": "csv_report_generation_failed",
                "message": f"CSVãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}",
                "image_id": image_id
            }
        )

@app.get("/api/report/summary/{image_id}")
async def get_summary_report(image_id: str):
    """
    çµŒå–¶å±¤å‘ã‘ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’å–å¾—ã™ã‚‹
    """
    logger.info(f"ğŸ“Š ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆè¦æ±‚: image_id={image_id}")

    try:
        # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
        summary_data = generate_summary_report(image_id)

        logger.info(f"âœ… ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {image_id}")

        return {
            "success": True,
            "image_id": image_id,
            "report": summary_data,
            "generated_at": datetime.now().isoformat()
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": "summary_report_generation_failed",
                "message": f"ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}",
                "image_id": image_id
            }
        )

@app.post("/batch-upload")
async def batch_upload_images(files: List[UploadFile] = File(...)):
    """
    è¤‡æ•°ã®ç”»åƒã‚’ä¸€æ‹¬ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
    """
    logger.info(f"ğŸ“¤ ãƒãƒƒãƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {len(files)}ãƒ•ã‚¡ã‚¤ãƒ«")

    # ãƒ•ã‚¡ã‚¤ãƒ«æ•°åˆ¶é™ãƒã‚§ãƒƒã‚¯
    if len(files) > 10:
        raise HTTPException(
            status_code=400,
            detail={
                "error": "too_many_files",
                "message": "ãƒ•ã‚¡ã‚¤ãƒ«æ•°ãŒä¸Šé™ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚æœ€å¤§10ãƒ•ã‚¡ã‚¤ãƒ«ã¾ã§ã§ã™ã€‚",
                "max_files": 10,
                "received_files": len(files)
            }
        )

    total_size = 0
    uploaded_files = []
    errors = []

    for i, file in enumerate(files):
        try:
            logger.info(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ ({i+1}/{len(files)}): {file.filename}")

            # ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼
            if not validate_file(file):
                errors.append({
                    "filename": file.filename,
                    "error": "invalid_file_format",
                    "message": f"ç„¡åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {file.content_type}"
                })
                continue

            # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            content = await file.read()
            file_size = len(content)
            total_size += file_size

            # åˆè¨ˆã‚µã‚¤ã‚ºåˆ¶é™ãƒã‚§ãƒƒã‚¯ï¼ˆ50MBï¼‰
            if total_size > 50 * 1024 * 1024:
                errors.append({
                    "filename": file.filename,
                    "error": "total_size_exceeded",
                    "message": "åˆè¨ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒ50MBã‚’è¶…ãˆã¦ã„ã¾ã™"
                })
                break

            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆåˆ¶é™ã¯è¡Œã‚ãªã„ï¼‰
            logger.info(f"ğŸ“Š {file.filename}: {file_size / (1024*1024):.1f}MB")

            # ãƒ•ã‚¡ã‚¤ãƒ«ç¨®åˆ¥ã«ã‚ˆã‚‹æ¤œè¨¼
            is_pdf = is_pdf_file(file.content_type or "", file.filename or "")

            if is_pdf:
                # PDFæ¤œè¨¼
                if not PDF_SUPPORT:
                    errors.append({
                        "filename": file.filename,
                        "error": "pdf_not_supported",
                        "message": "PDFå‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“"
                    })
                    continue

                try:
                    # PDFã®æœ‰åŠ¹æ€§ã‚’ç¢ºèª
                    test_images = convert_pdf_to_images(content)
                    if not test_images:
                        raise Exception("PDFã‹ã‚‰ç”»åƒã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
                except Exception as e:
                    errors.append({
                        "filename": file.filename,
                        "error": "corrupted_pdf",
                        "message": f"ç ´æã—ãŸPDFãƒ•ã‚¡ã‚¤ãƒ«: {str(e)}"
                    })
                    continue
            else:
                # ç”»åƒæ¤œè¨¼
                try:
                    image = Image.open(BytesIO(content))
                    image.verify()
                except Exception as e:
                    errors.append({
                        "filename": file.filename,
                        "error": "corrupted_image",
                        "message": f"ç ´æã—ãŸç”»åƒãƒ•ã‚¡ã‚¤ãƒ«: {str(e)}"
                    })
                    continue

            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            file_id = str(uuid.uuid4())
            file_extension = os.path.splitext(file.filename or "image")[1].lower() or ".jpg"
            safe_filename = f"{file_id}{file_extension}"
            file_path = os.path.join(UPLOAD_DIR, safe_filename)

            with open(file_path, "wb") as f:
                f.write(content)

            # è¨˜éŒ²ä¿å­˜
            upload_record = {
                "id": file_id,
                "original_filename": file.filename,
                "saved_filename": safe_filename,
                "file_path": file_path,
                "content_type": file.content_type,
                "file_size": file_size,
                "upload_time": datetime.now().isoformat(),
                "status": "uploaded",
                "batch_upload": True,
                "file_type": "pdf" if is_pdf else "image"
            }

            upload_records[file_id] = upload_record
            uploaded_files.append({
                "file_id": file_id,
                "filename": file.filename,
                "size": file_size,
                "status": "success"
            })

            logger.info(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {file.filename} -> {file_id}")

        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼ {file.filename}: {str(e)}")
            errors.append({
                "filename": file.filename,
                "error": "processing_failed",
                "message": str(e)
            })

    # è¨˜éŒ²ã‚’ä¿å­˜
    save_records()

    logger.info(f"âœ… ãƒãƒƒãƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: æˆåŠŸ={len(uploaded_files)}ä»¶, ã‚¨ãƒ©ãƒ¼={len(errors)}ä»¶")

    return {
        "success": True,
        "total_files": len(files),
        "uploaded_count": len(uploaded_files),
        "error_count": len(errors),
        "total_size": total_size,
        "files": uploaded_files,
        "errors": errors,
        "upload_time": datetime.now().isoformat()
    }

@app.post("/batch-search")
async def batch_search_images(
    background_tasks: BackgroundTasks,
    request: dict,
    batch_id: Optional[str] = None
):
    """
    è¤‡æ•°ã®ç”»åƒã‚’ä¸€æ‹¬ã§æ¤œç´¢ã™ã‚‹
    """
    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£ã‹ã‚‰ file_ids ã‚’å–å¾—
    file_ids = request.get("file_ids", [])
    if not file_ids:
        raise HTTPException(
            status_code=422,
            detail="file_ids is required in request body"
        )

    if not batch_id:
        batch_id = str(uuid.uuid4())

    logger.info(f"ğŸ” ãƒãƒƒãƒæ¤œç´¢é–‹å§‹: batch_id={batch_id}, {len(file_ids)}ãƒ•ã‚¡ã‚¤ãƒ«")

    # ãƒãƒƒãƒã‚¸ãƒ§ãƒ–åˆæœŸåŒ–
    batch_jobs[batch_id] = {
        "batch_id": batch_id,
        "total_files": len(file_ids),
        "completed_files": 0,
        "status": "processing",
        "start_time": datetime.now().isoformat(),
        "files": []
    }

    # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆæœŸçŠ¶æ…‹ã‚’è¨­å®šï¼ˆã™ã¹ã¦ã®file_idsã«å¯¾å¿œï¼‰
    for file_id in file_ids:
        if file_id in upload_records:
            batch_jobs[batch_id]["files"].append({
                "file_id": file_id,
                "filename": upload_records[file_id].get("original_filename", "ä¸æ˜"),
                "status": "pending",
                "progress": 0
            })
        else:
            # å­˜åœ¨ã—ãªã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚é…åˆ—ã«è¿½åŠ ï¼ˆã‚¨ãƒ©ãƒ¼çŠ¶æ…‹ã§ï¼‰
            batch_jobs[batch_id]["files"].append({
                "file_id": file_id,
                "filename": "ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“",
                "status": "error",
                "progress": 0,
                "error": "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            })

    # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å‡¦ç†é–‹å§‹
    background_tasks.add_task(lambda: process_batch_search(batch_id, file_ids))

    return {
        "success": True,
        "batch_id": batch_id,
        "message": f"ãƒãƒƒãƒæ¤œç´¢ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚{len(file_ids)}ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¾ã™ã€‚",
        "total_files": len(file_ids)
    }

def process_batch_search(batch_id: str, file_ids: List[str]):
    """
    ãƒãƒƒãƒæ¤œç´¢ã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œ
    """
    try:
        for i, file_id in enumerate(file_ids):
            if batch_id not in batch_jobs:
                return

            # æ—¢ã«ã‚¨ãƒ©ãƒ¼çŠ¶æ…‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if batch_jobs[batch_id]["files"][i]["status"] == "error":
                logger.info(f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ— ({i+1}/{len(file_ids)}): {file_id} - æ—¢ã«ã‚¨ãƒ©ãƒ¼çŠ¶æ…‹")
                batch_jobs[batch_id]["completed_files"] = i + 1
                continue

            # ãƒ•ã‚¡ã‚¤ãƒ«çŠ¶æ…‹ã‚’æ›´æ–°
            batch_jobs[batch_id]["files"][i]["status"] = "processing"
            batch_jobs[batch_id]["files"][i]["progress"] = 0

            logger.info(f"ğŸ”„ ãƒãƒƒãƒæ¤œç´¢å‡¦ç†ä¸­ ({i+1}/{len(file_ids)}): {file_id}")
            logger.info(f"ğŸ“Š ãƒãƒƒãƒé€²æ—: {i+1}/{len(file_ids)} ({((i+1)/len(file_ids)*100):.1f}%)")

            try:
                # æ—¢å­˜ã®åˆ†æãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨
                if file_id not in upload_records:
                    batch_jobs[batch_id]["files"][i]["status"] = "error"
                    batch_jobs[batch_id]["files"][i]["error"] = "ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
                    continue

                record = upload_records[file_id]
                file_path = record["file_path"]
                file_type = record.get("file_type", "image")

                # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
                with open(file_path, 'rb') as file:
                    file_content = file.read()

                # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°
                batch_jobs[batch_id]["files"][i]["progress"] = 10

                # ãƒ•ã‚¡ã‚¤ãƒ«ç¨®åˆ¥ã«å¿œã˜ã¦å‡¦ç†ã‚’åˆ†å²
                if file_type == "pdf":
                    # PDFã®å ´åˆï¼šå„ãƒšãƒ¼ã‚¸ã‚’ç”»åƒã«å¤‰æ›ã—ã¦å‡¦ç†
                    pdf_images = convert_pdf_to_images(file_content)
                    if not pdf_images:
                        raise Exception("PDFã‹ã‚‰ç”»åƒã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")

                    # å„ãƒšãƒ¼ã‚¸ã®ç”»åƒãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—ï¼ˆæœ€åˆã®ãƒšãƒ¼ã‚¸ã‚’ãƒ¡ã‚¤ãƒ³ãƒãƒƒã‚·ãƒ¥ã¨ã™ã‚‹ï¼‰
                    image_hash = calculate_image_hash(pdf_images[0])

                    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°
                    batch_jobs[batch_id]["files"][i]["progress"] = 25

                    # å„ãƒšãƒ¼ã‚¸ã‚’å€‹åˆ¥ã«åˆ†æï¼ˆæ‹¡å¼µæ¤œç´¢ï¼‰
                    all_url_lists = []
                    for page_i, page_image_content in enumerate(pdf_images):
                        page_urls = enhanced_image_search_with_reverse(page_image_content)
                        all_url_lists.extend(page_urls)

                        # ãƒšãƒ¼ã‚¸ã”ã¨ã®ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°
                        page_progress = 25 + (page_i + 1) * 35 // len(pdf_images)
                        batch_jobs[batch_id]["files"][i]["progress"] = min(page_progress, 60)

                    # é‡è¤‡URLã‚’é™¤å»ï¼ˆè¾æ›¸å½¢å¼ãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰
                    seen_urls = set()
                    url_list = []
                    for url_data in all_url_lists:
                        url = url_data["url"] if isinstance(url_data, dict) else url_data
                        if url not in seen_urls:
                            seen_urls.add(url)
                            url_list.append(url_data)

                else:
                    # ç”»åƒã®å ´åˆï¼šå¾“æ¥ã®å‡¦ç†
                    image_content = file_content
                    image_hash = calculate_image_hash(image_content)

                    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°
                    batch_jobs[batch_id]["files"][i]["progress"] = 20

                    # æ‹¡å¼µWebæ¤œç´¢å®Ÿè¡Œï¼ˆé€†æ¤œç´¢æ©Ÿèƒ½ä»˜ãï¼‰
                    url_list = enhanced_image_search_with_reverse(image_content)

                # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°
                batch_jobs[batch_id]["files"][i]["progress"] = 60

                # URLåˆ†æ
                processed_results = []
                for j, url_data in enumerate(url_list[:50]):
                    # url_dataãŒè¾æ›¸å½¢å¼ã®å ´åˆã¨stringå½¢å¼ã®å ´åˆã«å¯¾å¿œ
                    if isinstance(url_data, dict):
                        url = url_data["url"]
                        search_method = url_data.get("search_method", "ä¸æ˜")
                        search_source = url_data.get("search_source", "ä¸æ˜")
                        confidence = url_data.get("confidence", "ä¸æ˜")
                    else:
                        # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã€stringå½¢å¼ã‚‚ã‚µãƒãƒ¼ãƒˆ
                        url = url_data
                        search_method = "ä¸æ˜"
                        search_source = "ä¸æ˜"
                        confidence = "ä¸æ˜"

                    result = analyze_url_efficiently(url)
                    if result:
                        # æ¤œç´¢æ–¹æ³•ã®æƒ…å ±ã‚’çµæœã«è¿½åŠ 
                        result["search_method"] = search_method
                        result["search_source"] = search_source
                        result["confidence"] = confidence
                        processed_results.append(result)

                    # å°åˆ»ã¿ãªé€²æ—æ›´æ–°
                    progress = 60 + (j + 1) * 30 // min(len(url_list), 50)  # 60% + 30%åˆ†ã‚’ URLåˆ†æã§ä½¿ç”¨
                    batch_jobs[batch_id]["files"][i]["progress"] = min(progress, 90)

                # çµæœä¿å­˜ï¼ˆç”Ÿã®æ¤œç´¢çµæœã‚‚å«ã‚ã‚‹ï¼‰
                search_results[file_id] = {
                    "processed_results": processed_results,
                    "raw_urls": url_list,  # ç”Ÿã®æ¤œç´¢çµæœï¼ˆsearch_method, search_source, confidenceä»˜ãï¼‰
                    "total_found": len(url_list),
                    "total_processed": len(processed_results)
                }

                # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è¨˜éŒ²æ›´æ–°
                record["analysis_status"] = "completed"
                record["analysis_time"] = datetime.now().isoformat()
                record["found_urls_count"] = len(url_list)
                record["processed_results_count"] = len(processed_results)
                record["image_hash"] = image_hash

                # å±¥æ­´ä¿å­˜
                save_analysis_to_history(file_id, image_hash, processed_results)

                # å®Œäº†çŠ¶æ…‹æ›´æ–°
                batch_jobs[batch_id]["files"][i]["status"] = "completed"
                batch_jobs[batch_id]["files"][i]["progress"] = 100
                batch_jobs[batch_id]["files"][i]["results_count"] = len(processed_results)

                logger.info(f"âœ… ãƒãƒƒãƒæ¤œç´¢å®Œäº† ({i+1}/{len(file_ids)}): {file_id}")
                logger.info(f"ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ« {i+1} ã®çµæœ: URLç™ºè¦‹={len(url_list)}ä»¶, åˆ†æå®Œäº†={len(processed_results)}ä»¶")

            except Exception as e:
                logger.error(f"âŒ ãƒãƒƒãƒæ¤œç´¢ã‚¨ãƒ©ãƒ¼ {file_id}: {str(e)}")
                batch_jobs[batch_id]["files"][i]["status"] = "error"
                batch_jobs[batch_id]["files"][i]["error"] = str(e)

            # å®Œäº†ãƒ•ã‚¡ã‚¤ãƒ«æ•°æ›´æ–°
            batch_jobs[batch_id]["completed_files"] = i + 1

            # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ï¼ˆå„ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å¾Œï¼‰
            gc.collect()

        # å…¨ä½“å®Œäº†
        batch_jobs[batch_id]["status"] = "completed"
        batch_jobs[batch_id]["end_time"] = datetime.now().isoformat()
        save_records()

        logger.info(f"âœ… ãƒãƒƒãƒæ¤œç´¢å…¨ä½“å®Œäº†: batch_id={batch_id}")

    except Exception as e:
        import traceback
        logger.error(f"âŒ ãƒãƒƒãƒæ¤œç´¢å…¨ä½“ã‚¨ãƒ©ãƒ¼: {str(e)}")
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")
        if batch_id in batch_jobs:
            batch_jobs[batch_id]["status"] = "error"
            batch_jobs[batch_id]["error"] = str(e)
            batch_jobs[batch_id]["end_time"] = datetime.now().isoformat()

@app.get("/batch-status/{batch_id}")
async def get_batch_status(batch_id: str):
    """
    ãƒãƒƒãƒå‡¦ç†ã®é€²æ—çŠ¶æ³ã‚’å–å¾—
    """
    if batch_id not in batch_jobs:
        raise HTTPException(
            status_code=404,
            detail="æŒ‡å®šã•ã‚ŒãŸãƒãƒƒãƒIDãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        )

    return {
        "success": True,
        "batch": batch_jobs[batch_id]
    }

@app.get("/image/{file_id}")
async def get_image(file_id: str):
    """
    ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ï¼ˆã‚¨ãƒ©ãƒ¼å›é¿å¼·åŒ–ç‰ˆï¼‰
    """
    try:
        if file_id not in upload_records:
            logger.warning(f"âš ï¸ ç”»åƒå–å¾—: å­˜åœ¨ã—ãªã„file_id {file_id}")
            raise HTTPException(
                status_code=404,
                detail="æŒ‡å®šã•ã‚ŒãŸç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            )

        record = upload_records[file_id]
        file_path = record.get("file_path")

        if not file_path:
            logger.warning(f"âš ï¸ ç”»åƒå–å¾—: file_pathãŒç©º {file_id}")
            raise HTTPException(
                status_code=404,
                detail="ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ãŒè¨˜éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“"
            )

        if not os.path.exists(file_path):
            logger.warning(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«æ¶ˆå¤±æ¤œå‡º: {file_id} - {file_path}")

            # PDFãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯ä»£æ›¿å‡¦ç†ã‚’æä¾›
            if record.get("file_type") == "pdf":
                raise HTTPException(
                    status_code=404,
                    detail=f"PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆå†ãƒ‡ãƒ—ãƒ­ã‚¤ã«ã‚ˆã‚Šæ¶ˆå¤±ï¼‰: {record.get('original_filename', 'unknown')}"
                )
            else:
                raise HTTPException(
                    status_code=404,
                        detail=f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆå†ãƒ‡ãƒ—ãƒ­ã‚¤ã«ã‚ˆã‚Šæ¶ˆå¤±ï¼‰: {record.get('original_filename', 'unknown')}"
                )

        # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã‹ã‚‰é©åˆ‡ãªãƒ¡ãƒ‡ã‚£ã‚¢ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
        _, ext = os.path.splitext(file_path)
        media_type_map = {
            '.jpg': 'image/jpeg',
            '.jpeg': 'image/jpeg',
            '.png': 'image/png',
            '.gif': 'image/gif',
            '.webp': 'image/webp',
            '.pdf': 'application/pdf'
        }
        media_type = media_type_map.get(ext.lower(), 'image/jpeg')

        return FileResponse(
            file_path,
            media_type=media_type,
            filename=record.get("original_filename", f"image{ext}")
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ç”»åƒå–å¾—ã‚¨ãƒ©ãƒ¼ {file_id}: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail="ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"
        )

@app.get("/file-info/{file_id}")
async def get_file_info(file_id: str):
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã®æƒ…å ±ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã€ã‚¿ã‚¤ãƒ—ç­‰ï¼‰ã‚’å–å¾—ã™ã‚‹
    """
    if file_id not in upload_records:
        raise HTTPException(
            status_code=404,
            detail="æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
        )

    record = upload_records[file_id]

    # ãƒ•ã‚¡ã‚¤ãƒ«ã®ç‰©ç†çš„å­˜åœ¨ã‚’ãƒã‚§ãƒƒã‚¯
    file_path = record.get("file_path", "")
    file_exists = os.path.exists(file_path) if file_path else False

    return {
        "file_id": file_id,
        "filename": record.get("original_filename", "ä¸æ˜"),
        "fileType": record.get("file_type", "image"),
        "fileSize": record.get("file_size", 0),
        "uploadTime": record.get("upload_time", ""),
        "analysisStatus": record.get("analysis_status", "pending"),
        "fileExists": file_exists,
        "filePath": file_path if file_exists else None
    }

@app.get("/pdf-preview/{file_id}")
async def get_pdf_preview(file_id: str):
    """
    PDFãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€åˆã®ãƒšãƒ¼ã‚¸ã‚’ç”»åƒã¨ã—ã¦å–å¾—ã™ã‚‹ï¼ˆã‚¨ãƒ©ãƒ¼å›é¿å¼·åŒ–ç‰ˆï¼‰
    """
    try:
        if file_id not in upload_records:
            logger.warning(f"âš ï¸ PDFãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: å­˜åœ¨ã—ãªã„file_id {file_id}")
            raise HTTPException(
                status_code=404,
                detail="æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            )

        record = upload_records[file_id]
        file_path = record.get("file_path")

        if not file_path:
            logger.warning(f"âš ï¸ PDFãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: file_pathãŒç©º {file_id}")
            raise HTTPException(
                status_code=404,
                detail="ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ãŒè¨˜éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“"
            )

        if not os.path.exists(file_path):
            logger.warning(f"âš ï¸ PDFãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«æ¶ˆå¤±æ¤œå‡º {file_id} - {file_path}")
            raise HTTPException(
                status_code=404,
                detail=f"PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆå†ãƒ‡ãƒ—ãƒ­ã‚¤ã«ã‚ˆã‚Šæ¶ˆå¤±ï¼‰: {record.get('original_filename', 'unknown')}"
            )

        # PDFãƒ•ã‚¡ã‚¤ãƒ«ã‹ãƒã‚§ãƒƒã‚¯
        if record.get("file_type") != "pdf":
            logger.warning(f"âš ï¸ PDFãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: PDFä»¥å¤–ã®ãƒ•ã‚¡ã‚¤ãƒ« {file_id}")
            raise HTTPException(
                status_code=400,
                detail="æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã¯PDFã§ã¯ã‚ã‚Šã¾ã›ã‚“"
            )

        # PDFã®æœ€åˆã®ãƒšãƒ¼ã‚¸ã‚’ç”»åƒã«å¤‰æ›
        with open(file_path, 'rb') as file:
            pdf_content = file.read()

        pdf_images = convert_pdf_to_images(pdf_content)
        if not pdf_images:
            logger.error(f"âŒ PDFãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: ç”»åƒå¤‰æ›å¤±æ•— {file_id}")
            raise HTTPException(
                status_code=500,
                detail="PDFã‹ã‚‰ç”»åƒã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸ"
            )

        # æœ€åˆã®ãƒšãƒ¼ã‚¸ã®ç”»åƒã‚’è¿”ã™
        first_page_image = pdf_images[0]

        return Response(
            content=first_page_image,
            media_type="image/png",
            headers={"Content-Disposition": f"inline; filename=\"{file_id}_preview.png\""}
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ PDFãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼ {file_id}: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"PDFãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        )

# URLåˆ†æé–¢æ•°ç¾¤
def analyze_url_efficiently(url: str) -> dict | None:
    """
    URLã‚’åŠ¹ç‡çš„ã«åˆ†æã—ã€åˆ¤å®šçµæœã‚’è¿”ã™
    X URLã¯ç‰¹åˆ¥å‡¦ç†ã§APIçµŒç”±ã§è©³ç´°åˆ†æ
    """
    try:
        logger.info(f"ğŸ”„ URLåˆ†æé–‹å§‹: {url}")

        # X (Twitter) URLã®ç‰¹åˆ¥å‡¦ç†
        if 'twitter.com' in url or 'x.com' in url:
            logger.info(f"ğŸ¦ X URLæ¤œå‡º - APIçµŒç”±ã§è©³ç´°åˆ†æ: {url}")

            # X APIã§ãƒ„ã‚¤ãƒ¼ãƒˆå†…å®¹ã‚’å–å¾—
            x_data = get_x_tweet_content(url)
            if x_data:
                # Gemini AIã§åˆ¤å®š
                judgment_result = judge_x_content_with_gemini(x_data)

                # çµæœã‚’æ§‹ç¯‰
                return {
                    "url": url,
                    "judgment": judgment_result["judgment"],
                    "reason": judgment_result["reason"],
                    "confidence": judgment_result["confidence"],
                    "analysis_type": "X API + Gemini AI",
                    "x_username": x_data.get("username", ""),
                    "x_display_name": x_data.get("display_name", ""),
                    "x_tweet_text": x_data.get("tweet_text", "")[:100] + "..." if len(x_data.get("tweet_text", "")) > 100 else x_data.get("tweet_text", "")
                }
            else:
                # X APIå–å¾—å¤±æ•—æ™‚ã¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                logger.warning(f"âš ï¸ X APIå–å¾—å¤±æ•—ã€ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {url}")
                return analyze_url_with_scraping(url)

        # ãã®ä»–ã®URLã¯é€šå¸¸ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°åˆ†æ
        else:
            return analyze_url_with_scraping(url)

    except Exception as e:
        logger.error(f"âŒ URLåˆ†æã‚¨ãƒ©ãƒ¼ {url}: {str(e)}")
        return None

def analyze_url_with_scraping(url: str) -> dict | None:
    """
    URLã‚’ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†é¡ã«åŸºã¥ã„ã¦åŠ¹ç‡çš„ã«åˆ¤å®š
    å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ â†’ å³æ™‚â—‹åˆ¤å®šï¼ˆGemini APIä¸ä½¿ç”¨ï¼‰
    éå…¬å¼/SNS â†’ Gemini AIã§è©³ç´°åˆ†æ
    """
    try:
        from urllib.parse import urlparse
        parsed = urlparse(url)
        domain = parsed.netloc.lower()

        # 1. å…¬å¼ãƒ»ä¿¡é ¼ãƒ‰ãƒ¡ã‚¤ãƒ³ã®å³æ™‚â—‹åˆ¤å®šï¼ˆGemini APIä¸ä½¿ç”¨ï¼‰
        official_domains = [
            # å¤§æ‰‹ECãƒ»å…¬å¼ã‚µã‚¤ãƒˆ
            'amazon.co.jp', 'amazon.com', 'rakuten.co.jp', 'yahoo.co.jp',
            'mercari.com', 'mercari.jp', 'paypay.ne.jp', 'paypaymall.yahoo.co.jp',

            # å¤§æ‰‹ä¼æ¥­å…¬å¼
            'nintendo.com', 'sony.com', 'microsoft.com', 'apple.com',
            'google.com', 'youtube.com', 'wikipedia.org',

            # æ”¿åºœãƒ»æ•™è‚²æ©Ÿé–¢
            'gov.jp', 'go.jp', 'ac.jp', 'ed.jp',

            # å¤§æ‰‹ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ»ãƒ‹ãƒ¥ãƒ¼ã‚¹
            'nhk.or.jp', 'asahi.com', 'yomiuri.co.jp', 'mainichi.jp',
            'nikkei.com', 'sankei.com', 'tokyo-np.co.jp',

            # ã‚¨ãƒ³ã‚¿ãƒ¡ãƒ»å°‚é–€ãƒ¡ãƒ‡ã‚£ã‚¢
            'famitsu.com', 'oricon.co.jp', 'natalie.mu',
            'animenewsnetwork.com', 'seigura.com', 'dengekionline.com',

            # å‡ºç‰ˆç¤¾å…¬å¼
            'kadokawa.co.jp', 'shogakukan.co.jp', 'kodansha.co.jp',
            'shueisha.co.jp', 'hakusensha.co.jp', 'futabasha.co.jp',

            # ã‚²ãƒ¼ãƒ ãƒ»ã‚¢ãƒ‹ãƒ¡å…¬å¼
            'square-enix.com', 'bandai.co.jp', 'konami.com',
            'capcom.com', 'sega.com', 'atlus.com'
        ]

        for official in official_domains:
            if official in domain:
                logger.info(f"âœ… å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãŸã‚å³æ™‚â—‹åˆ¤å®šï¼ˆGemini APIä¸ä½¿ç”¨ï¼‰: {url}")
                return {
                    "url": url,
                    "judgment": "â—‹",
                    "reason": "ä¿¡é ¼ã§ãã‚‹å…¬å¼ã‚µã‚¤ãƒˆ",
                    "confidence": "é«˜",
                    "analysis_type": "å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³å³æ™‚åˆ¤å®š",
                    "domain_category": "å…¬å¼ã‚µã‚¤ãƒˆ"
                }

        # 2. éå…¬å¼ãƒ»SNSãƒ»ä¸æ˜ãƒ‰ãƒ¡ã‚¤ãƒ³ã®è©³ç´°åˆ†æï¼ˆGemini APIä½¿ç”¨ï¼‰
        logger.info(f"ğŸ” éå…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³æ¤œå‡º - Gemini AIã§è©³ç´°åˆ†æ: {url}")

        # ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚«ãƒ†ã‚´ãƒªã‚’åˆ¤å®š
        domain_category = classify_domain_type(domain)

        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—
        content = scrape_page_content(url)
        if not content:
            return {
                "url": url,
                "judgment": "ï¼Ÿ",
                "reason": "ãƒšãƒ¼ã‚¸å†…å®¹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ",
                "confidence": "ä¸æ˜",
                "analysis_type": "ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—",
                "domain_category": domain_category
            }

        # Gemini AIã§è©³ç´°åˆ¤å®š
        judgment_result = judge_content_with_gemini(content, domain_category)

        return {
            "url": url,
            "judgment": judgment_result["judgment"],
            "reason": judgment_result["reason"],
            "confidence": judgment_result["confidence"],
            "analysis_type": "Gemini AIè©³ç´°åˆ†æ",
            "domain_category": domain_category
        }

    except Exception as e:
        logger.error(f"âŒ URLåˆ†æã‚¨ãƒ©ãƒ¼ {url}: {str(e)}")
        return None

def classify_domain_type(domain: str) -> str:
    """
    ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ã‚¿ã‚¤ãƒ—ã‚’åˆ†é¡
    """
    domain_lower = domain.lower()

    # SNSãƒ»ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢
    if any(sns in domain_lower for sns in [
        'twitter.com', 'x.com', 'instagram.com', 'facebook.com',
        'tiktok.com', 'youtube.com', 'pinterest.com', 'tumblr.com',
        'threads.net', 'discord.com', 'reddit.com'
    ]):
        return "SNSãƒ»ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢"

    # ãƒ–ãƒ­ã‚°ãƒ»å€‹äººã‚µã‚¤ãƒˆ
    elif any(blog in domain_lower for blog in [
        'blog', 'diary', 'note.', 'hatenablog', 'ameblo', 'fc2',
        'wordpress', 'blogspot', 'medium.com'
    ]):
        return "ãƒ–ãƒ­ã‚°ãƒ»å€‹äººã‚µã‚¤ãƒˆ"

    # ãƒ•ã‚¡ã‚¤ãƒ«å…±æœ‰ãƒ»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚µã‚¤ãƒˆ
    elif any(file_share in domain_lower for file_share in [
        'mediafire', 'mega.nz', 'dropbox', 'drive.google',
        'onedrive', 'box.com', 'wetransfer'
    ]):
        return "ãƒ•ã‚¡ã‚¤ãƒ«å…±æœ‰ã‚µã‚¤ãƒˆ"

    # æ²ç¤ºæ¿ãƒ»ãƒ•ã‚©ãƒ¼ãƒ©ãƒ 
    elif any(forum in domain_lower for forum in [
        '2ch', '5ch', 'reddit', 'discord', 'slack'
    ]):
        return "æ²ç¤ºæ¿ãƒ»ãƒ•ã‚©ãƒ¼ãƒ©ãƒ "

    # ãã®ä»–ãƒ»ä¸æ˜
    else:
        return "ãã®ä»–ãƒ»ä¸æ˜ã‚µã‚¤ãƒˆ"

def judge_content_with_gemini(content: str, domain_category: str = "ä¸æ˜") -> dict:
    """
    ãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’Gemini AIã§åˆ¤å®š
    """
    if not gemini_model:
        return {
            "judgment": "ï¼Ÿ",
            "reason": "Gemini AIãŒåˆ©ç”¨ã§ãã¾ã›ã‚“",
            "confidence": "ä¸æ˜"
        }

    try:
        prompt = f"""
ã€ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†é¡ã€‘{domain_category}
ã€ãƒšãƒ¼ã‚¸å†…å®¹ã€‘{content[:1500]}

è‘—ä½œæ¨©ä¾µå®³ãƒ»é•æ³•ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

åˆ¤å®šåŸºæº–ï¼š
â—‹ï¼ˆå®‰å…¨ï¼‰: å…¬å¼ã‚µã‚¤ãƒˆã€æ­£å½“ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„
Ã—ï¼ˆå±é™ºï¼‰: è‘—ä½œæ¨©ä¾µå®³ã€é•æ³•ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€æµ·è³Šç‰ˆ
ï¼Ÿï¼ˆä¸æ˜ï¼‰: åˆ¤å®šå›°é›£

å›ç­”å½¢å¼: "åˆ¤å®š:[â—‹/Ã—/?] ç†ç”±:[150å­—ä»¥å†…ã®ç°¡æ½”ãªç†ç”±]"
å¿…ãš150å­—ä»¥å†…ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
"""

        logger.info("ğŸ¤– Gemini AIåˆ¤å®šé–‹å§‹")
        response = gemini_model.generate_content(prompt)

        if not response or not response.text:
            return {
                "judgment": "ï¼Ÿ",
                "reason": "AIå¿œç­”ãŒç©ºã§ã—ãŸ",
                "confidence": "ä¸æ˜"
            }

        response_text = response.text.strip()
        logger.info(f"ğŸ“‹ Geminiå¿œç­”: {response_text}")

        # å¿œç­”ã‚’è§£æ
        judgment = "ï¼Ÿ"
        reason = "åˆ¤å®šã§ãã¾ã›ã‚“ã§ã—ãŸ"

        if "åˆ¤å®š:" in response_text and "ç†ç”±:" in response_text:
            parts = response_text.split("ç†ç”±:")
            judgment_part = parts[0].replace("åˆ¤å®š:", "").strip()
            reason = parts[1].strip()

            if "â—‹" in judgment_part:
                judgment = "â—‹"
            elif "Ã—" in judgment_part:
                judgment = "Ã—"
            else:
                judgment = "ï¼Ÿ"
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è§£æ
            if "â—‹" in response_text:
                judgment = "â—‹"
            elif "Ã—" in response_text:
                judgment = "Ã—"
            reason = response_text

        # ç†ç”±ã‚’300å­—ä»¥å†…ã«åˆ¶é™
        if len(reason) > 300:
            reason = reason[:297] + "..."
            logger.info(f"ğŸ“ ç†ç”±ã‚’300å­—ã«çŸ­ç¸®ã—ã¾ã—ãŸ")

        logger.info(f"âœ… Geminiåˆ¤å®šå®Œäº†: {judgment} - {reason[:50]}...")

        return {
            "judgment": judgment,
            "reason": reason,
            "confidence": "é«˜" if judgment in ["â—‹", "Ã—"] else "ä½"
        }

    except Exception as e:
        logger.error(f"âŒ Geminiåˆ¤å®šã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {
            "judgment": "ï¼Ÿ",
            "reason": f"åˆ¤å®šã‚¨ãƒ©ãƒ¼: {str(e)}",
            "confidence": "ä¸æ˜"
        }

def scrape_page_content(url: str) -> str | None:
    """
    URLã‹ã‚‰ãƒšãƒ¼ã‚¸å†…å®¹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
    """
    # ç”»åƒURLã®å ´åˆã¯ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ™ãƒ¼ã‚¹ã§åˆ†é¡
    image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp']
    if any(url.lower().endswith(ext) for ext in image_extensions):
        logger.info(f"ğŸ–¼ï¸ ç”»åƒURLæ¤œå‡º - ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ™ãƒ¼ã‚¹åˆ†é¡: {url}")
        return f"ç”»åƒURL: {url}"

    # Instagramå°‚ç”¨å‡¦ç†
    if 'instagram.com' in url:
        return extract_instagram_content(url)

    # Threadså°‚ç”¨å‡¦ç†
    if 'threads.net' in url:
        return extract_threads_content(url)

    logger.info(f"ğŸŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {url}")
    try:
        with httpx.Client(timeout=10.0, follow_redirects=True) as client:
            # Content-Typeã‚’äº‹å‰ç¢ºèª
            try:
                head_response = client.head(url, headers={'User-Agent': 'Mozilla/5.0'})
                content_type = head_response.headers.get('content-type', '').lower()
                if 'text/html' not in content_type:
                    logger.info(f"â­ï¸  HTMLã§ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ— (Content-Type: {content_type}): {url}")
                    return None
            except httpx.RequestError as e:
                logger.warning(f"âš ï¸ HEADãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•— (GETã§ç¶šè¡Œ): {e}")

            # GETãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—
            response = client.get(url, headers={'User-Agent': 'Mozilla/5.0'})
            response.raise_for_status()

        # BeautifulSoupã§è§£æ
        soup = BeautifulSoup(response.text, 'html.parser')
        title = soup.title.string if soup.title else ""
        body_text = " ".join([p.get_text() for p in soup.find_all('p', limit=5)])

        content = f"Title: {title.strip()}\n\nBody: {body_text.strip()}"
        logger.info(f"ğŸ“ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: {len(content)} chars")
        return content

    except httpx.HTTPStatusError as e:
        logger.error(f"âŒ HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚¨ãƒ©ãƒ¼ {url}: {e.response.status_code} {e.response.reason_phrase}")
        return None
    except Exception as e:
        logger.error(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸€èˆ¬ã‚¨ãƒ©ãƒ¼ {url}: {e}")
        return None

def extract_instagram_content(url: str) -> str:
    """InstagramæŠ•ç¨¿ã‹ã‚‰å†…å®¹ã‚’æŠ½å‡º"""
    try:
        logger.info(f"ğŸ“¸ Instagramå°‚ç”¨è§£æ: {url}")

        with httpx.Client(timeout=10.0, follow_redirects=True) as client:
            response = client.get(url, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
        title = ""
        description = ""

        # og:title
        og_title = soup.find('meta', property='og:title')
        if og_title:
            title = og_title.get('content', '')

        # og:description
        og_desc = soup.find('meta', property='og:description')
        if og_desc:
            description = og_desc.get('content', '')

        content = f"InstagramæŠ•ç¨¿\nã‚¿ã‚¤ãƒˆãƒ«: {title}\nèª¬æ˜: {description}"
        logger.info(f"ğŸ“¸ Instagramè§£æå®Œäº†: {len(content)} chars")
        return content

    except Exception as e:
        return f"InstagramæŠ•ç¨¿: {url}"

def extract_threads_content(url: str) -> str:
    """ThreadsæŠ•ç¨¿ã‹ã‚‰å†…å®¹ã‚’æŠ½å‡º"""
    try:
        logger.info(f"ğŸ§µ Threadså°‚ç”¨è§£æ: {url}")

        with httpx.Client(timeout=10.0, follow_redirects=True) as client:
            response = client.get(url, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
        title = ""
        description = ""

        # og:title
        og_title = soup.find('meta', property='og:title')
        if og_title:
            title = og_title.get('content', '')

        # og:description
        og_desc = soup.find('meta', property='og:description')
        if og_desc:
            description = og_desc.get('content', '')

        content = f"ThreadsæŠ•ç¨¿\nã‚¿ã‚¤ãƒˆãƒ«: {title}\nèª¬æ˜: {description}"
        logger.info(f"ğŸ§µ Threadsè§£æå®Œäº†: {len(content)} chars")
        return content

    except Exception as e:
        return f"ThreadsæŠ•ç¨¿: {url}"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)