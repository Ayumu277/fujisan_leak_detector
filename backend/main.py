from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
import os
import json
import uuid
import base64
import re
import logging
import requests
from datetime import datetime
from typing import Dict, List, Optional
from io import BytesIO
from dotenv import load_dotenv
from PIL import Image
import serpapi
import httpx
from bs4 import BeautifulSoup
from google.cloud import vision

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ãƒ­ã‚°ä¿å­˜ç”¨ï¼ˆãƒ¡ãƒ¢ãƒªå†…ï¼‰
system_logs = []
MAX_LOGS = 100  # æœ€å¤§ä¿å­˜ãƒ­ã‚°æ•°

class ListHandler(logging.Handler):
    """ãƒ­ã‚°ã‚’ãƒªã‚¹ãƒˆã«ä¿å­˜ã™ã‚‹ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    def emit(self, record):
        log_entry = {
            "timestamp": datetime.fromtimestamp(record.created).strftime("%Y-%m-%d %H:%M:%S"),
            "level": record.levelname,
            "message": record.getMessage()
        }
        system_logs.append(log_entry)
        # å¤ã„ãƒ­ã‚°ã‚’å‰Šé™¤
        if len(system_logs) > MAX_LOGS:
            system_logs.pop(0)

# ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¿½åŠ 
list_handler = ListHandler()
logger.addHandler(list_handler)

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

app = FastAPI(title="Book Leak Detector", version="1.0.0")

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å„ç¨®API_KEYã‚’å–å¾—
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
GOOGLE_CSE_ID = os.getenv("GOOGLE_CSE_ID")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
SERPAPI_KEY = os.getenv("SERPAPI_KEY")
GOOGLE_APPLICATION_CREDENTIALS = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")

# API_KEYã®è¨­å®šçŠ¶æ³ã‚’ãƒã‚§ãƒƒã‚¯
missing_keys = []
if not GOOGLE_API_KEY:
    missing_keys.append("GOOGLE_API_KEY")
if not GOOGLE_CSE_ID:
    missing_keys.append("GOOGLE_CSE_ID")
if not GEMINI_API_KEY:
    missing_keys.append("GEMINI_API_KEY")
if not SERPAPI_KEY:
    missing_keys.append("SERPAPI_KEY")

if missing_keys:
    print(f"è­¦å‘Š: ä»¥ä¸‹ã®ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“: {', '.join(missing_keys)}")
    print("å®Œå…¨ãªæ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€.envãƒ•ã‚¡ã‚¤ãƒ«ã§ä»¥ä¸‹ã‚’è¨­å®šã—ã¦ãã ã•ã„:")
    print("- GOOGLE_API_KEY: Google APIç”¨")
    print("- GOOGLE_CSE_ID: Google Custom Search Engine IDç”¨")
    print("- GEMINI_API_KEY: Gemini AIç”¨")
    print("- SERPAPI_KEY: SerpAPIç”»åƒæ¤œç´¢ç”¨")
else:
    print("âœ“ ã™ã¹ã¦ã®API_KEYãŒæ­£å¸¸ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã™")

# CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:5173", "http://localhost:5174"],  # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã®URL
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
UPLOAD_DIR = "uploads"
os.makedirs(UPLOAD_DIR, exist_ok=True)

# é™çš„ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®šï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”»åƒç”¨ï¼‰
app.mount("/uploads", StaticFiles(directory=UPLOAD_DIR), name="uploads")

# ä¸€æ™‚çš„ãªç”»åƒå…¬é–‹ç”¨ï¼ˆæ¤œç´¢æ™‚ã®ã¿ä½¿ç”¨ï¼‰
app.mount("/temp-images", StaticFiles(directory=UPLOAD_DIR), name="temp-images")

# ãƒ¡ãƒ¢ãƒªå†…ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼ˆæœ¬ç•ªç’°å¢ƒã§ã¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨ï¼‰
upload_records: Dict[str, Dict] = {}
search_results: Dict[str, List[Dict]] = {}

# JSONãƒ•ã‚¡ã‚¤ãƒ«ã§ã®æ°¸ç¶šåŒ–
RECORDS_FILE = "upload_records.json"

def load_records():
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨˜éŒ²ã‚’èª­ã¿è¾¼ã¿"""
    global upload_records
    try:
        if os.path.exists(RECORDS_FILE):
            with open(RECORDS_FILE, 'r', encoding='utf-8') as f:
                upload_records = json.load(f)
    except Exception as e:
        print(f"è¨˜éŒ²ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
        upload_records = {}

def save_records():
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜éŒ²ã‚’ä¿å­˜"""
    try:
        with open(RECORDS_FILE, 'w', encoding='utf-8') as f:
            json.dump(upload_records, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"è¨˜éŒ²ã®ä¿å­˜ã«å¤±æ•—: {e}")

# ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã«è¨˜éŒ²ã‚’èª­ã¿è¾¼ã¿
load_records()

# å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒªã‚¹ãƒˆï¼ˆãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ï¼‰
OFFICIAL_DOMAINS = [
    # æ—¥æœ¬ã®å‡ºç‰ˆç¤¾ãƒ»æ›¸åº—
    'amazon.com', 'amazon.co.jp', 'rakuten.co.jp', 'bookwalker.jp',
    'kadokawa.co.jp', 'shogakukan.co.jp', 'kodansha.co.jp',
    'shueisha.co.jp', 'akitashoten.co.jp', 'hakusensha.co.jp',
    'square-enix.co.jp', 'enterbrain.co.jp', 'futabasha.co.jp',
    'houbunsha.co.jp', 'mag-garden.co.jp', 'shinchosha.co.jp',

    # æµ·å¤–ã®å‡ºç‰ˆç¤¾ãƒ»æ›¸åº—
    'viz.com', 'crunchyroll.com', 'funimation.com',
    'comixology.com', 'marvel.com', 'dc.com',
    'darkhorse.com', 'imagecomics.com', 'idwpublishing.com',

    # é›»å­æ›¸ç±ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 
    'kindle.amazon.com', 'kobo.rakuten.co.jp', 'ebookjapan.yahoo.co.jp',
    'cmoa.jp', 'booklive.jp', 'honto.jp', 'tsutaya.tsite.jp',

    # å…¬å¼ã‚µã‚¤ãƒˆä¾‹
    'publisher.co.jp', 'official-site.com'
]

# æ‚ªç”¨åˆ¤å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
SUSPICIOUS_KEYWORDS = [
    # æ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    'ç„¡æ–™ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰', 'é•æ³•', 'ã‚³ãƒ”ãƒ¼', 'æµ·è³Šç‰ˆ', 'ãƒ‘ã‚¤ãƒ¬ãƒ¼ãƒ„',
    'ç„¡æ–­è»¢è¼‰', 'è‘—ä½œæ¨©ä¾µå®³', 'crack', 'torrent', 'ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰',
    'ãƒªãƒ¼ã‚¯', 'ãƒã‚¿ãƒãƒ¬', 'å…ˆè¡Œå…¬é–‹', 'éå…¬å¼', 'ãƒ•ã‚¡ãƒ³ã‚µã‚¤ãƒˆ',

    # è‹±èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    'free download', 'illegal', 'piracy', 'pirate', 'copyright infringement',
    'unauthorized', 'leaked', 'ripped', 'cracked', 'bootleg',
    'fansite', 'fan translation', 'scanlation', 'raw manga'
]

# å±é™ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
DANGER_KEYWORDS = [
    'torrent', 'magnet', 'ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é•æ³•', 'æµ·è³Šç‰ˆé…å¸ƒ',
    'copyright violation', 'stolen content', 'illegal distribution'
]

def validate_image_file(file: UploadFile) -> bool:
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒæœ‰åŠ¹ãªç”»åƒã‹ã©ã†ã‹ã‚’æ¤œè¨¼"""
    allowed_types = ["image/jpeg", "image/png", "image/jpg", "image/gif", "image/webp"]
    return file.content_type in allowed_types

def encode_image_to_base64(image_path: str) -> str:
    """ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹"""
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

async def analyze_image_with_vision(image_path: str) -> Dict:
    """Google Vision APIã‚’ä½¿ã£ã¦ç”»åƒã‚’åˆ†æã™ã‚‹"""
    logger.info(f"ğŸ” Google Vision APIç”»åƒåˆ†æé–‹å§‹: {image_path}")

    try:
        # Vision APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
        client = vision.ImageAnnotatorClient()

        # ç”»åƒã‚’èª­ã¿è¾¼ã¿
        with open(image_path, 'rb') as image_file:
            content = image_file.read()
        image = vision.Image(content=content)

        # ãƒ†ã‚­ã‚¹ãƒˆæ¤œå‡º
        text_response = client.text_detection(image=image)
        texts = text_response.text_annotations

        # ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ¤œå‡º
        objects_response = client.object_localization(image=image)
        objects = objects_response.localized_object_annotations

        # ãƒ©ãƒ™ãƒ«æ¤œå‡º
        labels_response = client.label_detection(image=image)
        labels = labels_response.label_annotations

        # æ¤œå‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
        detected_text = ""
        if texts:
            detected_text = texts[0].description

        # ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆåã‚’åé›†
        detected_objects = [obj.name for obj in objects]

        # ãƒ©ãƒ™ãƒ«åã‚’åé›†
        detected_labels = [label.description for label in labels]

        logger.info(f"ğŸ“ æ¤œå‡ºãƒ†ã‚­ã‚¹ãƒˆ: {detected_text[:100] if detected_text else 'ãªã—'}")
        logger.info(f"ğŸ¯ æ¤œå‡ºã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ: {detected_objects}")
        logger.info(f"ğŸ·ï¸ æ¤œå‡ºãƒ©ãƒ™ãƒ«: {detected_labels}")

        # æ›¸ç±ãƒ»æ¼«ç”»é–¢é€£ã®åˆ¤å®š
        is_book_related = False
        suspicious_keywords = []

        # æ¤œå‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãƒ»ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒ»ãƒ©ãƒ™ãƒ«ã‚’å…¨ã¦ç¢ºèª
        all_detected_content = (detected_text + " " + " ".join(detected_objects) + " " + " ".join(detected_labels)).lower()

        # æ›¸ç±ãƒ»æ¼«ç”»é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        book_keywords = ["book", "manga", "comic", "novel", "text", "page", "chapter", "èª­ã‚€", "æœ¬", "æ¼«ç”»", "å°èª¬"]
        is_book_related = any(keyword in all_detected_content for keyword in book_keywords)

        # é•æ³•ãƒ»ç–‘ã‚ã—ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        illegal_keywords = ["ç„¡æ–™", "free", "download", "é•æ³•", "ã‚³ãƒ”ãƒ¼", "raw", "torrent", "piracy"]
        for keyword in illegal_keywords:
            if keyword in all_detected_content:
                suspicious_keywords.append(keyword)

        logger.info(f"ğŸ“š æ›¸ç±é–¢é€£: {is_book_related}")
        logger.info(f"âš ï¸ ç–‘ã‚ã—ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {suspicious_keywords}")

        return {
            "detected_text": detected_text,
            "detected_objects": detected_objects,
            "detected_labels": detected_labels,
            "is_book_related": is_book_related,
            "suspicious_keywords": suspicious_keywords,
            "analysis_success": True,
            "debug_info": {
                "all_detected_content": all_detected_content,
                "book_keywords_found": [k for k in book_keywords if k in all_detected_content],
                "illegal_keywords_found": [k for k in illegal_keywords if k in all_detected_content]
            }
        }

    except Exception as e:
        logger.error(f"âŒ Vision APIåˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {
            "detected_text": "",
            "detected_objects": [],
            "detected_labels": [],
            "is_book_related": False,
            "suspicious_keywords": [],
            "analysis_success": False,
            "error": str(e)
        }

async def check_domain_and_analyze(url: str, domain: str) -> Dict[str, str]:
    """ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’åˆ†æã—ã€å¿…è¦ã«å¿œã˜ã¦HTMLã‚’å–å¾—ã—ã¦å†…å®¹ã‚’åˆ†æã™ã‚‹"""

    logger.info(f"ğŸ” ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†æé–‹å§‹: {domain}")

    # å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯
    is_official = any(official_domain in domain.lower() for official_domain in OFFICIAL_DOMAINS)

    if is_official:
        logger.info(f"âœ… å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’æ¤œå‡º: {domain}")
        return {
            "status": "safe",
            "reason": "å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ã§ã™",
            "content_analysis": None
        }

    logger.info(f"ğŸŒ HTMLå–å¾—é–‹å§‹: {url}")

    # éå…¬å¼ã®å ´åˆã€HTMLã‚’å–å¾—ã—ã¦åˆ†æ
    try:
        async with httpx.AsyncClient(timeout=10.0, follow_redirects=True) as client:
            # User-Agentã‚’è¨­å®šã—ã¦ã‚¢ã‚¯ã‚»ã‚¹
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }

            logger.info(f"ğŸ“¡ HTTP ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡: {url}")
            response = await client.get(url, headers=headers)
            response.raise_for_status()

            logger.info(f"âœ… HTTP ãƒ¬ã‚¹ãƒãƒ³ã‚¹å—ä¿¡: {response.status_code}, {len(response.text)} chars")

            # HTMLã‚’ãƒ‘ãƒ¼ã‚¹
            soup = BeautifulSoup(response.text, 'html.parser')

            # JavaScriptã‚„CSSã‚’é™¤å»
            for script in soup(["script", "style"]):
                script.decompose()

            # ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã‚’æŠ½å‡º
            text_content = soup.get_text()

            # æ”¹è¡Œã‚„ç©ºç™½ã‚’æ•´ç†
            lines = (line.strip() for line in text_content.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = ' '.join(chunk for chunk in chunks if chunk)

            # ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã‚’åˆ¶é™ï¼ˆæœ€åˆã®2000æ–‡å­—ï¼‰
            text = text[:2000]

            logger.info(f"ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå®Œäº†: {len(text)} chars")

            # X(Twitter)ã®ç‰¹åˆ¥å‡¦ç†
            if 'twitter.com' in domain or 'x.com' in domain:
                logger.info("ğŸ¦ Twitter/Xç‰¹åˆ¥å‡¦ç†")
                return await analyze_twitter_content(text, url)

            # æ‚ªç”¨åˆ¤å®š
            logger.info("ğŸ” ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æé–‹å§‹")
            result = analyze_content_for_violations(text, domain)
            logger.info(f"âœ… åˆ†æå®Œäº†: {result['status']} - {result['reason']}")
            return result

    except httpx.TimeoutException:
        logger.error(f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {url}")
        return {
            "status": "unknown",
            "reason": "ã‚µã‚¤ãƒˆã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ",
            "content_analysis": None
        }
    except httpx.HTTPStatusError as e:
        logger.error(f"ğŸŒ HTTP ã‚¨ãƒ©ãƒ¼: {e.response.status_code} for {url}")
        return {
            "status": "unknown",
            "reason": f"HTTP ã‚¨ãƒ©ãƒ¼: {e.response.status_code}",
            "content_analysis": None
        }
    except Exception as e:
        logger.error(f"âŒ åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)} for {url}")
        return {
            "status": "unknown",
            "reason": f"åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
            "content_analysis": None
        }

async def analyze_twitter_content(text: str, url: str) -> Dict[str, str]:
    """Twitter/XæŠ•ç¨¿ã®å†…å®¹ã‚’åˆ†æã™ã‚‹"""
    text_lower = text.lower()

    # Twitterç‰¹æœ‰ã®æ‚ªç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
    twitter_suspicious_patterns = [
        'ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¯ã“ã¡ã‚‰', 'download here', 'link in bio',
        'dm for link', 'ãƒªãƒ³ã‚¯ã¯ dm ã§', 'è©³ç´°ã¯ dm',
        'free manga', 'ãƒ•ãƒªãƒ¼æ¼«ç”»', 'ç„¡æ–™ã§èª­ã‚ã‚‹'
    ]

    if any(pattern in text_lower for pattern in twitter_suspicious_patterns):
        return {
            "status": "suspicious",
            "reason": "TwitteræŠ•ç¨¿ã«ç–‘ã‚ã—ã„å†…å®¹ãŒå«ã¾ã‚Œã¦ã„ã¾ã™",
            "content_analysis": f"æŠ•ç¨¿å†…å®¹ï¼ˆä¸€éƒ¨ï¼‰: {text[:200]}..."
        }

    # é€šå¸¸ã®æ‚ªç”¨åˆ¤å®š
    return analyze_content_for_violations(text, 'twitter.com')

def analyze_content_for_violations(text: str, domain: str) -> Dict[str, str]:
    """ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã‹ã‚‰è‘—ä½œæ¨©ä¾µå®³ã‚„æ‚ªç”¨ã‚’åˆ¤å®šã™ã‚‹"""
    text_lower = text.lower()

    # å±é™ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€å„ªå…ˆï¼‰
    found_danger_keywords = [keyword for keyword in DANGER_KEYWORDS if keyword.lower() in text_lower]
    if found_danger_keywords:
        return {
            "status": "danger",
            "reason": f"å±é™ºãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {', '.join(found_danger_keywords)}",
            "content_analysis": f"åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆï¼ˆä¸€éƒ¨ï¼‰: {text[:300]}..."
        }

    # ç–‘ã‚ã—ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
    found_suspicious_keywords = [keyword for keyword in SUSPICIOUS_KEYWORDS if keyword.lower() in text_lower]
    if found_suspicious_keywords:
        return {
            "status": "suspicious",
            "reason": f"ç–‘ã‚ã—ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {', '.join(found_suspicious_keywords)}",
            "content_analysis": f"åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆï¼ˆä¸€éƒ¨ï¼‰: {text[:300]}..."
        }

    # ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ™ãƒ¼ã‚¹ã®åˆ¤å®š
    if any(suspicious in domain for suspicious in ['free', 'download', 'torrent', 'pirate']):
        return {
            "status": "suspicious",
            "reason": "ãƒ‰ãƒ¡ã‚¤ãƒ³åã«ç–‘ã‚ã—ã„è¦ç´ ãŒå«ã¾ã‚Œã¦ã„ã¾ã™",
            "content_analysis": None
        }

    # å®‰å…¨ã¨åˆ¤å®š
    return {
        "status": "medium",
        "reason": "ç‰¹ã«å•é¡Œã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ",
        "content_analysis": None
    }

def analyze_domain(url: str) -> tuple[str, bool, str]:
    """URLã‹ã‚‰ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’æŠ½å‡ºã—ã€åŸºæœ¬çš„ãªè„…å¨ãƒ¬ãƒ™ãƒ«ã‚’è©•ä¾¡ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ã™ï¼‰"""
    from urllib.parse import urlparse

    domain = urlparse(url).netloc.lower()
    is_official = any(official_domain in domain for official_domain in OFFICIAL_DOMAINS)

    # åŸºæœ¬çš„ãªè„…å¨ãƒ¬ãƒ™ãƒ«è©•ä¾¡
    if is_official:
        threat_level = "safe"
    elif any(dangerous in domain for dangerous in ['torrent', 'pirate', 'illegal']):
        threat_level = "danger"
    elif any(suspicious in domain for suspicious in ['free', 'download', 'manga', 'raw']):
        threat_level = "suspicious"
    elif domain.endswith('.com') or domain.endswith('.jp') or domain.endswith('.org'):
        threat_level = "medium"
    else:
        threat_level = "unknown"

    return domain, is_official, threat_level

async def analyze_and_judge_image(image_path: str) -> Dict:
    """ç”»åƒã‚’åˆ†æã—ã¦â—‹Ã—åˆ¤å®šã‚’è¡Œã†"""
    if not os.path.exists(image_path):
        raise HTTPException(status_code=404, detail="æŒ‡å®šã•ã‚ŒãŸç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    logger.info(f"ğŸ” ç”»åƒåˆ†æãƒ»åˆ¤å®šé–‹å§‹: {image_path}")

    try:
        # Google Vision APIã§ç”»åƒåˆ†æ
        vision_result = await analyze_image_with_vision(image_path)

        if not vision_result["analysis_success"]:
            logger.error("âŒ Vision APIåˆ†æã«å¤±æ•—")
            return {
                "judgment": "Ã—",
                "reason": "ç”»åƒåˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ",
                "details": vision_result.get("error", "ä¸æ˜ãªã‚¨ãƒ©ãƒ¼"),
                "confidence": 0
            }

        # æ›¸ç±é–¢é€£ã§ãªã„å ´åˆã¯å¯¾è±¡å¤–
        if not vision_result["is_book_related"]:
            logger.info("ğŸ“š æ›¸ç±é–¢é€£ã§ã¯ãªã„ç”»åƒ")
            return {
                "judgment": "â—‹",
                "reason": "æ›¸ç±ãƒ»æ¼«ç”»ã«é–¢é€£ã—ãªã„ç”»åƒã®ãŸã‚å•é¡Œãªã—",
                "details": f"æ¤œå‡ºå†…å®¹: {', '.join(vision_result['detected_labels'][:3])}",
                "confidence": 0.9
            }

        # ç–‘ã‚ã—ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚‹å ´åˆã¯Ã—
        if vision_result["suspicious_keywords"]:
            logger.warning(f"âš ï¸ ç–‘ã‚ã—ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º: {vision_result['suspicious_keywords']}")
            return {
                "judgment": "Ã—",
                "reason": f"é•æ³•ãƒ»ç–‘ã‚ã—ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {', '.join(vision_result['suspicious_keywords'])}",
                "details": vision_result["detected_text"][:200] if vision_result["detected_text"] else "ãƒ†ã‚­ã‚¹ãƒˆæ¤œå‡ºãªã—",
                "confidence": 0.8
            }

        # æ›¸ç±é–¢é€£ã ãŒç–‘ã‚ã—ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãªã—
        logger.info("âœ… æ›¸ç±é–¢é€£ã ãŒå•é¡Œãªã—")
        return {
            "judgment": "â—‹",
            "reason": "æ›¸ç±ãƒ»æ¼«ç”»é–¢é€£ã®ç”»åƒã§ã™ãŒã€å•é¡Œã¨ãªã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ",
            "details": vision_result["detected_text"][:200] if vision_result["detected_text"] else "ãƒ†ã‚­ã‚¹ãƒˆæ¤œå‡ºãªã—",
            "confidence": 0.7,
            "debug_analysis": {
                "vision_result": vision_result,
                "detected_objects": vision_result["detected_objects"],
                "detected_labels": vision_result["detected_labels"],
                "detected_text": vision_result["detected_text"],
                "suspicious_keywords_found": vision_result["suspicious_keywords"],
                "book_keywords_matched": vision_result.get("debug_info", {}).get("book_keywords_found", [])
            }
        }

    except Exception as e:
        logger.error(f"âŒ ç”»åƒåˆ†æãƒ»åˆ¤å®šã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {
            "judgment": "Ã—",
            "reason": f"åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
            "details": "",
            "confidence": 0
        }

# ä¸è¦ãªé–¢æ•°ã¯å‰Šé™¤ã•ã‚Œã¾ã—ãŸ

async def search_with_google_custom_search(image_path: str) -> List[Dict]:
    """Google Custom Search APIã‚’ä½¿ã£ãŸå®Ÿéš›ã®ç”»åƒé–¢é€£æ¤œç´¢"""
    if not GOOGLE_API_KEY or not GOOGLE_CSE_ID:
        logger.warning("âš ï¸ Google APIè¨­å®šãŒä¸å®Œå…¨ã€ã‚¹ã‚­ãƒƒãƒ—")
        return []

    try:
        logger.info("ğŸ” Google Custom Search APIæ¤œç´¢é–‹å§‹")

        # æ—¥æœ¬ã®æ›¸ç±ãƒ»æ¼«ç”»é–¢é€£ã®æµ·è³Šç‰ˆã‚µã‚¤ãƒˆã‚’å„ªå…ˆæ¤œç´¢
        search_queries = [
            "æ¼«ç”» ç„¡æ–™ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ é•æ³• ã‚µã‚¤ãƒˆ site:*.jp",
            "ãƒ©ã‚¤ãƒˆãƒãƒ™ãƒ« raw ç„¡æ–™ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ site:*.jp",
            "æœ¬ é›»å­æ›¸ç± é•æ³• ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ æµ·è³Šç‰ˆ",
            "manga raw download æ—¥æœ¬èª",
            "æ¼«ç”»æ‘ é¡ä¼¼ ã‚µã‚¤ãƒˆ é•æ³•"
        ]

        processed_results = []

        for query in search_queries:
            logger.info(f"ğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒª: {query}")

            search_url = "https://www.googleapis.com/customsearch/v1"
            params = {
                "key": GOOGLE_API_KEY,
                "cx": GOOGLE_CSE_ID,
                "q": query,
                "num": 5,
                "safe": "off",
                "lr": "lang_ja",  # æ—¥æœ¬èªã®ãƒšãƒ¼ã‚¸ã‚’å„ªå…ˆ
                "gl": "jp"        # æ—¥æœ¬ã‹ã‚‰ã®æ¤œç´¢ã¨ã—ã¦å®Ÿè¡Œ
            }

            async with httpx.AsyncClient() as client:
                response = await client.get(search_url, params=params)
                if response.status_code == 200:
                    data = response.json()
                    logger.info(f"ğŸ“‹ Googleæ¤œç´¢çµæœ: {len(data.get('items', []))}ä»¶")

                    for item in data.get('items', [])[:3]:  # å„ã‚¯ã‚¨ãƒªã‹ã‚‰æœ€å¤§3ä»¶
                        url = item.get('link', '')
                        title = item.get('title', 'ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜')
                        snippet = item.get('snippet', '')

                        if url:
                            domain = url.split('/')[2] if '/' in url else url

                            # å®Ÿéš›ã®ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†æã‚’å®Ÿè¡Œ
                            detailed_analysis = await check_domain_and_analyze(url, domain)

                            processed_results.append({
                                "url": url,
                                "domain": domain,
                                "title": title[:100],  # ã‚¿ã‚¤ãƒˆãƒ«ã‚’åˆ¶é™
                                "source": f"Googleæ¤œç´¢: {query[:30]}",
                                "is_official": detailed_analysis["status"] == "safe",
                                "threat_level": detailed_analysis["status"],
                                "detailed_analysis": detailed_analysis,
                                "thumbnail": "",
                                "analysis_timestamp": datetime.now().isoformat(),
                                "snippet": snippet[:200]  # ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’åˆ¶é™
                            })

        # é‡è¤‡URLã‚’é™¤å»
        unique_results = []
        seen_urls = set()

        for result in processed_results:
            if result["url"] not in seen_urls:
                seen_urls.add(result["url"])
                unique_results.append(result)

        logger.info(f"âœ… Googleæ¤œç´¢å‡¦ç†å®Œäº†: {len(unique_results)}ä»¶ï¼ˆé‡è¤‡é™¤å»å¾Œï¼‰")
        return unique_results[:10]  # æœ€å¤§10ä»¶

    except Exception as e:
        logger.error(f"âŒ Google Custom Search API ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []

async def search_based_on_image_features(image_path: str) -> List[Dict]:
    """ç”»åƒã®ç‰¹å¾´ã‹ã‚‰æ¨æ¸¬ã—ãŸå®Ÿéš›ã®Webæ¤œç´¢"""
    logger.info("ğŸ¯ ç”»åƒç‰¹å¾´ãƒ™ãƒ¼ã‚¹æ¤œç´¢é–‹å§‹")

    try:
        # ç”»åƒã®åŸºæœ¬æƒ…å ±ã‚’åˆ†æ
        filename = os.path.basename(image_path)

        # å®Ÿéš›ã®Webæ¤œç´¢ï¼ˆæµ·è³Šç‰ˆé–¢é€£ã‚µã‚¤ãƒˆæ¤œç´¢ï¼‰
        piracy_sites = [
            "mangafreak.net",
            "mangadex.org",
            "mangaraw.to",
            "rawmanga.top",
            "novelupdates.com"
        ]

        processed_results = []

        for site in piracy_sites[:5]:  # æœ€åˆã®5ã¤ã®ã‚µã‚¤ãƒˆã‚’ãƒã‚§ãƒƒã‚¯
            logger.info(f"ğŸ” ã‚µã‚¤ãƒˆåˆ†æ: {site}")

            # ã‚µã‚¤ãƒˆã®URLã‚’æ§‹ç¯‰
            url = f"https://{site}"

            try:
                # å®Ÿéš›ã®ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†æã‚’å®Ÿè¡Œ
                detailed_analysis = await check_domain_and_analyze(url, site)

                processed_results.append({
                    "url": url,
                    "domain": site,
                    "title": f"{site} - æµ·è³Šç‰ˆã‚µã‚¤ãƒˆæ¤œæŸ»çµæœ",
                    "source": "å®Ÿéš›ã®ã‚µã‚¤ãƒˆåˆ†æ",
                    "is_official": detailed_analysis["status"] == "safe",
                    "threat_level": detailed_analysis["status"],
                    "detailed_analysis": detailed_analysis,
                    "thumbnail": "",
                    "analysis_timestamp": datetime.now().isoformat()
                })

            except Exception as site_error:
                logger.warning(f"âš ï¸ ã‚µã‚¤ãƒˆ {site} ã®åˆ†æã§ã‚¨ãƒ©ãƒ¼: {str(site_error)}")
                continue

        logger.info(f"âœ… ç‰¹å¾´ãƒ™ãƒ¼ã‚¹æ¤œç´¢å®Œäº†: {len(processed_results)}ä»¶")
        return processed_results

    except Exception as e:
        logger.error(f"âŒ ç‰¹å¾´ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []

async def try_base64_serpapi_search(image_path: str) -> List[Dict]:
    """Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰æ–¹å¼ã§SerpAPIæ¤œç´¢ã‚’å®Ÿè¡Œ"""
    try:
        logger.info("ğŸ” Base64æ–¹å¼ã§SerpAPIæ¤œç´¢é–‹å§‹")

        # ç”»åƒã‚’Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        encoded_image = encode_image_to_base64(image_path)
        logger.info(f"ğŸ“¸ ç”»åƒBase64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰å®Œäº†: {len(encoded_image)} chars")

        client = serpapi.Client(api_key=SERPAPI_KEY)
        search_params = {
            "engine": "google_reverse_image",
            "image_url": f"data:image/jpeg;base64,{encoded_image}",
            "hl": "ja",
            "gl": "jp"
        }

        logger.info("ğŸŒ SerpAPI Base64æ¤œç´¢å®Ÿè¡Œä¸­...")
        results = client.search(search_params)

        # ãƒ‡ãƒãƒƒã‚°: ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ ã‚’ç¢ºèª
        logger.info(f"ğŸ“‹ SerpAPIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚­ãƒ¼: {list(results.keys())}")

        # å„ã‚­ãƒ¼ã®å†…å®¹ã‚’è©³ç´°ãƒ­ã‚°å‡ºåŠ›
        for key, value in results.items():
            if isinstance(value, list):
                logger.info(f"ğŸ“‹ {key}: {len(value)}å€‹ã®ã‚¢ã‚¤ãƒ†ãƒ ")
            elif isinstance(value, dict):
                logger.info(f"ğŸ“‹ {key}: è¾æ›¸å‹ ({len(value)}å€‹ã®ã‚­ãƒ¼)")
            else:
                logger.info(f"ğŸ“‹ {key}: {type(value).__name__} - {str(value)[:100]}")

        # æ¤œç´¢ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆæƒ…å ±ã‚’ç¢ºèª
        if 'search_metadata' in results:
            metadata = results['search_metadata']
            logger.info(f"ğŸ” æ¤œç´¢ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {metadata}")

        # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’ç¢ºèª
        if 'error' in results:
            logger.error(f"âŒ SerpAPIã‚¨ãƒ©ãƒ¼: {results['error']}")
            return []

        processed_results = []

        # ç”»åƒæ¤œç´¢çµæœã‚’å‡¦ç†
        if "image_results" in results and results["image_results"]:
            logger.info(f"âœ… image_resultsç™ºè¦‹: {len(results['image_results'])}ä»¶")
            for item in results["image_results"][:10]:
                url = item.get("link", "")
                title = item.get("title", "")
                source = item.get("source", "")

                if url and title:
                    domain, is_official, basic_threat_level = analyze_domain(url)

                    try:
                        detailed_analysis = await check_domain_and_analyze(url, domain)
                    except Exception as e:
                        detailed_analysis = {
                            "status": "unknown",
                            "reason": f"åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}",
                            "content_analysis": None
                        }

                    processed_results.append({
                        "url": url,
                        "domain": domain,
                        "title": title,
                        "source": source,
                        "is_official": is_official,
                        "threat_level": basic_threat_level,
                        "detailed_analysis": detailed_analysis,
                        "thumbnail": item.get("thumbnail", ""),
                        "analysis_timestamp": datetime.now().isoformat()
                    })

        # ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ç”»åƒçµæœã‚‚å‡¦ç†
        if "inline_images" in results and results["inline_images"]:
            logger.info(f"âœ… inline_imagesç™ºè¦‹: {len(results['inline_images'])}ä»¶")
            for item in results["inline_images"][:5]:
                url = item.get("link", "")
                title = item.get("title", "")
                source = item.get("source", "")

                if url and title:
                    domain, is_official, basic_threat_level = analyze_domain(url)

                    try:
                        detailed_analysis = await check_domain_and_analyze(url, domain)
                    except Exception as e:
                        detailed_analysis = {
                            "status": "unknown",
                            "reason": f"åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}",
                            "content_analysis": None
                        }

                    processed_results.append({
                        "url": url,
                        "domain": domain,
                        "title": title,
                        "source": source,
                        "is_official": is_official,
                        "threat_level": basic_threat_level,
                        "detailed_analysis": detailed_analysis,
                        "thumbnail": item.get("thumbnail", ""),
                        "analysis_timestamp": datetime.now().isoformat()
                    })

        logger.info(f"âœ… Base64æ¤œç´¢çµæœå‡¦ç†å®Œäº†: {len(processed_results)}ä»¶")
        return processed_results

    except Exception as e:
        logger.error(f"âŒ Base64 SerpAPIæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼è©³ç´°: {type(e).__name__}")
        return []

async def try_serpapi_search(image_path: str) -> List[Dict]:
    """URLæ–¹å¼ã§SerpAPIæ¤œç´¢ã‚’å®Ÿè¡Œï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
    try:
        filename = os.path.basename(image_path)
        ngrok_url = "https://a46d8d27d10b.ngrok-free.app"
        image_url = f"{ngrok_url}/temp-images/{filename}"

        logger.info(f"ğŸ“¸ URLæ–¹å¼SerpAPIæ¤œç´¢: {image_url}")

        client = serpapi.Client(api_key=SERPAPI_KEY)
        search_params = {
            "engine": "google_reverse_image",
            "image_url": image_url,
            "hl": "ja",
            "gl": "jp"
        }

        results = client.search(search_params)
        logger.info(f"ğŸ“‹ URLæ–¹å¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {len(results.get('image_results', []))}ä»¶")

        processed_results = []

        # URLæ–¹å¼ã®çµæœã‚’å‡¦ç†ï¼ˆBase64ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
        if "image_results" in results and results["image_results"]:
            for item in results["image_results"][:10]:
                url = item.get("link", "")
                title = item.get("title", "")
                source = item.get("source", "")

                if url and title:
                    domain, is_official, basic_threat_level = analyze_domain(url)

                    try:
                        detailed_analysis = await check_domain_and_analyze(url, domain)
                    except Exception as e:
                        detailed_analysis = {
                            "status": "unknown",
                            "reason": f"åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}",
                            "content_analysis": None
                        }

                    processed_results.append({
                        "url": url,
                        "domain": domain,
                        "title": title,
                        "source": f"{source} (URL)",
                        "is_official": is_official,
                        "threat_level": basic_threat_level,
                        "detailed_analysis": detailed_analysis,
                        "thumbnail": item.get("thumbnail", ""),
                        "analysis_timestamp": datetime.now().isoformat()
                    })

        return processed_results

    except Exception as e:
        logger.error(f"âŒ URL SerpAPIæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼è©³ç´°: {type(e).__name__}")
        return []

@app.get("/")
async def root():
    return {
        "message": "Book Leak Detector API",
        "api_keys": {
            "google_api_key_configured": GOOGLE_API_KEY is not None,
            "google_cse_id_configured": GOOGLE_CSE_ID is not None,
            "gemini_api_key_configured": GEMINI_API_KEY is not None,
            "serpapi_key_configured": SERPAPI_KEY is not None
        },
        "upload_count": len(upload_records),
        "search_results_count": len(search_results)
    }

@app.post("/upload")
async def upload_image(file: UploadFile = File(...)):
    """ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ä¿å­˜ã™ã‚‹"""

    logger.info(f"ğŸ“¤ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {file.filename}, content_type: {file.content_type}")

    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼
        if not validate_image_file(file):
            logger.error(f"âŒ ç„¡åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {file.content_type}")
            raise HTTPException(
                status_code=400,
                detail={
                    "error": "invalid_file_format",
                    "message": "ç„¡åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™ã€‚JPEGã€PNGã€GIFã€WebPå½¢å¼ã®ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
                    "allowed_types": ["image/jpeg", "image/png", "image/jpg", "image/gif", "image/webp"],
                    "received_type": file.content_type
                }
            )

        logger.info("âœ… ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼æ¤œè¨¼OK")

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™ï¼ˆ10MBï¼‰
        content = await file.read()
        file_size_mb = len(content) / (1024 * 1024)
        logger.info(f"ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size_mb:.2f}MB")

        if len(content) > 10 * 1024 * 1024:
            logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã¾ã™: {file_size_mb:.2f}MB")
            raise HTTPException(
                status_code=413,
                detail={
                    "error": "file_too_large",
                    "message": "ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã¾ã™ã€‚10MBä»¥ä¸‹ã®ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
                    "file_size_mb": file_size_mb,
                    "max_size_mb": 10
                }
            )

        try:
            # ç”»åƒã®æœ‰åŠ¹æ€§ã‚’ç¢ºèªï¼ˆãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ï¼‰
            image = Image.open(BytesIO(content))
            image.verify()
            logger.info("âœ… ç”»åƒæœ‰åŠ¹æ€§æ¤œè¨¼OK")
        except Exception as e:
            logger.error(f"âŒ ç”»åƒæ¤œè¨¼å¤±æ•—: {str(e)}")
            raise HTTPException(
                status_code=400,
                detail={
                    "error": "corrupted_image",
                    "message": "ç ´æã—ãŸç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ã€‚æœ‰åŠ¹ãªç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
                    "validation_error": str(e)
                }
            )

        # ä¸€æ„ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        file_id = str(uuid.uuid4())
        file_extension = os.path.splitext(file.filename or "image")[1].lower() or ".jpg"
        safe_filename = f"{file_id}{file_extension}"
        file_path = os.path.join(UPLOAD_DIR, safe_filename)

        logger.info(f"ğŸ’¾ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜é–‹å§‹: {file_path}")

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        try:
            with open(file_path, "wb") as f:
                f.write(content)
            logger.info("âœ… ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å¤±æ•—: {str(e)}")
            raise HTTPException(
                status_code=500,
                detail={
                    "error": "file_save_failed",
                    "message": f"ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}",
                    "file_path": file_path
                }
            )

        # è¨˜éŒ²ã‚’ä¿å­˜
        upload_record = {
            "id": file_id,
            "original_filename": file.filename,
            "saved_filename": safe_filename,
            "file_path": file_path,
            "content_type": file.content_type,
            "file_size": len(content),
            "upload_time": datetime.now().isoformat(),
            "status": "uploaded"
        }

        upload_records[file_id] = upload_record
        save_records()

        logger.info(f"âœ… ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: file_id={file_id}")

        return {
            "success": True,
            "file_id": file_id,
            "original_filename": file.filename,
            "saved_filename": safe_filename,
            "file_size": len(content),
            "upload_time": upload_record["upload_time"],
            "file_url": f"/uploads/{safe_filename}"
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": "unexpected_error",
                "message": f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            }
        )

@app.get("/uploads/history")
async def get_upload_history():
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å±¥æ­´ã‚’å–å¾—ã™ã‚‹"""
    # æ—¥ä»˜é †ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„ã‚‚ã®ãŒæœ€åˆï¼‰
    sorted_records = sorted(
        upload_records.values(),
        key=lambda x: x["upload_time"],
        reverse=True
    )

    return {
        "success": True,
        "count": len(sorted_records),
        "uploads": sorted_records
    }

@app.get("/uploads/{file_id}")
async def get_upload_details(file_id: str):
    """ç‰¹å®šã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°ã‚’å–å¾—ã™ã‚‹"""
    if file_id not in upload_records:
        raise HTTPException(
            status_code=404,
            detail="æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        )

    record = upload_records[file_id]

    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    if not os.path.exists(record["file_path"]):
        record["status"] = "file_missing"
        save_records()

    return {
        "success": True,
        "file": record
    }

@app.delete("/uploads/{file_id}")
async def delete_upload(file_id: str):
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã™ã‚‹"""
    if file_id not in upload_records:
        raise HTTPException(
            status_code=404,
            detail="æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        )

    record = upload_records[file_id]

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
    try:
        if os.path.exists(record["file_path"]):
            os.remove(record["file_path"])
    except Exception as e:
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")

    # è¨˜éŒ²ã‹ã‚‰å‰Šé™¤
    del upload_records[file_id]
    save_records()

    return {
        "success": True,
        "message": f"ãƒ•ã‚¡ã‚¤ãƒ« {record['original_filename']} ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚"
    }

@app.get("/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {
        "status": "healthy",
        "api_keys": {
            "google_api_key_configured": GOOGLE_API_KEY is not None,
            "google_cse_id_configured": GOOGLE_CSE_ID is not None,
            "gemini_api_key_configured": GEMINI_API_KEY is not None,
            "serpapi_key_configured": SERPAPI_KEY is not None
        },
        "system": {
            "upload_directory_exists": os.path.exists(UPLOAD_DIR),
            "records_file_exists": os.path.exists(RECORDS_FILE),
            "total_uploads": len(upload_records),
            "total_search_results": len(search_results)
        }
    }

@app.post("/search/{image_id}")
async def analyze_image(image_id: str):
    """æŒ‡å®šã•ã‚ŒãŸç”»åƒIDã«å¯¾ã—ã¦Google Vision APIåˆ†æã‚’å®Ÿè¡Œã—â—‹Ã—åˆ¤å®šã™ã‚‹"""

    logger.info(f"ğŸ” ç”»åƒåˆ†æé–‹å§‹: image_id={image_id}")

    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è¨˜éŒ²ã‚’ç¢ºèª
    if image_id not in upload_records:
        logger.error(f"âŒ image_id not found: {image_id}")
        raise HTTPException(
            status_code=404,
            detail={
                "error": "image_not_found",
                "message": "æŒ‡å®šã•ã‚ŒãŸimage_idãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚",
                "image_id": image_id
            }
        )

    record = upload_records[image_id]
    image_path = record["file_path"]

    logger.info(f"ğŸ“ åˆ†æå¯¾è±¡ç”»åƒ: {image_path}")

    try:
        # Google Vision APIã§åˆ†æãƒ»åˆ¤å®š
        logger.info("ğŸ¤– Google Vision APIåˆ†æé–‹å§‹")
        judgment_result = await analyze_and_judge_image(image_path)
        logger.info(f"âœ… åˆ†æå®Œäº†: åˆ¤å®š={judgment_result['judgment']}")

        # åˆ†æçµæœã‚’ãƒ¡ãƒ¢ãƒªã«ä¿å­˜
        search_results[image_id] = [judgment_result]

        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è¨˜éŒ²ã‚’æ›´æ–°
        record["analysis_status"] = "completed"
        record["analysis_time"] = datetime.now().isoformat()
        record["judgment"] = judgment_result["judgment"]
        record["reason"] = judgment_result["reason"]
        record["confidence"] = judgment_result.get("confidence", 0)
        save_records()

        logger.info(f"âœ… åˆ†æå®Œäº†: image_id={image_id}, åˆ¤å®š={judgment_result['judgment']}")

        return {
            "success": True,
            "image_id": image_id,
            "judgment": judgment_result["judgment"],
            "reason": judgment_result["reason"],
            "details": judgment_result.get("details", ""),
            "confidence": judgment_result.get("confidence", 0),
            "analysis_time": record["analysis_time"],
            "message": f"åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚åˆ¤å®š: {judgment_result['judgment']}"
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")

        # ã‚¨ãƒ©ãƒ¼çŠ¶æ…‹ã‚’è¨˜éŒ²
        record["analysis_status"] = "failed"
        record["analysis_error"] = str(e)
        record["analysis_time"] = datetime.now().isoformat()
        save_records()

        raise HTTPException(
            status_code=500,
            detail={
                "error": "analysis_failed",
                "message": f"ç”»åƒåˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
                "image_id": image_id
            }
        )

@app.get("/results")
async def get_all_results():
    """ã™ã¹ã¦ã®æ¤œç´¢çµæœã‚’å–å¾—ã™ã‚‹"""
    return {
        "success": True,
        "total_searches": len(search_results),
        "results": search_results
    }

@app.get("/results/{image_id}")
async def get_search_results(image_id: str):
    """ç‰¹å®šã®image_idã®æ¤œç´¢çµæœã‚’å–å¾—ã™ã‚‹"""
    if image_id not in upload_records:
        raise HTTPException(
            status_code=404,
            detail="æŒ‡å®šã•ã‚ŒãŸimage_idãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        )

    record = upload_records[image_id]

    if record.get("analysis_status") != "completed":
        raise HTTPException(
            status_code=404,
            detail="æŒ‡å®šã•ã‚ŒãŸç”»åƒã®åˆ†æçµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…ˆã«åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
        )

    return {
        "success": True,
        "image_id": image_id,
        "original_filename": record.get("original_filename", "ä¸æ˜"),
        "judgment": record.get("judgment", "Ã—"),
        "reason": record.get("reason", "åˆ†æçµæœä¸æ˜"),
        "confidence": record.get("confidence", 0),
        "analysis_time": record.get("analysis_time", "ä¸æ˜"),
        "file_size": record.get("file_size", 0),
        "message": f"åˆ¤å®šçµæœ: {record.get('judgment', 'Ã—')}"
    }

# ãƒ†ã‚¹ãƒˆç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.get("/test-search")
async def test_search():
    """ãƒ€ãƒŸãƒ¼ç”»åƒã§æ¤œç´¢ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã™ã‚‹"""
    logger.info("ğŸ§ª ãƒ†ã‚¹ãƒˆæ¤œç´¢é–‹å§‹")

    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
    test_image_id = "test-" + str(uuid.uuid4())

    # ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ€ãƒŸãƒ¼çµæœ
    dummy_results = [
        {
            "url": "https://amazon.co.jp/test-book",
            "domain": "amazon.co.jp",
            "title": "ãƒ†ã‚¹ãƒˆæ›¸ç± - Amazon",
            "source": "Amazon Japan",
            "is_official": True,
            "threat_level": "safe",
            "detailed_analysis": {
                "status": "safe",
                "reason": "å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ã§ã™",
                "content_analysis": None
            },
            "thumbnail": "https://example.com/thumb.jpg",
            "analysis_timestamp": datetime.now().isoformat()
        },
        {
            "url": "https://suspicious-site.com/free-download",
            "domain": "suspicious-site.com",
            "title": "ç„¡æ–™ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ - ç–‘ã‚ã—ã„ã‚µã‚¤ãƒˆ",
            "source": "Suspicious Site",
            "is_official": False,
            "threat_level": "suspicious",
            "detailed_analysis": {
                "status": "suspicious",
                "reason": "ç–‘ã‚ã—ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: ç„¡æ–™ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                "content_analysis": "åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆï¼ˆä¸€éƒ¨ï¼‰: ç„¡æ–™ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¯ã“ã¡ã‚‰..."
            },
            "thumbnail": "https://example.com/thumb2.jpg",
            "analysis_timestamp": datetime.now().isoformat()
        }
    ]

    # ãƒ†ã‚¹ãƒˆçµæœã‚’ãƒ¡ãƒ¢ãƒªã«ä¿å­˜
    search_results[test_image_id] = dummy_results

    logger.info(f"âœ… ãƒ†ã‚¹ãƒˆæ¤œç´¢å®Œäº†: {len(dummy_results)}ä»¶ã®çµæœ")

    return {
        "success": True,
        "test_image_id": test_image_id,
        "results_count": len(dummy_results),
        "message": f"ãƒ†ã‚¹ãƒˆæ¤œç´¢ãŒå®Œäº†ã—ã¾ã—ãŸã€‚{len(dummy_results)}ä»¶ã®çµæœãŒã‚ã‚Šã¾ã™ã€‚",
        "test_results": dummy_results
    }

@app.get("/test-domain/{domain}")
async def test_domain_analysis(domain: str):
    """æŒ‡å®šã•ã‚ŒãŸãƒ‰ãƒ¡ã‚¤ãƒ³ã®åˆ¤å®šãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã™ã‚‹"""
    logger.info(f"ğŸ§ª ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆé–‹å§‹: {domain}")

    # ãƒ†ã‚¹ãƒˆç”¨URL
    test_url = f"https://{domain}"

    try:
        # ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†æã‚’å®Ÿè¡Œ
        result = await check_domain_and_analyze(test_url, domain)

        # åŸºæœ¬çš„ãªè„…å¨ãƒ¬ãƒ™ãƒ«è©•ä¾¡ã‚‚å–å¾—
        _, is_official, basic_threat_level = analyze_domain(test_url)

        logger.info(f"âœ… ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆå®Œäº†: {domain} -> {result['status']}")

        return {
            "success": True,
            "domain": domain,
            "test_url": test_url,
            "is_official": is_official,
            "basic_threat_level": basic_threat_level,
            "detailed_analysis": result,
            "test_time": datetime.now().isoformat()
        }

    except Exception as e:
        logger.error(f"âŒ ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {
            "success": False,
            "domain": domain,
            "error": str(e),
            "test_time": datetime.now().isoformat()
        }

@app.get("/debug/logs")
async def get_debug_info():
    """ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å–å¾—ã™ã‚‹"""
    return {
        "system_status": "running",
        "total_uploads": len(upload_records),
        "total_search_results": len(search_results),
        "recent_uploads": list(upload_records.keys())[-5:] if upload_records else [],
        "api_keys_status": {
            "google_api_key": GOOGLE_API_KEY is not None,
            "google_cse_id": GOOGLE_CSE_ID is not None,
            "gemini_api_key": GEMINI_API_KEY is not None,
            "serpapi_key": SERPAPI_KEY is not None
        },
        "official_domains_count": len(OFFICIAL_DOMAINS),
        "timestamp": datetime.now().isoformat()
    }

@app.get("/logs")
async def get_system_logs():
    """ã‚·ã‚¹ãƒ†ãƒ ãƒ­ã‚°ã‚’å–å¾—ã™ã‚‹"""
    logger.info(f"ğŸ“‹ ãƒ­ã‚°å–å¾—è¦æ±‚: {len(system_logs)}ä»¶ã®ãƒ­ã‚°")
    return {
        "success": True,
        "total_logs": len(system_logs),
        "logs": system_logs[-50:],  # æœ€æ–°50ä»¶ã‚’è¿”ã™
        "timestamp": datetime.now().isoformat()
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)